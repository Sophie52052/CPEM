{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#from skimage.measure import structural_similarity as ssim\n",
    "#from sporco import util\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "import multiprocessing\n",
    "import datetime\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnn(x, input_size, output_size, keep_prob, stddev=0.01, constant=0.0001, dropout=True, end=False):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([input_size,output_size], stddev=stddev,seed=np.random.seed(2018)))\n",
    "    fc_b = tf.Variable(tf.constant(constant,shape=[output_size]), dtype=tf.float32)\n",
    "    fc_h = tf.nn.relu(tf.matmul(x,fc_w)+fc_b) if not end else tf.matmul(x,fc_w)+fc_b\n",
    "    return tf.nn.dropout(fc_h, keep_prob,seed=np.random.seed(2018)) if dropout else fc_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn(x, input_size, output_size, nlayers, nparameters, keep_prob):\n",
    "    if nlayers == 1:\n",
    "        h1 = fnn(x, input_size, output_size, keep_prob, end=True)\n",
    "    elif nlayers == 2:\n",
    "        h1 = fnn(fnn(x, input_size, nparameters, keep_prob, end=False), nparameters, output_size, keep_prob, end=True)\n",
    "    elif nlayers >= 3:\n",
    "        h0 = fnn(x, input_size, nparameters, keep_prob, end=False)\n",
    "        for j in range(0,nlayers-2):\n",
    "            if j == 0:\n",
    "                h1 = fnn(h0, nparameters, nparameters, keep_prob, end=False)\n",
    "            else:\n",
    "                h1 = fnn(h1, nparameters, nparameters, keep_prob, end=False)\n",
    "        h1 = fnn(h1, nparameters, output_size, keep_prob, end=True)\n",
    "    else:\n",
    "        print(\"# of layers can't be smaller than 0\")\n",
    "    return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc(train_data, train_label, test_data, test_label):\n",
    "    rf = RandomForestClassifier(n_estimators=150,\n",
    "                                    criterion='gini',\n",
    "                                    max_depth=None,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_weight_fraction_leaf=0.0,\n",
    "                                    max_features=None,\n",
    "                                    max_leaf_nodes=None,\n",
    "                                    bootstrap=True,\n",
    "                                    oob_score=False,\n",
    "                                    n_jobs=-1,\n",
    "                                    random_state=123,\n",
    "                                    verbose=0,\n",
    "                                    warm_start=False,\n",
    "                                    class_weight=None)\n",
    "    rf.fit(train_data, train_label.ravel())\n",
    "    result = rf.predict_proba(test_data)\n",
    "    acc = 0.0\n",
    "    for i in range(np.shape(test_data)[0]):\n",
    "        r = np.argmax(result[i])\n",
    "        if r == test_label[i]:\n",
    "            acc += 1\n",
    "    acc /= np.shape(test_data)[0]\n",
    "    acc *= 100\n",
    "    return acc, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(train_data, train_label, test_data, test_label):\n",
    "    g = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    batch_size = 10\n",
    "    input_size = np.shape(train_data)[1]\n",
    "    output_size = 31\n",
    "\n",
    "    with g.as_default():\n",
    "        p_x = tf.placeholder(tf.float32, [batch_size, 1, input_size, 1])\n",
    "        p_y = tf.placeholder(tf.float32, [batch_size, output_size])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h10_flat = tf.reshape(p_x, [batch_size,-1])\n",
    "        h1 = fnn(h10_flat, input_size, 2048, keep_prob, end=False)\n",
    "        h2 = fnn(h1, 2048, 2048, keep_prob, end=False)\n",
    "        h3 = fnn(h2, 2048, 31, keep_prob, end=True)\n",
    "        h4 = tf.reshape(h3, [batch_size, 31])\n",
    "        h_c = tf.nn.softmax(h4)\n",
    "        loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=p_y, logits=h4))\n",
    "        optim = tf.train.AdamOptimizer(1e-5)\n",
    "        trainer = optim.minimize(loss)\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    result = np.zeros([np.shape(test_data)[0], 31])\n",
    "    with tf.Session(graph=g, config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(0,120):\n",
    "            loss_tot = 0.0\n",
    "            for i in range(0,int(np.ceil(np.shape(train_data)[0]/batch_size))):\n",
    "                a = np.random.randint(0,np.shape(train_data)[0],size=batch_size)\n",
    "                x = train_data[a].reshape([batch_size, 1, input_size, 1])#[4,1,18181,1]\n",
    "                y = np.zeros([batch_size, output_size])\n",
    "                index = train_label[a]\n",
    "                for u in range(0,batch_size):\n",
    "                    y[u,index[u]] = 1\n",
    "                _ , loss_val = sess.run([trainer, loss], feed_dict={p_x:x, p_y:y, keep_prob:0.6})\n",
    "                loss_tot += loss_val\n",
    "            print(\"%d epoch Loss: %f\" % (e,(loss_tot)/np.shape(train_data)[0]))\n",
    "        temp = 0\n",
    "        for i in range(0,int(np.floor(np.shape(test_data)[0]/batch_size))):\n",
    "            x = test_data[i*batch_size:(i+1)*batch_size].reshape([batch_size, 1, input_size, 1])\n",
    "            out = sess.run(h_c, feed_dict={p_x:x, keep_prob:1})\n",
    "            for j in range(0, batch_size):\n",
    "                t = np.squeeze(out[j])\n",
    "                result[temp] = t\n",
    "                temp+=1\n",
    "        remain = int(np.shape(test_data)[0]-np.floor(np.shape(test_data)[0]/batch_size)*batch_size)\n",
    "        if remain > 0:\n",
    "            x = test_data[-batch_size-1:-1].reshape([batch_size, 1, input_size, 1])\n",
    "            out = sess.run(h_c, feed_dict={p_x:x, keep_prob:1})\n",
    "            for j in range(0,int(remain)):\n",
    "                t = np.squeeze(out[j+(batch_size-remain)])\n",
    "                result[temp] = t\n",
    "                temp+=1\n",
    "        for i in range(0,np.shape(test_data)[0]):\n",
    "            ind = np.argmax(np.squeeze(result[i]))\n",
    "            if ind == test_label[i]:\n",
    "                accuracy += 1\n",
    "        accuracy /= np.shape(test_data)[0]*0.01\n",
    "        sess.close()\n",
    "    return accuracy, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataID = hdf5storage.loadmat('data.mat')\n",
    "data = np.array(dataID['data'], dtype=np.float32)\n",
    "gt1 = scipy.io.loadmat('label.mat')\n",
    "label = np.array(gt1['label'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outer_loop = 10\n",
    "Inner_loop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-2-6d0cdc7b68c3>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-5c2fee5bad1b>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0 epoch Loss: 3.110789\n",
      "1 epoch Loss: 2.655721\n",
      "2 epoch Loss: 2.449468\n",
      "3 epoch Loss: 2.351386\n",
      "4 epoch Loss: 2.236539\n",
      "5 epoch Loss: 2.202395\n",
      "6 epoch Loss: 2.086961\n",
      "7 epoch Loss: 2.037218\n",
      "8 epoch Loss: 1.984925\n",
      "9 epoch Loss: 1.939601\n",
      "10 epoch Loss: 1.872381\n",
      "11 epoch Loss: 1.877065\n",
      "12 epoch Loss: 1.828764\n",
      "13 epoch Loss: 1.759474\n",
      "14 epoch Loss: 1.740209\n",
      "15 epoch Loss: 1.649005\n",
      "16 epoch Loss: 1.633552\n",
      "17 epoch Loss: 1.589401\n",
      "18 epoch Loss: 1.560510\n",
      "19 epoch Loss: 1.543371\n",
      "20 epoch Loss: 1.499568\n",
      "21 epoch Loss: 1.453408\n",
      "22 epoch Loss: 1.450271\n",
      "23 epoch Loss: 1.402040\n",
      "24 epoch Loss: 1.377945\n",
      "25 epoch Loss: 1.331619\n",
      "26 epoch Loss: 1.339056\n",
      "27 epoch Loss: 1.304530\n",
      "28 epoch Loss: 1.258558\n",
      "29 epoch Loss: 1.296593\n",
      "30 epoch Loss: 1.250849\n",
      "31 epoch Loss: 1.256989\n",
      "32 epoch Loss: 1.254351\n",
      "33 epoch Loss: 1.222121\n",
      "34 epoch Loss: 1.212230\n",
      "35 epoch Loss: 1.212813\n",
      "36 epoch Loss: 1.170247\n",
      "37 epoch Loss: 1.178917\n",
      "38 epoch Loss: 1.156375\n",
      "39 epoch Loss: 1.142012\n",
      "40 epoch Loss: 1.166041\n",
      "41 epoch Loss: 1.141753\n",
      "42 epoch Loss: 1.126396\n",
      "43 epoch Loss: 1.112944\n",
      "44 epoch Loss: 1.143993\n",
      "45 epoch Loss: 1.126142\n",
      "46 epoch Loss: 1.126969\n",
      "47 epoch Loss: 1.108247\n",
      "48 epoch Loss: 1.136934\n",
      "49 epoch Loss: 1.078798\n",
      "50 epoch Loss: 1.094691\n",
      "51 epoch Loss: 1.111869\n",
      "52 epoch Loss: 1.062697\n",
      "53 epoch Loss: 1.055383\n",
      "54 epoch Loss: 1.063337\n",
      "55 epoch Loss: 1.067175\n",
      "56 epoch Loss: 1.061195\n",
      "57 epoch Loss: 1.052514\n",
      "58 epoch Loss: 1.071491\n",
      "59 epoch Loss: 1.053526\n",
      "60 epoch Loss: 1.085711\n",
      "61 epoch Loss: 1.054114\n",
      "62 epoch Loss: 1.079047\n",
      "63 epoch Loss: 1.069849\n",
      "64 epoch Loss: 1.048119\n",
      "65 epoch Loss: 1.036121\n",
      "66 epoch Loss: 1.040662\n",
      "67 epoch Loss: 1.058330\n",
      "68 epoch Loss: 1.034359\n",
      "69 epoch Loss: 1.032367\n",
      "70 epoch Loss: 1.023963\n",
      "71 epoch Loss: 1.016117\n",
      "72 epoch Loss: 1.018904\n",
      "73 epoch Loss: 1.044949\n",
      "74 epoch Loss: 1.067366\n",
      "75 epoch Loss: 1.034902\n",
      "76 epoch Loss: 1.063619\n",
      "77 epoch Loss: 1.015432\n",
      "78 epoch Loss: 1.024049\n",
      "79 epoch Loss: 1.047164\n",
      "80 epoch Loss: 1.028419\n",
      "81 epoch Loss: 1.043117\n",
      "82 epoch Loss: 1.037906\n",
      "83 epoch Loss: 1.045961\n",
      "84 epoch Loss: 1.054302\n",
      "85 epoch Loss: 1.026609\n",
      "86 epoch Loss: 1.037790\n",
      "87 epoch Loss: 1.009985\n",
      "88 epoch Loss: 1.042771\n",
      "89 epoch Loss: 0.989969\n",
      "90 epoch Loss: 1.013209\n",
      "91 epoch Loss: 1.008377\n",
      "92 epoch Loss: 1.031869\n",
      "93 epoch Loss: 1.047252\n",
      "94 epoch Loss: 1.015455\n",
      "95 epoch Loss: 1.044682\n",
      "96 epoch Loss: 1.035356\n",
      "97 epoch Loss: 1.047286\n",
      "98 epoch Loss: 0.997680\n",
      "99 epoch Loss: 1.015685\n",
      "100 epoch Loss: 1.007460\n",
      "101 epoch Loss: 1.046912\n",
      "102 epoch Loss: 0.998829\n",
      "103 epoch Loss: 1.052697\n",
      "104 epoch Loss: 1.021690\n",
      "105 epoch Loss: 1.042797\n",
      "106 epoch Loss: 1.028611\n",
      "107 epoch Loss: 1.026209\n",
      "108 epoch Loss: 1.036255\n",
      "109 epoch Loss: 1.045865\n",
      "110 epoch Loss: 1.022478\n",
      "111 epoch Loss: 1.036912\n",
      "112 epoch Loss: 1.029386\n",
      "113 epoch Loss: 1.020294\n",
      "114 epoch Loss: 1.034067\n",
      "115 epoch Loss: 1.015034\n",
      "116 epoch Loss: 1.023841\n",
      "117 epoch Loss: 1.005588\n",
      "118 epoch Loss: 1.034354\n",
      "119 epoch Loss: 1.028773\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 78.857143, Random forests accuracy: 74.428571, Ensemble accuracy: 80.571429\n",
      "0 epoch Loss: 3.105799\n",
      "1 epoch Loss: 2.657524\n",
      "2 epoch Loss: 2.448728\n",
      "3 epoch Loss: 2.297298\n",
      "4 epoch Loss: 2.209436\n",
      "5 epoch Loss: 2.190890\n",
      "6 epoch Loss: 2.145736\n",
      "7 epoch Loss: 2.079490\n",
      "8 epoch Loss: 2.000071\n",
      "9 epoch Loss: 1.913428\n",
      "10 epoch Loss: 1.909759\n",
      "11 epoch Loss: 1.836984\n",
      "12 epoch Loss: 1.810414\n",
      "13 epoch Loss: 1.757895\n",
      "14 epoch Loss: 1.715501\n",
      "15 epoch Loss: 1.684785\n",
      "16 epoch Loss: 1.649767\n",
      "17 epoch Loss: 1.614624\n",
      "18 epoch Loss: 1.557123\n",
      "19 epoch Loss: 1.532431\n",
      "20 epoch Loss: 1.482861\n",
      "21 epoch Loss: 1.461671\n",
      "22 epoch Loss: 1.457021\n",
      "23 epoch Loss: 1.403368\n",
      "24 epoch Loss: 1.381295\n",
      "25 epoch Loss: 1.373772\n",
      "26 epoch Loss: 1.368954\n",
      "27 epoch Loss: 1.334472\n",
      "28 epoch Loss: 1.281570\n",
      "29 epoch Loss: 1.295929\n",
      "30 epoch Loss: 1.280245\n",
      "31 epoch Loss: 1.293364\n",
      "32 epoch Loss: 1.260792\n",
      "33 epoch Loss: 1.250688\n",
      "34 epoch Loss: 1.234298\n",
      "35 epoch Loss: 1.196319\n",
      "36 epoch Loss: 1.185015\n",
      "37 epoch Loss: 1.165167\n",
      "38 epoch Loss: 1.160165\n",
      "39 epoch Loss: 1.177834\n",
      "40 epoch Loss: 1.143978\n",
      "41 epoch Loss: 1.162532\n",
      "42 epoch Loss: 1.145736\n",
      "43 epoch Loss: 1.115941\n",
      "44 epoch Loss: 1.146594\n",
      "45 epoch Loss: 1.142566\n",
      "46 epoch Loss: 1.137075\n",
      "47 epoch Loss: 1.119183\n",
      "48 epoch Loss: 1.109027\n",
      "49 epoch Loss: 1.111699\n",
      "50 epoch Loss: 1.111737\n",
      "51 epoch Loss: 1.104192\n",
      "52 epoch Loss: 1.085745\n",
      "53 epoch Loss: 1.064940\n",
      "54 epoch Loss: 1.091437\n",
      "55 epoch Loss: 1.081865\n",
      "56 epoch Loss: 1.050471\n",
      "57 epoch Loss: 1.069310\n",
      "58 epoch Loss: 1.049788\n",
      "59 epoch Loss: 1.044534\n",
      "60 epoch Loss: 1.073835\n",
      "61 epoch Loss: 1.053638\n",
      "62 epoch Loss: 1.030608\n",
      "63 epoch Loss: 1.052867\n",
      "64 epoch Loss: 1.044104\n",
      "65 epoch Loss: 1.041607\n",
      "66 epoch Loss: 1.074754\n",
      "67 epoch Loss: 1.076519\n",
      "68 epoch Loss: 1.053263\n",
      "69 epoch Loss: 1.032238\n",
      "70 epoch Loss: 1.063068\n",
      "71 epoch Loss: 1.025769\n",
      "72 epoch Loss: 1.066532\n",
      "73 epoch Loss: 1.021027\n",
      "74 epoch Loss: 1.028067\n",
      "75 epoch Loss: 1.042268\n",
      "76 epoch Loss: 1.048709\n",
      "77 epoch Loss: 1.021248\n",
      "78 epoch Loss: 1.029102\n",
      "79 epoch Loss: 1.045269\n",
      "80 epoch Loss: 1.055346\n",
      "81 epoch Loss: 1.024720\n",
      "82 epoch Loss: 1.057456\n",
      "83 epoch Loss: 1.028380\n",
      "84 epoch Loss: 1.043598\n",
      "85 epoch Loss: 1.023256\n",
      "86 epoch Loss: 1.039997\n",
      "87 epoch Loss: 1.032375\n",
      "88 epoch Loss: 1.048734\n",
      "89 epoch Loss: 1.010756\n",
      "90 epoch Loss: 1.033045\n",
      "91 epoch Loss: 1.034365\n",
      "92 epoch Loss: 1.034237\n",
      "93 epoch Loss: 1.018104\n",
      "94 epoch Loss: 1.030774\n",
      "95 epoch Loss: 1.026222\n",
      "96 epoch Loss: 1.039775\n",
      "97 epoch Loss: 1.037325\n",
      "98 epoch Loss: 1.000393\n",
      "99 epoch Loss: 1.033354\n",
      "100 epoch Loss: 1.014561\n",
      "101 epoch Loss: 1.034580\n",
      "102 epoch Loss: 1.025312\n",
      "103 epoch Loss: 1.014271\n",
      "104 epoch Loss: 1.008386\n",
      "105 epoch Loss: 1.026304\n",
      "106 epoch Loss: 1.022489\n",
      "107 epoch Loss: 1.037531\n",
      "108 epoch Loss: 1.005967\n",
      "109 epoch Loss: 1.048857\n",
      "110 epoch Loss: 1.008502\n",
      "111 epoch Loss: 1.019455\n",
      "112 epoch Loss: 1.003005\n",
      "113 epoch Loss: 1.038391\n",
      "114 epoch Loss: 1.015578\n",
      "115 epoch Loss: 1.035484\n",
      "116 epoch Loss: 1.023258\n",
      "117 epoch Loss: 1.027100\n",
      "118 epoch Loss: 1.038878\n",
      "119 epoch Loss: 1.028996\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.142857, Random forests accuracy: 73.714286, Ensemble accuracy: 79.571429\n",
      "0 epoch Loss: 3.117091\n",
      "1 epoch Loss: 2.641972\n",
      "2 epoch Loss: 2.448610\n",
      "3 epoch Loss: 2.325398\n",
      "4 epoch Loss: 2.255291\n",
      "5 epoch Loss: 2.185675\n",
      "6 epoch Loss: 2.120033\n",
      "7 epoch Loss: 2.031516\n",
      "8 epoch Loss: 2.000238\n",
      "9 epoch Loss: 1.945272\n",
      "10 epoch Loss: 1.895809\n",
      "11 epoch Loss: 1.851263\n",
      "12 epoch Loss: 1.830661\n",
      "13 epoch Loss: 1.772869\n",
      "14 epoch Loss: 1.763741\n",
      "15 epoch Loss: 1.664678\n",
      "16 epoch Loss: 1.662331\n",
      "17 epoch Loss: 1.588373\n",
      "18 epoch Loss: 1.558517\n",
      "19 epoch Loss: 1.529553\n",
      "20 epoch Loss: 1.490791\n",
      "21 epoch Loss: 1.487948\n",
      "22 epoch Loss: 1.458672\n",
      "23 epoch Loss: 1.462913\n",
      "24 epoch Loss: 1.384076\n",
      "25 epoch Loss: 1.396361\n",
      "26 epoch Loss: 1.316734\n",
      "27 epoch Loss: 1.310296\n",
      "28 epoch Loss: 1.319560\n",
      "29 epoch Loss: 1.308519\n",
      "30 epoch Loss: 1.286491\n",
      "31 epoch Loss: 1.262055\n",
      "32 epoch Loss: 1.239070\n",
      "33 epoch Loss: 1.219002\n",
      "34 epoch Loss: 1.188780\n",
      "35 epoch Loss: 1.169991\n",
      "36 epoch Loss: 1.191360\n",
      "37 epoch Loss: 1.188211\n",
      "38 epoch Loss: 1.165578\n",
      "39 epoch Loss: 1.159347\n",
      "40 epoch Loss: 1.139030\n",
      "41 epoch Loss: 1.145073\n",
      "42 epoch Loss: 1.142719\n",
      "43 epoch Loss: 1.149246\n",
      "44 epoch Loss: 1.136428\n",
      "45 epoch Loss: 1.118200\n",
      "46 epoch Loss: 1.104913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 epoch Loss: 1.139266\n",
      "48 epoch Loss: 1.099404\n",
      "49 epoch Loss: 1.103059\n",
      "50 epoch Loss: 1.074333\n",
      "51 epoch Loss: 1.114221\n",
      "52 epoch Loss: 1.092705\n",
      "53 epoch Loss: 1.098148\n",
      "54 epoch Loss: 1.067369\n",
      "55 epoch Loss: 1.085901\n",
      "56 epoch Loss: 1.086450\n",
      "57 epoch Loss: 1.058019\n",
      "58 epoch Loss: 1.033146\n",
      "59 epoch Loss: 1.073377\n",
      "60 epoch Loss: 1.057879\n",
      "61 epoch Loss: 1.062973\n",
      "62 epoch Loss: 1.069221\n",
      "63 epoch Loss: 1.062959\n",
      "64 epoch Loss: 1.086612\n",
      "65 epoch Loss: 1.029951\n",
      "66 epoch Loss: 1.056532\n",
      "67 epoch Loss: 1.044552\n",
      "68 epoch Loss: 1.064686\n",
      "69 epoch Loss: 1.029901\n",
      "70 epoch Loss: 1.054319\n",
      "71 epoch Loss: 1.051597\n",
      "72 epoch Loss: 1.045254\n",
      "73 epoch Loss: 1.050666\n",
      "74 epoch Loss: 1.060179\n",
      "75 epoch Loss: 1.031216\n",
      "76 epoch Loss: 1.066315\n",
      "77 epoch Loss: 1.073136\n",
      "78 epoch Loss: 1.058837\n",
      "79 epoch Loss: 1.022108\n",
      "80 epoch Loss: 1.021134\n",
      "81 epoch Loss: 1.033943\n",
      "82 epoch Loss: 1.062733\n",
      "83 epoch Loss: 1.032640\n",
      "84 epoch Loss: 1.039889\n",
      "85 epoch Loss: 1.039289\n",
      "86 epoch Loss: 1.036772\n",
      "87 epoch Loss: 1.043366\n",
      "88 epoch Loss: 1.013409\n",
      "89 epoch Loss: 1.032652\n",
      "90 epoch Loss: 1.032789\n",
      "91 epoch Loss: 1.025126\n",
      "92 epoch Loss: 1.059953\n",
      "93 epoch Loss: 1.042233\n",
      "94 epoch Loss: 1.023440\n",
      "95 epoch Loss: 1.063335\n",
      "96 epoch Loss: 1.038651\n",
      "97 epoch Loss: 1.028911\n",
      "98 epoch Loss: 1.033108\n",
      "99 epoch Loss: 1.033588\n",
      "100 epoch Loss: 1.059857\n",
      "101 epoch Loss: 1.021247\n",
      "102 epoch Loss: 1.011260\n",
      "103 epoch Loss: 1.009982\n",
      "104 epoch Loss: 1.008630\n",
      "105 epoch Loss: 1.034812\n",
      "106 epoch Loss: 1.031462\n",
      "107 epoch Loss: 1.022213\n",
      "108 epoch Loss: 1.026500\n",
      "109 epoch Loss: 1.018191\n",
      "110 epoch Loss: 1.046519\n",
      "111 epoch Loss: 1.031370\n",
      "112 epoch Loss: 0.997925\n",
      "113 epoch Loss: 1.033157\n",
      "114 epoch Loss: 1.043094\n",
      "115 epoch Loss: 1.000347\n",
      "116 epoch Loss: 1.048890\n",
      "117 epoch Loss: 1.028366\n",
      "118 epoch Loss: 1.034671\n",
      "119 epoch Loss: 0.990335\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.428571, Random forests accuracy: 73.714286, Ensemble accuracy: 80.571429\n",
      "0 epoch Loss: 3.091425\n",
      "1 epoch Loss: 2.649080\n",
      "2 epoch Loss: 2.455654\n",
      "3 epoch Loss: 2.330912\n",
      "4 epoch Loss: 2.226298\n",
      "5 epoch Loss: 2.211321\n",
      "6 epoch Loss: 2.066357\n",
      "7 epoch Loss: 2.071766\n",
      "8 epoch Loss: 1.985806\n",
      "9 epoch Loss: 1.926766\n",
      "10 epoch Loss: 1.903458\n",
      "11 epoch Loss: 1.861970\n",
      "12 epoch Loss: 1.792344\n",
      "13 epoch Loss: 1.735042\n",
      "14 epoch Loss: 1.722769\n",
      "15 epoch Loss: 1.667197\n",
      "16 epoch Loss: 1.636420\n",
      "17 epoch Loss: 1.621105\n",
      "18 epoch Loss: 1.561505\n",
      "19 epoch Loss: 1.531653\n",
      "20 epoch Loss: 1.490245\n",
      "21 epoch Loss: 1.458013\n",
      "22 epoch Loss: 1.463139\n",
      "23 epoch Loss: 1.412994\n",
      "24 epoch Loss: 1.393326\n",
      "25 epoch Loss: 1.368162\n",
      "26 epoch Loss: 1.331740\n",
      "27 epoch Loss: 1.354126\n",
      "28 epoch Loss: 1.319990\n",
      "29 epoch Loss: 1.280528\n",
      "30 epoch Loss: 1.278481\n",
      "31 epoch Loss: 1.241144\n",
      "32 epoch Loss: 1.220602\n",
      "33 epoch Loss: 1.224148\n",
      "34 epoch Loss: 1.223091\n",
      "35 epoch Loss: 1.211458\n",
      "36 epoch Loss: 1.192027\n",
      "37 epoch Loss: 1.166042\n",
      "38 epoch Loss: 1.159815\n",
      "39 epoch Loss: 1.164478\n",
      "40 epoch Loss: 1.175714\n",
      "41 epoch Loss: 1.174844\n",
      "42 epoch Loss: 1.131674\n",
      "43 epoch Loss: 1.135158\n",
      "44 epoch Loss: 1.132814\n",
      "45 epoch Loss: 1.116271\n",
      "46 epoch Loss: 1.142246\n",
      "47 epoch Loss: 1.128629\n",
      "48 epoch Loss: 1.099777\n",
      "49 epoch Loss: 1.078842\n",
      "50 epoch Loss: 1.079505\n",
      "51 epoch Loss: 1.085944\n",
      "52 epoch Loss: 1.111688\n",
      "53 epoch Loss: 1.086863\n",
      "54 epoch Loss: 1.073516\n",
      "55 epoch Loss: 1.099793\n",
      "56 epoch Loss: 1.091016\n",
      "57 epoch Loss: 1.063714\n",
      "58 epoch Loss: 1.068567\n",
      "59 epoch Loss: 1.048523\n",
      "60 epoch Loss: 1.074592\n",
      "61 epoch Loss: 1.049932\n",
      "62 epoch Loss: 1.065307\n",
      "63 epoch Loss: 1.054795\n",
      "64 epoch Loss: 1.061228\n",
      "65 epoch Loss: 1.050117\n",
      "66 epoch Loss: 1.021371\n",
      "67 epoch Loss: 1.025609\n",
      "68 epoch Loss: 1.048465\n",
      "69 epoch Loss: 1.055325\n",
      "70 epoch Loss: 1.044756\n",
      "71 epoch Loss: 1.033868\n",
      "72 epoch Loss: 1.048959\n",
      "73 epoch Loss: 1.049784\n",
      "74 epoch Loss: 1.022401\n",
      "75 epoch Loss: 1.027011\n",
      "76 epoch Loss: 1.070123\n",
      "77 epoch Loss: 1.026637\n",
      "78 epoch Loss: 1.054232\n",
      "79 epoch Loss: 1.044912\n",
      "80 epoch Loss: 1.031833\n",
      "81 epoch Loss: 1.012168\n",
      "82 epoch Loss: 1.046595\n",
      "83 epoch Loss: 1.037914\n",
      "84 epoch Loss: 1.032873\n",
      "85 epoch Loss: 1.024219\n",
      "86 epoch Loss: 1.053671\n",
      "87 epoch Loss: 1.018576\n",
      "88 epoch Loss: 1.024455\n",
      "89 epoch Loss: 1.020955\n",
      "90 epoch Loss: 1.033565\n",
      "91 epoch Loss: 1.043323\n",
      "92 epoch Loss: 1.019446\n",
      "93 epoch Loss: 1.019119\n",
      "94 epoch Loss: 1.030319\n",
      "95 epoch Loss: 1.018704\n",
      "96 epoch Loss: 1.022695\n",
      "97 epoch Loss: 1.047638\n",
      "98 epoch Loss: 1.063374\n",
      "99 epoch Loss: 1.031529\n",
      "100 epoch Loss: 1.032284\n",
      "101 epoch Loss: 1.037189\n",
      "102 epoch Loss: 1.019673\n",
      "103 epoch Loss: 1.019956\n",
      "104 epoch Loss: 1.039462\n",
      "105 epoch Loss: 1.009562\n",
      "106 epoch Loss: 1.021798\n",
      "107 epoch Loss: 1.026223\n",
      "108 epoch Loss: 1.016271\n",
      "109 epoch Loss: 0.986998\n",
      "110 epoch Loss: 1.019684\n",
      "111 epoch Loss: 1.011329\n",
      "112 epoch Loss: 1.044281\n",
      "113 epoch Loss: 1.029874\n",
      "114 epoch Loss: 1.035675\n",
      "115 epoch Loss: 1.026119\n",
      "116 epoch Loss: 1.020474\n",
      "117 epoch Loss: 1.041476\n",
      "118 epoch Loss: 1.007068\n",
      "119 epoch Loss: 1.029048\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 78.142857, Random forests accuracy: 73.000000, Ensemble accuracy: 79.285714\n",
      "0 epoch Loss: 3.101485\n",
      "1 epoch Loss: 2.626260\n",
      "2 epoch Loss: 2.435734\n",
      "3 epoch Loss: 2.313478\n",
      "4 epoch Loss: 2.221545\n",
      "5 epoch Loss: 2.176597\n",
      "6 epoch Loss: 2.096276\n",
      "7 epoch Loss: 2.046044\n",
      "8 epoch Loss: 2.008953\n",
      "9 epoch Loss: 1.925890\n",
      "10 epoch Loss: 1.885677\n",
      "11 epoch Loss: 1.869160\n",
      "12 epoch Loss: 1.793410\n",
      "13 epoch Loss: 1.757777\n",
      "14 epoch Loss: 1.719423\n",
      "15 epoch Loss: 1.660309\n",
      "16 epoch Loss: 1.631706\n",
      "17 epoch Loss: 1.584844\n",
      "18 epoch Loss: 1.542382\n",
      "19 epoch Loss: 1.500265\n",
      "20 epoch Loss: 1.504199\n",
      "21 epoch Loss: 1.462692\n",
      "22 epoch Loss: 1.436355\n",
      "23 epoch Loss: 1.407442\n",
      "24 epoch Loss: 1.385628\n",
      "25 epoch Loss: 1.363522\n",
      "26 epoch Loss: 1.311936\n",
      "27 epoch Loss: 1.356046\n",
      "28 epoch Loss: 1.298485\n",
      "29 epoch Loss: 1.320863\n",
      "30 epoch Loss: 1.262058\n",
      "31 epoch Loss: 1.263357\n",
      "32 epoch Loss: 1.244201\n",
      "33 epoch Loss: 1.227474\n",
      "34 epoch Loss: 1.216326\n",
      "35 epoch Loss: 1.183969\n",
      "36 epoch Loss: 1.228922\n",
      "37 epoch Loss: 1.189334\n",
      "38 epoch Loss: 1.158031\n",
      "39 epoch Loss: 1.173547\n",
      "40 epoch Loss: 1.128498\n",
      "41 epoch Loss: 1.148809\n",
      "42 epoch Loss: 1.136679\n",
      "43 epoch Loss: 1.118095\n",
      "44 epoch Loss: 1.121184\n",
      "45 epoch Loss: 1.125904\n",
      "46 epoch Loss: 1.097739\n",
      "47 epoch Loss: 1.148991\n",
      "48 epoch Loss: 1.105659\n",
      "49 epoch Loss: 1.082338\n",
      "50 epoch Loss: 1.091482\n",
      "51 epoch Loss: 1.113001\n",
      "52 epoch Loss: 1.091404\n",
      "53 epoch Loss: 1.066478\n",
      "54 epoch Loss: 1.068011\n",
      "55 epoch Loss: 1.086721\n",
      "56 epoch Loss: 1.073349\n",
      "57 epoch Loss: 1.053990\n",
      "58 epoch Loss: 1.050651\n",
      "59 epoch Loss: 1.075932\n",
      "60 epoch Loss: 1.040651\n",
      "61 epoch Loss: 1.048412\n",
      "62 epoch Loss: 1.078560\n",
      "63 epoch Loss: 1.047957\n",
      "64 epoch Loss: 1.049706\n",
      "65 epoch Loss: 1.065702\n",
      "66 epoch Loss: 1.039008\n",
      "67 epoch Loss: 1.057271\n",
      "68 epoch Loss: 1.034120\n",
      "69 epoch Loss: 1.042331\n",
      "70 epoch Loss: 1.052500\n",
      "71 epoch Loss: 1.033139\n",
      "72 epoch Loss: 1.042206\n",
      "73 epoch Loss: 1.038347\n",
      "74 epoch Loss: 1.051865\n",
      "75 epoch Loss: 1.042192\n",
      "76 epoch Loss: 1.031408\n",
      "77 epoch Loss: 1.030037\n",
      "78 epoch Loss: 1.055977\n",
      "79 epoch Loss: 1.033192\n",
      "80 epoch Loss: 1.025677\n",
      "81 epoch Loss: 1.028908\n",
      "82 epoch Loss: 1.052552\n",
      "83 epoch Loss: 1.028393\n",
      "84 epoch Loss: 1.009059\n",
      "85 epoch Loss: 1.012801\n",
      "86 epoch Loss: 1.033859\n",
      "87 epoch Loss: 1.036994\n",
      "88 epoch Loss: 1.055428\n",
      "89 epoch Loss: 1.028460\n",
      "90 epoch Loss: 1.060332\n",
      "91 epoch Loss: 1.024032\n",
      "92 epoch Loss: 1.017574\n",
      "93 epoch Loss: 0.996961\n",
      "94 epoch Loss: 1.030942\n",
      "95 epoch Loss: 1.031228\n",
      "96 epoch Loss: 1.050091\n",
      "97 epoch Loss: 1.037513\n",
      "98 epoch Loss: 1.039335\n",
      "99 epoch Loss: 1.009952\n",
      "100 epoch Loss: 1.031057\n",
      "101 epoch Loss: 1.034137\n",
      "102 epoch Loss: 1.028803\n",
      "103 epoch Loss: 1.026070\n",
      "104 epoch Loss: 1.038440\n",
      "105 epoch Loss: 1.036039\n",
      "106 epoch Loss: 1.028288\n",
      "107 epoch Loss: 1.047017\n",
      "108 epoch Loss: 0.992374\n",
      "109 epoch Loss: 1.010263\n",
      "110 epoch Loss: 1.037414\n",
      "111 epoch Loss: 1.009216\n",
      "112 epoch Loss: 0.991815\n",
      "113 epoch Loss: 1.031774\n",
      "114 epoch Loss: 1.030118\n",
      "115 epoch Loss: 0.997776\n",
      "116 epoch Loss: 1.011646\n",
      "117 epoch Loss: 1.048139\n",
      "118 epoch Loss: 0.987340\n",
      "119 epoch Loss: 1.031851\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.285714, Random forests accuracy: 75.142857, Ensemble accuracy: 79.571429\n",
      "0 epoch Loss: 3.110519\n",
      "1 epoch Loss: 2.646147\n",
      "2 epoch Loss: 2.452166\n",
      "3 epoch Loss: 2.306587\n",
      "4 epoch Loss: 2.243105\n",
      "5 epoch Loss: 2.176519\n",
      "6 epoch Loss: 2.135118\n",
      "7 epoch Loss: 2.061255\n",
      "8 epoch Loss: 2.025759\n",
      "9 epoch Loss: 1.952943\n",
      "10 epoch Loss: 1.906214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 epoch Loss: 1.852383\n",
      "12 epoch Loss: 1.805297\n",
      "13 epoch Loss: 1.761635\n",
      "14 epoch Loss: 1.719393\n",
      "15 epoch Loss: 1.680133\n",
      "16 epoch Loss: 1.643024\n",
      "17 epoch Loss: 1.613421\n",
      "18 epoch Loss: 1.563152\n",
      "19 epoch Loss: 1.517663\n",
      "20 epoch Loss: 1.485067\n",
      "21 epoch Loss: 1.506374\n",
      "22 epoch Loss: 1.424089\n",
      "23 epoch Loss: 1.405121\n",
      "24 epoch Loss: 1.373335\n",
      "25 epoch Loss: 1.370389\n",
      "26 epoch Loss: 1.339172\n",
      "27 epoch Loss: 1.317954\n",
      "28 epoch Loss: 1.321470\n",
      "29 epoch Loss: 1.267472\n",
      "30 epoch Loss: 1.305418\n",
      "31 epoch Loss: 1.280021\n",
      "32 epoch Loss: 1.249059\n",
      "33 epoch Loss: 1.189250\n",
      "34 epoch Loss: 1.195233\n",
      "35 epoch Loss: 1.198614\n",
      "36 epoch Loss: 1.206030\n",
      "37 epoch Loss: 1.178925\n",
      "38 epoch Loss: 1.164223\n",
      "39 epoch Loss: 1.184287\n",
      "40 epoch Loss: 1.151603\n",
      "41 epoch Loss: 1.126793\n",
      "42 epoch Loss: 1.134464\n",
      "43 epoch Loss: 1.149514\n",
      "44 epoch Loss: 1.120582\n",
      "45 epoch Loss: 1.105401\n",
      "46 epoch Loss: 1.102811\n",
      "47 epoch Loss: 1.084885\n",
      "48 epoch Loss: 1.108272\n",
      "49 epoch Loss: 1.104902\n",
      "50 epoch Loss: 1.106782\n",
      "51 epoch Loss: 1.113303\n",
      "52 epoch Loss: 1.073526\n",
      "53 epoch Loss: 1.092909\n",
      "54 epoch Loss: 1.069931\n",
      "55 epoch Loss: 1.046541\n",
      "56 epoch Loss: 1.091277\n",
      "57 epoch Loss: 1.085399\n",
      "58 epoch Loss: 1.079640\n",
      "59 epoch Loss: 1.073789\n",
      "60 epoch Loss: 1.070415\n",
      "61 epoch Loss: 1.065901\n",
      "62 epoch Loss: 1.048210\n",
      "63 epoch Loss: 1.052735\n",
      "64 epoch Loss: 1.055834\n",
      "65 epoch Loss: 1.057199\n",
      "66 epoch Loss: 1.036279\n",
      "67 epoch Loss: 1.052054\n",
      "68 epoch Loss: 1.049480\n",
      "69 epoch Loss: 1.034714\n",
      "70 epoch Loss: 1.047620\n",
      "71 epoch Loss: 1.013436\n",
      "72 epoch Loss: 1.059899\n",
      "73 epoch Loss: 1.056679\n",
      "74 epoch Loss: 0.999974\n",
      "75 epoch Loss: 1.012003\n",
      "76 epoch Loss: 1.005923\n",
      "77 epoch Loss: 1.055206\n",
      "78 epoch Loss: 1.054889\n",
      "79 epoch Loss: 1.035821\n",
      "80 epoch Loss: 1.038071\n",
      "81 epoch Loss: 1.016620\n",
      "82 epoch Loss: 1.027072\n",
      "83 epoch Loss: 1.039037\n",
      "84 epoch Loss: 1.040663\n",
      "85 epoch Loss: 1.042850\n",
      "86 epoch Loss: 1.014930\n",
      "87 epoch Loss: 1.017162\n",
      "88 epoch Loss: 1.032819\n",
      "89 epoch Loss: 1.014758\n",
      "90 epoch Loss: 1.056782\n",
      "91 epoch Loss: 1.029347\n",
      "92 epoch Loss: 1.029297\n",
      "93 epoch Loss: 1.037197\n",
      "94 epoch Loss: 0.994058\n",
      "95 epoch Loss: 1.032307\n",
      "96 epoch Loss: 1.037160\n",
      "97 epoch Loss: 1.063584\n",
      "98 epoch Loss: 1.057043\n",
      "99 epoch Loss: 1.037817\n",
      "100 epoch Loss: 1.046281\n",
      "101 epoch Loss: 1.024173\n",
      "102 epoch Loss: 1.026538\n",
      "103 epoch Loss: 1.004160\n",
      "104 epoch Loss: 1.053805\n",
      "105 epoch Loss: 1.019272\n",
      "106 epoch Loss: 1.007330\n",
      "107 epoch Loss: 1.049367\n",
      "108 epoch Loss: 1.024283\n",
      "109 epoch Loss: 1.062247\n",
      "110 epoch Loss: 1.026720\n",
      "111 epoch Loss: 1.000923\n",
      "112 epoch Loss: 1.042694\n",
      "113 epoch Loss: 1.046854\n",
      "114 epoch Loss: 1.036938\n",
      "115 epoch Loss: 1.025737\n",
      "116 epoch Loss: 1.028121\n",
      "117 epoch Loss: 1.026009\n",
      "118 epoch Loss: 1.012492\n",
      "119 epoch Loss: 1.043481\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 79.285714, Random forests accuracy: 71.428571, Ensemble accuracy: 81.142857\n",
      "0 epoch Loss: 3.098353\n",
      "1 epoch Loss: 2.639168\n",
      "2 epoch Loss: 2.434768\n",
      "3 epoch Loss: 2.340164\n",
      "4 epoch Loss: 2.232027\n",
      "5 epoch Loss: 2.161127\n",
      "6 epoch Loss: 2.103353\n",
      "7 epoch Loss: 2.063458\n",
      "8 epoch Loss: 1.986990\n",
      "9 epoch Loss: 1.922172\n",
      "10 epoch Loss: 1.893857\n",
      "11 epoch Loss: 1.854733\n",
      "12 epoch Loss: 1.803937\n",
      "13 epoch Loss: 1.790094\n",
      "14 epoch Loss: 1.754950\n",
      "15 epoch Loss: 1.672720\n",
      "16 epoch Loss: 1.619906\n",
      "17 epoch Loss: 1.586215\n",
      "18 epoch Loss: 1.528887\n",
      "19 epoch Loss: 1.525065\n",
      "20 epoch Loss: 1.492250\n",
      "21 epoch Loss: 1.470312\n",
      "22 epoch Loss: 1.458247\n",
      "23 epoch Loss: 1.401365\n",
      "24 epoch Loss: 1.408126\n",
      "25 epoch Loss: 1.371624\n",
      "26 epoch Loss: 1.360246\n",
      "27 epoch Loss: 1.294799\n",
      "28 epoch Loss: 1.296731\n",
      "29 epoch Loss: 1.292545\n",
      "30 epoch Loss: 1.270300\n",
      "31 epoch Loss: 1.256224\n",
      "32 epoch Loss: 1.224597\n",
      "33 epoch Loss: 1.211988\n",
      "34 epoch Loss: 1.214789\n",
      "35 epoch Loss: 1.200446\n",
      "36 epoch Loss: 1.147026\n",
      "37 epoch Loss: 1.172913\n",
      "38 epoch Loss: 1.163850\n",
      "39 epoch Loss: 1.158356\n",
      "40 epoch Loss: 1.135404\n",
      "41 epoch Loss: 1.138270\n",
      "42 epoch Loss: 1.188128\n",
      "43 epoch Loss: 1.142876\n",
      "44 epoch Loss: 1.136209\n",
      "45 epoch Loss: 1.112106\n",
      "46 epoch Loss: 1.131217\n",
      "47 epoch Loss: 1.119640\n",
      "48 epoch Loss: 1.118361\n",
      "49 epoch Loss: 1.104353\n",
      "50 epoch Loss: 1.092799\n",
      "51 epoch Loss: 1.069329\n",
      "52 epoch Loss: 1.097341\n",
      "53 epoch Loss: 1.112670\n",
      "54 epoch Loss: 1.087363\n",
      "55 epoch Loss: 1.078802\n",
      "56 epoch Loss: 1.045304\n",
      "57 epoch Loss: 1.094179\n",
      "58 epoch Loss: 1.041917\n",
      "59 epoch Loss: 1.110301\n",
      "60 epoch Loss: 1.073543\n",
      "61 epoch Loss: 1.063018\n",
      "62 epoch Loss: 1.076834\n",
      "63 epoch Loss: 1.061169\n",
      "64 epoch Loss: 1.075859\n",
      "65 epoch Loss: 1.051493\n",
      "66 epoch Loss: 1.067262\n",
      "67 epoch Loss: 1.057681\n",
      "68 epoch Loss: 1.025161\n",
      "69 epoch Loss: 1.042620\n",
      "70 epoch Loss: 1.036801\n",
      "71 epoch Loss: 1.052464\n",
      "72 epoch Loss: 1.065051\n",
      "73 epoch Loss: 1.073740\n",
      "74 epoch Loss: 1.038027\n",
      "75 epoch Loss: 1.051457\n",
      "76 epoch Loss: 1.054152\n",
      "77 epoch Loss: 1.027898\n",
      "78 epoch Loss: 1.070294\n",
      "79 epoch Loss: 1.031180\n",
      "80 epoch Loss: 1.020472\n",
      "81 epoch Loss: 1.046923\n",
      "82 epoch Loss: 1.052840\n",
      "83 epoch Loss: 1.005532\n",
      "84 epoch Loss: 1.023164\n",
      "85 epoch Loss: 1.030431\n",
      "86 epoch Loss: 1.038520\n",
      "87 epoch Loss: 1.025658\n",
      "88 epoch Loss: 1.044459\n",
      "89 epoch Loss: 1.034905\n",
      "90 epoch Loss: 1.005877\n",
      "91 epoch Loss: 1.056239\n",
      "92 epoch Loss: 1.005678\n",
      "93 epoch Loss: 1.007829\n",
      "94 epoch Loss: 1.039729\n",
      "95 epoch Loss: 0.990667\n",
      "96 epoch Loss: 1.031808\n",
      "97 epoch Loss: 1.023530\n",
      "98 epoch Loss: 0.984419\n",
      "99 epoch Loss: 1.007055\n",
      "100 epoch Loss: 1.052127\n",
      "101 epoch Loss: 1.020456\n",
      "102 epoch Loss: 1.028688\n",
      "103 epoch Loss: 1.022847\n",
      "104 epoch Loss: 1.029504\n",
      "105 epoch Loss: 1.037610\n",
      "106 epoch Loss: 1.037799\n",
      "107 epoch Loss: 1.026113\n",
      "108 epoch Loss: 1.041852\n",
      "109 epoch Loss: 1.024516\n",
      "110 epoch Loss: 1.040380\n",
      "111 epoch Loss: 1.032271\n",
      "112 epoch Loss: 1.050979\n",
      "113 epoch Loss: 1.030723\n",
      "114 epoch Loss: 1.032717\n",
      "115 epoch Loss: 1.048783\n",
      "116 epoch Loss: 1.019660\n",
      "117 epoch Loss: 1.000832\n",
      "118 epoch Loss: 1.000340\n",
      "119 epoch Loss: 1.021011\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 78.285714, Random forests accuracy: 72.857143, Ensemble accuracy: 80.428571\n",
      "0 epoch Loss: 3.107225\n",
      "1 epoch Loss: 2.658951\n",
      "2 epoch Loss: 2.448837\n",
      "3 epoch Loss: 2.315885\n",
      "4 epoch Loss: 2.253402\n",
      "5 epoch Loss: 2.185845\n",
      "6 epoch Loss: 2.114013\n",
      "7 epoch Loss: 2.090119\n",
      "8 epoch Loss: 2.006085\n",
      "9 epoch Loss: 1.935822\n",
      "10 epoch Loss: 1.930654\n",
      "11 epoch Loss: 1.838938\n",
      "12 epoch Loss: 1.782245\n",
      "13 epoch Loss: 1.773889\n",
      "14 epoch Loss: 1.737484\n",
      "15 epoch Loss: 1.645312\n",
      "16 epoch Loss: 1.667810\n",
      "17 epoch Loss: 1.631642\n",
      "18 epoch Loss: 1.540505\n",
      "19 epoch Loss: 1.507487\n",
      "20 epoch Loss: 1.519093\n",
      "21 epoch Loss: 1.478605\n",
      "22 epoch Loss: 1.419735\n",
      "23 epoch Loss: 1.433171\n",
      "24 epoch Loss: 1.373721\n",
      "25 epoch Loss: 1.361130\n",
      "26 epoch Loss: 1.346649\n",
      "27 epoch Loss: 1.335969\n",
      "28 epoch Loss: 1.321578\n",
      "29 epoch Loss: 1.309070\n",
      "30 epoch Loss: 1.281972\n",
      "31 epoch Loss: 1.260998\n",
      "32 epoch Loss: 1.243249\n",
      "33 epoch Loss: 1.227086\n",
      "34 epoch Loss: 1.211417\n",
      "35 epoch Loss: 1.207691\n",
      "36 epoch Loss: 1.189756\n",
      "37 epoch Loss: 1.197600\n",
      "38 epoch Loss: 1.149738\n",
      "39 epoch Loss: 1.175149\n",
      "40 epoch Loss: 1.123828\n",
      "41 epoch Loss: 1.140439\n",
      "42 epoch Loss: 1.138407\n",
      "43 epoch Loss: 1.149354\n",
      "44 epoch Loss: 1.132895\n",
      "45 epoch Loss: 1.144888\n",
      "46 epoch Loss: 1.119614\n",
      "47 epoch Loss: 1.095520\n",
      "48 epoch Loss: 1.116387\n",
      "49 epoch Loss: 1.110911\n",
      "50 epoch Loss: 1.110707\n",
      "51 epoch Loss: 1.082864\n",
      "52 epoch Loss: 1.051878\n",
      "53 epoch Loss: 1.087817\n",
      "54 epoch Loss: 1.090121\n",
      "55 epoch Loss: 1.077300\n",
      "56 epoch Loss: 1.063981\n",
      "57 epoch Loss: 1.047483\n",
      "58 epoch Loss: 1.105637\n",
      "59 epoch Loss: 1.061966\n",
      "60 epoch Loss: 1.051796\n",
      "61 epoch Loss: 1.039239\n",
      "62 epoch Loss: 1.042446\n",
      "63 epoch Loss: 1.059863\n",
      "64 epoch Loss: 1.046099\n",
      "65 epoch Loss: 1.066447\n",
      "66 epoch Loss: 1.040600\n",
      "67 epoch Loss: 1.034224\n",
      "68 epoch Loss: 1.043401\n",
      "69 epoch Loss: 1.050963\n",
      "70 epoch Loss: 1.058355\n",
      "71 epoch Loss: 1.067186\n",
      "72 epoch Loss: 1.062009\n",
      "73 epoch Loss: 1.061880\n",
      "74 epoch Loss: 1.040027\n",
      "75 epoch Loss: 1.039282\n",
      "76 epoch Loss: 1.039058\n",
      "77 epoch Loss: 1.033642\n",
      "78 epoch Loss: 1.047812\n",
      "79 epoch Loss: 1.045318\n",
      "80 epoch Loss: 1.060131\n",
      "81 epoch Loss: 1.051152\n",
      "82 epoch Loss: 1.044099\n",
      "83 epoch Loss: 1.020137\n",
      "84 epoch Loss: 1.057880\n",
      "85 epoch Loss: 1.030169\n",
      "86 epoch Loss: 1.038243\n",
      "87 epoch Loss: 1.022452\n",
      "88 epoch Loss: 1.019666\n",
      "89 epoch Loss: 1.039682\n",
      "90 epoch Loss: 1.013019\n",
      "91 epoch Loss: 1.021925\n",
      "92 epoch Loss: 0.992301\n",
      "93 epoch Loss: 1.022562\n",
      "94 epoch Loss: 1.019894\n",
      "95 epoch Loss: 0.990912\n",
      "96 epoch Loss: 1.027715\n",
      "97 epoch Loss: 1.033943\n",
      "98 epoch Loss: 1.012733\n",
      "99 epoch Loss: 1.027818\n",
      "100 epoch Loss: 1.018366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 epoch Loss: 1.026934\n",
      "102 epoch Loss: 1.033193\n",
      "103 epoch Loss: 1.021565\n",
      "104 epoch Loss: 1.050575\n",
      "105 epoch Loss: 1.021151\n",
      "106 epoch Loss: 1.032406\n",
      "107 epoch Loss: 1.030612\n",
      "108 epoch Loss: 1.038260\n",
      "109 epoch Loss: 1.013158\n",
      "110 epoch Loss: 1.040054\n",
      "111 epoch Loss: 0.986673\n",
      "112 epoch Loss: 1.015466\n",
      "113 epoch Loss: 1.024130\n",
      "114 epoch Loss: 1.028192\n",
      "115 epoch Loss: 1.039162\n",
      "116 epoch Loss: 1.027259\n",
      "117 epoch Loss: 1.022991\n",
      "118 epoch Loss: 1.044685\n",
      "119 epoch Loss: 0.991400\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 79.142857, Random forests accuracy: 76.142857, Ensemble accuracy: 80.000000\n",
      "0 epoch Loss: 3.092706\n",
      "1 epoch Loss: 2.674574\n",
      "2 epoch Loss: 2.473862\n",
      "3 epoch Loss: 2.312465\n",
      "4 epoch Loss: 2.229769\n",
      "5 epoch Loss: 2.144419\n",
      "6 epoch Loss: 2.096243\n",
      "7 epoch Loss: 2.049141\n",
      "8 epoch Loss: 1.992454\n",
      "9 epoch Loss: 1.941604\n",
      "10 epoch Loss: 1.909385\n",
      "11 epoch Loss: 1.875473\n",
      "12 epoch Loss: 1.815857\n",
      "13 epoch Loss: 1.767686\n",
      "14 epoch Loss: 1.717684\n",
      "15 epoch Loss: 1.693780\n",
      "16 epoch Loss: 1.632386\n",
      "17 epoch Loss: 1.595965\n",
      "18 epoch Loss: 1.558861\n",
      "19 epoch Loss: 1.499683\n",
      "20 epoch Loss: 1.489065\n",
      "21 epoch Loss: 1.482812\n",
      "22 epoch Loss: 1.445891\n",
      "23 epoch Loss: 1.419037\n",
      "24 epoch Loss: 1.396886\n",
      "25 epoch Loss: 1.367348\n",
      "26 epoch Loss: 1.360255\n",
      "27 epoch Loss: 1.302934\n",
      "28 epoch Loss: 1.329167\n",
      "29 epoch Loss: 1.261988\n",
      "30 epoch Loss: 1.280152\n",
      "31 epoch Loss: 1.240365\n",
      "32 epoch Loss: 1.255707\n",
      "33 epoch Loss: 1.211923\n",
      "34 epoch Loss: 1.218610\n",
      "35 epoch Loss: 1.208192\n",
      "36 epoch Loss: 1.194322\n",
      "37 epoch Loss: 1.186918\n",
      "38 epoch Loss: 1.149486\n",
      "39 epoch Loss: 1.175932\n",
      "40 epoch Loss: 1.175653\n",
      "41 epoch Loss: 1.149877\n",
      "42 epoch Loss: 1.128512\n",
      "43 epoch Loss: 1.114409\n",
      "44 epoch Loss: 1.138635\n",
      "45 epoch Loss: 1.141406\n",
      "46 epoch Loss: 1.112953\n",
      "47 epoch Loss: 1.075520\n",
      "48 epoch Loss: 1.108747\n",
      "49 epoch Loss: 1.146852\n",
      "50 epoch Loss: 1.081922\n",
      "51 epoch Loss: 1.113087\n",
      "52 epoch Loss: 1.103208\n",
      "53 epoch Loss: 1.092015\n",
      "54 epoch Loss: 1.092704\n",
      "55 epoch Loss: 1.073894\n",
      "56 epoch Loss: 1.090789\n",
      "57 epoch Loss: 1.074575\n",
      "58 epoch Loss: 1.079768\n",
      "59 epoch Loss: 1.052378\n",
      "60 epoch Loss: 1.061252\n",
      "61 epoch Loss: 1.059484\n",
      "62 epoch Loss: 1.041982\n",
      "63 epoch Loss: 1.045101\n",
      "64 epoch Loss: 1.037891\n",
      "65 epoch Loss: 1.047823\n",
      "66 epoch Loss: 1.061697\n",
      "67 epoch Loss: 1.085100\n",
      "68 epoch Loss: 1.058050\n",
      "69 epoch Loss: 1.058499\n",
      "70 epoch Loss: 1.060032\n",
      "71 epoch Loss: 1.055859\n",
      "72 epoch Loss: 1.054855\n",
      "73 epoch Loss: 1.028009\n",
      "74 epoch Loss: 1.031885\n",
      "75 epoch Loss: 1.011713\n",
      "76 epoch Loss: 1.043755\n",
      "77 epoch Loss: 1.027820\n",
      "78 epoch Loss: 1.047460\n",
      "79 epoch Loss: 1.019742\n",
      "80 epoch Loss: 1.041686\n",
      "81 epoch Loss: 1.058428\n",
      "82 epoch Loss: 1.010576\n",
      "83 epoch Loss: 1.040789\n",
      "84 epoch Loss: 1.029442\n",
      "85 epoch Loss: 1.033346\n",
      "86 epoch Loss: 1.041724\n",
      "87 epoch Loss: 1.056594\n",
      "88 epoch Loss: 1.061924\n",
      "89 epoch Loss: 1.044164\n",
      "90 epoch Loss: 1.027220\n",
      "91 epoch Loss: 1.021055\n",
      "92 epoch Loss: 1.007625\n",
      "93 epoch Loss: 1.037712\n",
      "94 epoch Loss: 1.041830\n",
      "95 epoch Loss: 1.016477\n",
      "96 epoch Loss: 1.030415\n",
      "97 epoch Loss: 1.033490\n",
      "98 epoch Loss: 1.022115\n",
      "99 epoch Loss: 0.998996\n",
      "100 epoch Loss: 1.066300\n",
      "101 epoch Loss: 1.008896\n",
      "102 epoch Loss: 1.041117\n",
      "103 epoch Loss: 1.020860\n",
      "104 epoch Loss: 1.018787\n",
      "105 epoch Loss: 1.049997\n",
      "106 epoch Loss: 1.035151\n",
      "107 epoch Loss: 1.023596\n",
      "108 epoch Loss: 0.997239\n",
      "109 epoch Loss: 1.026699\n",
      "110 epoch Loss: 1.020428\n",
      "111 epoch Loss: 1.026493\n",
      "112 epoch Loss: 1.015438\n",
      "113 epoch Loss: 1.038553\n",
      "114 epoch Loss: 1.005783\n",
      "115 epoch Loss: 1.038982\n",
      "116 epoch Loss: 1.028465\n",
      "117 epoch Loss: 1.005872\n",
      "118 epoch Loss: 1.015790\n",
      "119 epoch Loss: 1.039211\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 78.857143, Random forests accuracy: 74.142857, Ensemble accuracy: 81.142857\n",
      "0 epoch Loss: 3.105224\n",
      "1 epoch Loss: 2.648577\n",
      "2 epoch Loss: 2.436715\n",
      "3 epoch Loss: 2.309682\n",
      "4 epoch Loss: 2.235021\n",
      "5 epoch Loss: 2.177825\n",
      "6 epoch Loss: 2.099478\n",
      "7 epoch Loss: 2.078502\n",
      "8 epoch Loss: 1.996466\n",
      "9 epoch Loss: 1.939320\n",
      "10 epoch Loss: 1.919938\n",
      "11 epoch Loss: 1.882576\n",
      "12 epoch Loss: 1.828508\n",
      "13 epoch Loss: 1.767329\n",
      "14 epoch Loss: 1.721238\n",
      "15 epoch Loss: 1.659722\n",
      "16 epoch Loss: 1.666876\n",
      "17 epoch Loss: 1.601898\n",
      "18 epoch Loss: 1.555488\n",
      "19 epoch Loss: 1.538334\n",
      "20 epoch Loss: 1.521011\n",
      "21 epoch Loss: 1.465038\n",
      "22 epoch Loss: 1.431494\n",
      "23 epoch Loss: 1.429081\n",
      "24 epoch Loss: 1.406878\n",
      "25 epoch Loss: 1.379982\n",
      "26 epoch Loss: 1.353307\n",
      "27 epoch Loss: 1.319647\n",
      "28 epoch Loss: 1.335166\n",
      "29 epoch Loss: 1.303098\n",
      "30 epoch Loss: 1.268788\n",
      "31 epoch Loss: 1.227382\n",
      "32 epoch Loss: 1.246412\n",
      "33 epoch Loss: 1.189642\n",
      "34 epoch Loss: 1.241271\n",
      "35 epoch Loss: 1.177840\n",
      "36 epoch Loss: 1.187664\n",
      "37 epoch Loss: 1.191322\n",
      "38 epoch Loss: 1.172306\n",
      "39 epoch Loss: 1.186844\n",
      "40 epoch Loss: 1.168478\n",
      "41 epoch Loss: 1.156499\n",
      "42 epoch Loss: 1.137273\n",
      "43 epoch Loss: 1.118075\n",
      "44 epoch Loss: 1.116265\n",
      "45 epoch Loss: 1.098896\n",
      "46 epoch Loss: 1.125451\n",
      "47 epoch Loss: 1.098595\n",
      "48 epoch Loss: 1.080259\n",
      "49 epoch Loss: 1.089006\n",
      "50 epoch Loss: 1.131406\n",
      "51 epoch Loss: 1.074273\n",
      "52 epoch Loss: 1.096204\n",
      "53 epoch Loss: 1.090735\n",
      "54 epoch Loss: 1.069567\n",
      "55 epoch Loss: 1.074955\n",
      "56 epoch Loss: 1.108683\n",
      "57 epoch Loss: 1.075295\n",
      "58 epoch Loss: 1.054561\n",
      "59 epoch Loss: 1.073063\n",
      "60 epoch Loss: 1.069208\n",
      "61 epoch Loss: 1.038117\n",
      "62 epoch Loss: 1.058848\n",
      "63 epoch Loss: 1.037436\n",
      "64 epoch Loss: 1.036655\n",
      "65 epoch Loss: 1.040215\n",
      "66 epoch Loss: 1.058819\n",
      "67 epoch Loss: 1.043559\n",
      "68 epoch Loss: 1.051902\n",
      "69 epoch Loss: 1.075173\n",
      "70 epoch Loss: 1.059412\n",
      "71 epoch Loss: 1.041703\n",
      "72 epoch Loss: 1.025708\n",
      "73 epoch Loss: 1.029703\n",
      "74 epoch Loss: 1.061104\n",
      "75 epoch Loss: 1.067555\n",
      "76 epoch Loss: 1.043264\n",
      "77 epoch Loss: 1.048573\n",
      "78 epoch Loss: 1.013586\n",
      "79 epoch Loss: 1.035695\n",
      "80 epoch Loss: 1.008557\n",
      "81 epoch Loss: 1.032894\n",
      "82 epoch Loss: 1.036219\n",
      "83 epoch Loss: 1.052989\n",
      "84 epoch Loss: 1.005885\n",
      "85 epoch Loss: 1.025534\n",
      "86 epoch Loss: 1.021540\n",
      "87 epoch Loss: 1.011258\n",
      "88 epoch Loss: 1.029108\n",
      "89 epoch Loss: 1.031418\n",
      "90 epoch Loss: 1.044387\n",
      "91 epoch Loss: 1.039088\n",
      "92 epoch Loss: 1.032085\n",
      "93 epoch Loss: 1.046896\n",
      "94 epoch Loss: 1.065059\n",
      "95 epoch Loss: 1.056735\n",
      "96 epoch Loss: 1.025873\n",
      "97 epoch Loss: 1.016053\n",
      "98 epoch Loss: 1.035556\n",
      "99 epoch Loss: 1.032820\n",
      "100 epoch Loss: 1.029601\n",
      "101 epoch Loss: 1.045582\n",
      "102 epoch Loss: 1.006473\n",
      "103 epoch Loss: 1.045336\n",
      "104 epoch Loss: 1.023958\n",
      "105 epoch Loss: 1.014819\n",
      "106 epoch Loss: 1.003646\n",
      "107 epoch Loss: 1.048063\n",
      "108 epoch Loss: 1.030781\n",
      "109 epoch Loss: 1.030998\n",
      "110 epoch Loss: 1.040678\n",
      "111 epoch Loss: 1.020697\n",
      "112 epoch Loss: 1.020893\n",
      "113 epoch Loss: 1.040369\n",
      "114 epoch Loss: 1.015141\n",
      "115 epoch Loss: 1.032200\n",
      "116 epoch Loss: 1.026695\n",
      "117 epoch Loss: 1.039803\n",
      "118 epoch Loss: 1.011531\n",
      "119 epoch Loss: 0.998026\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.207977, Random forests accuracy: 71.082621, Ensemble accuracy: 79.202279\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#Load data\n",
    "\n",
    "    \n",
    "    #Initialize\n",
    "    label -= 1\n",
    "    np.random.seed(2018)\n",
    "\n",
    "\n",
    "    \n",
    "    t_index = np.random.permutation(int(np.shape(data)[0]/Outer_loop)*Outer_loop)\n",
    "    t_index = np.reshape(t_index, [Outer_loop, -1])\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    box = np.array([4000], dtype=np.int32)\n",
    "    flag = 0\n",
    "    for test_index in t_index:\n",
    "        if flag == Outer_loop-1:\n",
    "            test_index = np.array(np.concatenate((test_index, np.array(range(int(np.shape(data)[0]/Outer_loop)*Outer_loop,np.shape(data)[0]))), axis=0), dtype=np.int32)\n",
    "        train_index = np.setdiff1d(np.array(range(0,np.shape(data)[0])), test_index)\n",
    "        train_data = data[train_index]\n",
    "        train_label = label[train_index]\n",
    "        test_data = data[test_index]\n",
    "        test_label = label[test_index]\n",
    "        \n",
    "        kf = np.random.permutation(int(np.shape(train_data)[0]/Inner_loop)*Inner_loop)\n",
    "        kf = kf.reshape([Inner_loop]+[-1])\n",
    "        val_result = np.zeros([np.shape(train_data)[0],48], dtype=np.float32)\n",
    "        \n",
    "        tot_acc = np.zeros([Inner_loop,5], dtype=np.float32)\n",
    "        #lasso = Lasso()\n",
    "        #lsvc = LinearSVC(C=1, penalty=\"l1\", dual=False).fit(data, label)\n",
    "        #coef = np.squeeze(np.sum(np.square(np.array(lsvc.coef_)), axis=0))\n",
    "        #coef = np.squeeze(np.sum(np.square(np.array(lasso.coef_)), axis=0))\n",
    "        #coefidx = np.argsort(coef)\n",
    "#         for inner_fold in range(0,Inner_loop):\n",
    "#             val_test_ind = kf[inner_fold]\n",
    "#             if inner_fold == Inner_loop-1:\n",
    "#                 val_test_ind = np.array(np.concatenate((val_test_ind,np.array(range(int(np.shape(train_data)[0]/Outer_loop)*Outer_loop,np.shape(train_data)[0]),dtype=np.int32)), axis=0),dtype=np.int32)\n",
    "            \n",
    "#             val_train_ind = np.setdiff1d(np.array(range(0,np.shape(train_data)[0]),dtype=np.int32), val_test_ind)\n",
    "#             val_train = train_data[val_train_ind]\n",
    "#             val_test = train_data[val_test_ind]\n",
    "#             val_train_label = train_label[val_train_ind]\n",
    "#             val_test_label = train_label[val_test_ind]\n",
    "#             temp = 0\n",
    "#             for item in box:\n",
    "#                 idx = coefidx[-item:]\n",
    "#                 vtrain = val_train[:,idx]\n",
    "#                 vtest = val_test[:,idx]\n",
    "#                 nn_acc, result_nn = dnn(vtrain, val_train_label, vtest, val_test_label)\n",
    "#                 rf_acc, result_rf = rfc(vtrain, val_train_label, vtest, val_test_label)\n",
    "#                 en_acc = 0.0\n",
    "#                 for i in range(0,np.shape(vtest)[0]):\n",
    "#                     r = np.argmax(result_nn[i]+result_rf[i])\n",
    "#                     if r == val_test_label[i]:\n",
    "#                         en_acc += 1\n",
    "#                 en_acc /= np.shape(vtest)[0]*0.01\n",
    "#                 tot_acc[inner_fold,temp] = en_acc\n",
    "#                 print(\"Inner_fold # of features: %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (item, nn_acc, rf_acc, en_acc))\n",
    "#                 temp += 1\n",
    "        \n",
    "        u = np.sum(tot_acc,0)\n",
    "       \n",
    "        best_n = box[np.argmax(u)]\n",
    "#        idx = coefidx[-best_n:]\n",
    "        \n",
    "#        tr_data = train_data[:,idx]\n",
    "#        te_data = test_data[:,idx]\n",
    "        nn_acc, result_nn = dnn(train_data, train_label, test_data, test_label)\n",
    "        rf_acc, result_rf = rfc(train_data, train_label, test_data, test_label)\n",
    "        en_acc = 0.0\n",
    "        for i in range(0,np.shape(test_data)[0]):\n",
    "            r = np.argmax(result_nn[i]+result_rf[i])\n",
    "            if r == test_label[i]:\n",
    "                en_acc += 1\n",
    "        en_acc /= np.shape(test_data)[0]*0.01\n",
    "        print(\"Outer_fold # of features:  %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (best_n, nn_acc, rf_acc, en_acc))\n",
    "        flag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_35565=[75.555556,74.761905,78.730159,80.158730,77.301587,74.920635,78.095238,78.571429,76.984127,75.316456]\n",
    "# RF_35565=[70.634921,72.222222,73.333333,74.761905,73.492063,73.174603,73.333333,71.904762,73.809524,72.468354]\n",
    "# emsemble_35565=[77.619048,78.571429,82.222222,82.539683,81.269841,77.301587,79.206349,80.793651,79.047619,77.848101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy':acc,'Model':md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\",rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.boxplot(x = \"Model\", y = \"Accuracy\", data = df, palette=\"Set3\", width=0.6, fliersize=2)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.310719\n",
      "1 epoch Loss: 3.057256\n",
      "2 epoch Loss: 2.842020\n",
      "3 epoch Loss: 2.679981\n",
      "4 epoch Loss: 2.518098\n",
      "5 epoch Loss: 2.460461\n",
      "6 epoch Loss: 2.322171\n",
      "7 epoch Loss: 2.276983\n",
      "8 epoch Loss: 2.190795\n",
      "9 epoch Loss: 2.159150\n",
      "10 epoch Loss: 2.060299\n",
      "11 epoch Loss: 2.058204\n",
      "12 epoch Loss: 1.976052\n",
      "13 epoch Loss: 1.913965\n",
      "14 epoch Loss: 1.945198\n",
      "15 epoch Loss: 1.869561\n",
      "16 epoch Loss: 1.817971\n",
      "17 epoch Loss: 1.792062\n",
      "18 epoch Loss: 1.741372\n",
      "19 epoch Loss: 1.715327\n",
      "20 epoch Loss: 1.644492\n",
      "21 epoch Loss: 1.659347\n",
      "22 epoch Loss: 1.634921\n",
      "23 epoch Loss: 1.588886\n",
      "24 epoch Loss: 1.562715\n",
      "25 epoch Loss: 1.513573\n",
      "26 epoch Loss: 1.511111\n",
      "27 epoch Loss: 1.493179\n",
      "28 epoch Loss: 1.429190\n",
      "29 epoch Loss: 1.448614\n",
      "30 epoch Loss: 1.452646\n",
      "31 epoch Loss: 1.440261\n",
      "32 epoch Loss: 1.381558\n",
      "33 epoch Loss: 1.366764\n",
      "34 epoch Loss: 1.337785\n",
      "35 epoch Loss: 1.370940\n",
      "36 epoch Loss: 1.320124\n",
      "37 epoch Loss: 1.287493\n",
      "38 epoch Loss: 1.285838\n",
      "39 epoch Loss: 1.245378\n",
      "40 epoch Loss: 1.262948\n",
      "41 epoch Loss: 1.283731\n",
      "42 epoch Loss: 1.255452\n",
      "43 epoch Loss: 1.219838\n",
      "44 epoch Loss: 1.233725\n",
      "45 epoch Loss: 1.225268\n",
      "46 epoch Loss: 1.191419\n",
      "47 epoch Loss: 1.187695\n",
      "48 epoch Loss: 1.171478\n",
      "49 epoch Loss: 1.137155\n",
      "50 epoch Loss: 1.170212\n",
      "51 epoch Loss: 1.168314\n",
      "52 epoch Loss: 1.152394\n",
      "53 epoch Loss: 1.166515\n",
      "54 epoch Loss: 1.133203\n",
      "55 epoch Loss: 1.111066\n",
      "56 epoch Loss: 1.156349\n",
      "57 epoch Loss: 1.122876\n",
      "58 epoch Loss: 1.110026\n",
      "59 epoch Loss: 1.131478\n",
      "60 epoch Loss: 1.102238\n",
      "61 epoch Loss: 1.086660\n",
      "62 epoch Loss: 1.088844\n",
      "63 epoch Loss: 1.118443\n",
      "64 epoch Loss: 1.080913\n",
      "65 epoch Loss: 1.082119\n",
      "66 epoch Loss: 1.091523\n",
      "67 epoch Loss: 1.083335\n",
      "68 epoch Loss: 1.088730\n",
      "69 epoch Loss: 1.080838\n",
      "70 epoch Loss: 1.059235\n",
      "71 epoch Loss: 1.075696\n",
      "72 epoch Loss: 1.076714\n",
      "73 epoch Loss: 1.080571\n",
      "74 epoch Loss: 1.063852\n",
      "75 epoch Loss: 1.032276\n",
      "76 epoch Loss: 1.071250\n",
      "77 epoch Loss: 1.043454\n",
      "78 epoch Loss: 1.040813\n",
      "79 epoch Loss: 1.032642\n",
      "80 epoch Loss: 1.036819\n",
      "81 epoch Loss: 1.033579\n",
      "82 epoch Loss: 1.075852\n",
      "83 epoch Loss: 1.041878\n",
      "84 epoch Loss: 1.020973\n",
      "85 epoch Loss: 1.061661\n",
      "86 epoch Loss: 1.029901\n",
      "87 epoch Loss: 1.071656\n",
      "88 epoch Loss: 1.047689\n",
      "89 epoch Loss: 1.047285\n",
      "90 epoch Loss: 1.048309\n",
      "91 epoch Loss: 1.045349\n",
      "92 epoch Loss: 1.024190\n",
      "93 epoch Loss: 1.014808\n",
      "94 epoch Loss: 1.046853\n",
      "95 epoch Loss: 1.033730\n",
      "96 epoch Loss: 1.019850\n",
      "97 epoch Loss: 1.010313\n",
      "98 epoch Loss: 1.026790\n",
      "99 epoch Loss: 1.033650\n",
      "100 epoch Loss: 1.022983\n",
      "101 epoch Loss: 1.020104\n",
      "102 epoch Loss: 1.019692\n",
      "103 epoch Loss: 1.012219\n",
      "104 epoch Loss: 1.006782\n",
      "105 epoch Loss: 1.021862\n",
      "106 epoch Loss: 1.022689\n",
      "107 epoch Loss: 1.051842\n",
      "108 epoch Loss: 1.034145\n",
      "109 epoch Loss: 1.037185\n",
      "110 epoch Loss: 1.011716\n",
      "111 epoch Loss: 1.008016\n",
      "112 epoch Loss: 1.015478\n",
      "113 epoch Loss: 1.026249\n",
      "114 epoch Loss: 1.043693\n",
      "115 epoch Loss: 1.030272\n",
      "116 epoch Loss: 1.048500\n",
      "117 epoch Loss: 1.017187\n",
      "118 epoch Loss: 1.041455\n",
      "119 epoch Loss: 1.006736\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.142857, Random forests accuracy: 74.857143, Ensemble accuracy: 85.714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.299424\n",
      "1 epoch Loss: 3.062167\n",
      "2 epoch Loss: 2.857814\n",
      "3 epoch Loss: 2.650655\n",
      "4 epoch Loss: 2.540277\n",
      "5 epoch Loss: 2.444268\n",
      "6 epoch Loss: 2.340232\n",
      "7 epoch Loss: 2.267813\n",
      "8 epoch Loss: 2.188918\n",
      "9 epoch Loss: 2.155684\n",
      "10 epoch Loss: 2.089480\n",
      "11 epoch Loss: 2.062339\n",
      "12 epoch Loss: 1.999219\n",
      "13 epoch Loss: 1.939038\n",
      "14 epoch Loss: 1.901223\n",
      "15 epoch Loss: 1.860395\n",
      "16 epoch Loss: 1.841956\n",
      "17 epoch Loss: 1.780767\n",
      "18 epoch Loss: 1.705276\n",
      "19 epoch Loss: 1.711139\n",
      "20 epoch Loss: 1.688436\n",
      "21 epoch Loss: 1.646381\n",
      "22 epoch Loss: 1.650312\n",
      "23 epoch Loss: 1.571922\n",
      "24 epoch Loss: 1.568630\n",
      "25 epoch Loss: 1.517843\n",
      "26 epoch Loss: 1.508607\n",
      "27 epoch Loss: 1.508571\n",
      "28 epoch Loss: 1.467395\n",
      "29 epoch Loss: 1.452396\n",
      "30 epoch Loss: 1.428513\n",
      "31 epoch Loss: 1.398368\n",
      "32 epoch Loss: 1.412186\n",
      "33 epoch Loss: 1.383392\n",
      "34 epoch Loss: 1.355056\n",
      "35 epoch Loss: 1.364988\n",
      "36 epoch Loss: 1.352904\n",
      "37 epoch Loss: 1.358970\n",
      "38 epoch Loss: 1.288204\n",
      "39 epoch Loss: 1.300534\n",
      "40 epoch Loss: 1.305390\n",
      "41 epoch Loss: 1.290972\n",
      "42 epoch Loss: 1.244797\n",
      "43 epoch Loss: 1.234481\n",
      "44 epoch Loss: 1.228014\n",
      "45 epoch Loss: 1.201970\n",
      "46 epoch Loss: 1.216864\n",
      "47 epoch Loss: 1.211889\n",
      "48 epoch Loss: 1.210457\n",
      "49 epoch Loss: 1.174193\n",
      "50 epoch Loss: 1.185286\n",
      "51 epoch Loss: 1.180340\n",
      "52 epoch Loss: 1.153614\n",
      "53 epoch Loss: 1.181857\n",
      "54 epoch Loss: 1.155063\n",
      "55 epoch Loss: 1.128161\n",
      "56 epoch Loss: 1.118938\n",
      "57 epoch Loss: 1.120123\n",
      "58 epoch Loss: 1.105596\n",
      "59 epoch Loss: 1.119586\n",
      "60 epoch Loss: 1.121641\n",
      "61 epoch Loss: 1.077719\n",
      "62 epoch Loss: 1.111983\n",
      "63 epoch Loss: 1.098786\n",
      "64 epoch Loss: 1.111748\n",
      "65 epoch Loss: 1.093320\n",
      "66 epoch Loss: 1.099916\n",
      "67 epoch Loss: 1.058952\n",
      "68 epoch Loss: 1.115578\n",
      "69 epoch Loss: 1.045198\n",
      "70 epoch Loss: 1.059724\n",
      "71 epoch Loss: 1.057018\n",
      "72 epoch Loss: 1.041436\n",
      "73 epoch Loss: 1.065830\n",
      "74 epoch Loss: 1.069061\n",
      "75 epoch Loss: 1.075720\n",
      "76 epoch Loss: 1.048943\n",
      "77 epoch Loss: 1.074621\n",
      "78 epoch Loss: 1.059271\n",
      "79 epoch Loss: 1.077728\n",
      "80 epoch Loss: 1.043528\n",
      "81 epoch Loss: 1.050420\n",
      "82 epoch Loss: 1.030019\n",
      "83 epoch Loss: 1.031270\n",
      "84 epoch Loss: 1.014002\n",
      "85 epoch Loss: 1.008886\n",
      "86 epoch Loss: 1.048129\n",
      "87 epoch Loss: 1.028836\n",
      "88 epoch Loss: 1.046412\n",
      "89 epoch Loss: 1.044609\n",
      "90 epoch Loss: 1.056470\n",
      "91 epoch Loss: 1.037031\n",
      "92 epoch Loss: 1.048817\n",
      "93 epoch Loss: 1.024988\n",
      "94 epoch Loss: 1.011188\n",
      "95 epoch Loss: 1.019432\n",
      "96 epoch Loss: 1.034532\n",
      "97 epoch Loss: 1.010636\n",
      "98 epoch Loss: 1.038268\n",
      "99 epoch Loss: 1.020529\n",
      "100 epoch Loss: 1.050814\n",
      "101 epoch Loss: 1.024968\n",
      "102 epoch Loss: 1.028733\n",
      "103 epoch Loss: 1.045499\n",
      "104 epoch Loss: 1.047804\n",
      "105 epoch Loss: 1.041386\n",
      "106 epoch Loss: 1.032213\n",
      "107 epoch Loss: 1.006788\n",
      "108 epoch Loss: 1.030310\n",
      "109 epoch Loss: 1.025514\n",
      "110 epoch Loss: 1.019787\n",
      "111 epoch Loss: 1.026533\n",
      "112 epoch Loss: 1.016586\n",
      "113 epoch Loss: 1.039489\n",
      "114 epoch Loss: 1.022479\n",
      "115 epoch Loss: 1.008078\n",
      "116 epoch Loss: 1.044860\n",
      "117 epoch Loss: 1.030627\n",
      "118 epoch Loss: 1.031197\n",
      "119 epoch Loss: 1.032500\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.428571, Random forests accuracy: 74.857143, Ensemble accuracy: 86.714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.305181\n",
      "1 epoch Loss: 3.081078\n",
      "2 epoch Loss: 2.849780\n",
      "3 epoch Loss: 2.662886\n",
      "4 epoch Loss: 2.540391\n",
      "5 epoch Loss: 2.439088\n",
      "6 epoch Loss: 2.358856\n",
      "7 epoch Loss: 2.268872\n",
      "8 epoch Loss: 2.217098\n",
      "9 epoch Loss: 2.169975\n",
      "10 epoch Loss: 2.081108\n",
      "11 epoch Loss: 2.049602\n",
      "12 epoch Loss: 1.992225\n",
      "13 epoch Loss: 1.904408\n",
      "14 epoch Loss: 1.933161\n",
      "15 epoch Loss: 1.859438\n",
      "16 epoch Loss: 1.831039\n",
      "17 epoch Loss: 1.763823\n",
      "18 epoch Loss: 1.727908\n",
      "19 epoch Loss: 1.711636\n",
      "20 epoch Loss: 1.672765\n",
      "21 epoch Loss: 1.633638\n",
      "22 epoch Loss: 1.654721\n",
      "23 epoch Loss: 1.592889\n",
      "24 epoch Loss: 1.596395\n",
      "25 epoch Loss: 1.524454\n",
      "26 epoch Loss: 1.517944\n",
      "27 epoch Loss: 1.512854\n",
      "28 epoch Loss: 1.453163\n",
      "29 epoch Loss: 1.444536\n",
      "30 epoch Loss: 1.436003\n",
      "31 epoch Loss: 1.416157\n",
      "32 epoch Loss: 1.402117\n",
      "33 epoch Loss: 1.407093\n",
      "34 epoch Loss: 1.373454\n",
      "35 epoch Loss: 1.340259\n",
      "36 epoch Loss: 1.356929\n",
      "37 epoch Loss: 1.327682\n",
      "38 epoch Loss: 1.310913\n",
      "39 epoch Loss: 1.283607\n",
      "40 epoch Loss: 1.289683\n",
      "41 epoch Loss: 1.262390\n",
      "42 epoch Loss: 1.271442\n",
      "43 epoch Loss: 1.232039\n",
      "44 epoch Loss: 1.219849\n",
      "45 epoch Loss: 1.233826\n",
      "46 epoch Loss: 1.188918\n",
      "47 epoch Loss: 1.198371\n",
      "48 epoch Loss: 1.200108\n",
      "49 epoch Loss: 1.192398\n",
      "50 epoch Loss: 1.193789\n",
      "51 epoch Loss: 1.174183\n",
      "52 epoch Loss: 1.157731\n",
      "53 epoch Loss: 1.169853\n",
      "54 epoch Loss: 1.170614\n",
      "55 epoch Loss: 1.141836\n",
      "56 epoch Loss: 1.150281\n",
      "57 epoch Loss: 1.122116\n",
      "58 epoch Loss: 1.134704\n",
      "59 epoch Loss: 1.137609\n",
      "60 epoch Loss: 1.111991\n",
      "61 epoch Loss: 1.094444\n",
      "62 epoch Loss: 1.105759\n",
      "63 epoch Loss: 1.091670\n",
      "64 epoch Loss: 1.082659\n",
      "65 epoch Loss: 1.059197\n",
      "66 epoch Loss: 1.103785\n",
      "67 epoch Loss: 1.054491\n",
      "68 epoch Loss: 1.088853\n",
      "69 epoch Loss: 1.081525\n",
      "70 epoch Loss: 1.066085\n",
      "71 epoch Loss: 1.063939\n",
      "72 epoch Loss: 1.092991\n",
      "73 epoch Loss: 1.056158\n",
      "74 epoch Loss: 1.084871\n",
      "75 epoch Loss: 1.038515\n",
      "76 epoch Loss: 1.067052\n",
      "77 epoch Loss: 1.077590\n",
      "78 epoch Loss: 1.066539\n",
      "79 epoch Loss: 1.030011\n",
      "80 epoch Loss: 1.047308\n",
      "81 epoch Loss: 1.038139\n",
      "82 epoch Loss: 1.055477\n",
      "83 epoch Loss: 1.051346\n",
      "84 epoch Loss: 1.037259\n",
      "85 epoch Loss: 1.056837\n",
      "86 epoch Loss: 1.050637\n",
      "87 epoch Loss: 1.012532\n",
      "88 epoch Loss: 1.050852\n",
      "89 epoch Loss: 1.055360\n",
      "90 epoch Loss: 1.053220\n",
      "91 epoch Loss: 1.026310\n",
      "92 epoch Loss: 1.041046\n",
      "93 epoch Loss: 1.030724\n",
      "94 epoch Loss: 1.039727\n",
      "95 epoch Loss: 1.054783\n",
      "96 epoch Loss: 1.017918\n",
      "97 epoch Loss: 0.998962\n",
      "98 epoch Loss: 1.021137\n",
      "99 epoch Loss: 1.031824\n",
      "100 epoch Loss: 1.026298\n",
      "101 epoch Loss: 1.008031\n",
      "102 epoch Loss: 1.031342\n",
      "103 epoch Loss: 1.041466\n",
      "104 epoch Loss: 1.049575\n",
      "105 epoch Loss: 1.035511\n",
      "106 epoch Loss: 1.009440\n",
      "107 epoch Loss: 1.044209\n",
      "108 epoch Loss: 1.030105\n",
      "109 epoch Loss: 0.990069\n",
      "110 epoch Loss: 1.024473\n",
      "111 epoch Loss: 1.021067\n",
      "112 epoch Loss: 1.036055\n",
      "113 epoch Loss: 1.027958\n",
      "114 epoch Loss: 1.025622\n",
      "115 epoch Loss: 1.007706\n",
      "116 epoch Loss: 1.008937\n",
      "117 epoch Loss: 1.027487\n",
      "118 epoch Loss: 1.003069\n",
      "119 epoch Loss: 1.003836\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 83.285714, Random forests accuracy: 73.285714, Ensemble accuracy: 85.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.301729\n",
      "1 epoch Loss: 3.059731\n",
      "2 epoch Loss: 2.848877\n",
      "3 epoch Loss: 2.654081\n",
      "4 epoch Loss: 2.549233\n",
      "5 epoch Loss: 2.483818\n",
      "6 epoch Loss: 2.353323\n",
      "7 epoch Loss: 2.292512\n",
      "8 epoch Loss: 2.209664\n",
      "9 epoch Loss: 2.138418\n",
      "10 epoch Loss: 2.078721\n",
      "11 epoch Loss: 2.068344\n",
      "12 epoch Loss: 2.023943\n",
      "13 epoch Loss: 1.936435\n",
      "14 epoch Loss: 1.906122\n",
      "15 epoch Loss: 1.845803\n",
      "16 epoch Loss: 1.815241\n",
      "17 epoch Loss: 1.777111\n",
      "18 epoch Loss: 1.759096\n",
      "19 epoch Loss: 1.731192\n",
      "20 epoch Loss: 1.697060\n",
      "21 epoch Loss: 1.658665\n",
      "22 epoch Loss: 1.606579\n",
      "23 epoch Loss: 1.589828\n",
      "24 epoch Loss: 1.565210\n",
      "25 epoch Loss: 1.560665\n",
      "26 epoch Loss: 1.524722\n",
      "27 epoch Loss: 1.502273\n",
      "28 epoch Loss: 1.487242\n",
      "29 epoch Loss: 1.480385\n",
      "30 epoch Loss: 1.434890\n",
      "31 epoch Loss: 1.430096\n",
      "32 epoch Loss: 1.399053\n",
      "33 epoch Loss: 1.408993\n",
      "34 epoch Loss: 1.365594\n",
      "35 epoch Loss: 1.320412\n",
      "36 epoch Loss: 1.346560\n",
      "37 epoch Loss: 1.314339\n",
      "38 epoch Loss: 1.259795\n",
      "39 epoch Loss: 1.289956\n",
      "40 epoch Loss: 1.299226\n",
      "41 epoch Loss: 1.286077\n",
      "42 epoch Loss: 1.225361\n",
      "43 epoch Loss: 1.258177\n",
      "44 epoch Loss: 1.235628\n",
      "45 epoch Loss: 1.223038\n",
      "46 epoch Loss: 1.240060\n",
      "47 epoch Loss: 1.208751\n",
      "48 epoch Loss: 1.187656\n",
      "49 epoch Loss: 1.169607\n",
      "50 epoch Loss: 1.203523\n",
      "51 epoch Loss: 1.150991\n",
      "52 epoch Loss: 1.172078\n",
      "53 epoch Loss: 1.142416\n",
      "54 epoch Loss: 1.152187\n",
      "55 epoch Loss: 1.148339\n",
      "56 epoch Loss: 1.142148\n",
      "57 epoch Loss: 1.136811\n",
      "58 epoch Loss: 1.105363\n",
      "59 epoch Loss: 1.145796\n",
      "60 epoch Loss: 1.107538\n",
      "61 epoch Loss: 1.106249\n",
      "62 epoch Loss: 1.090953\n",
      "63 epoch Loss: 1.115386\n",
      "64 epoch Loss: 1.082138\n",
      "65 epoch Loss: 1.090974\n",
      "66 epoch Loss: 1.090983\n",
      "67 epoch Loss: 1.092963\n",
      "68 epoch Loss: 1.062656\n",
      "69 epoch Loss: 1.045854\n",
      "70 epoch Loss: 1.047301\n",
      "71 epoch Loss: 1.040647\n",
      "72 epoch Loss: 1.052961\n",
      "73 epoch Loss: 1.062640\n",
      "74 epoch Loss: 1.067122\n",
      "75 epoch Loss: 1.049008\n",
      "76 epoch Loss: 1.046402\n",
      "77 epoch Loss: 1.072588\n",
      "78 epoch Loss: 1.051302\n",
      "79 epoch Loss: 1.046279\n",
      "80 epoch Loss: 1.056711\n",
      "81 epoch Loss: 1.045534\n",
      "82 epoch Loss: 1.053483\n",
      "83 epoch Loss: 1.036120\n",
      "84 epoch Loss: 1.011126\n",
      "85 epoch Loss: 1.024811\n",
      "86 epoch Loss: 1.048483\n",
      "87 epoch Loss: 1.045746\n",
      "88 epoch Loss: 1.023298\n",
      "89 epoch Loss: 1.044171\n",
      "90 epoch Loss: 1.022889\n",
      "91 epoch Loss: 1.047142\n",
      "92 epoch Loss: 1.054950\n",
      "93 epoch Loss: 1.069904\n",
      "94 epoch Loss: 1.054246\n",
      "95 epoch Loss: 1.038801\n",
      "96 epoch Loss: 1.032564\n",
      "97 epoch Loss: 1.053203\n",
      "98 epoch Loss: 1.039899\n",
      "99 epoch Loss: 1.028818\n",
      "100 epoch Loss: 1.046711\n",
      "101 epoch Loss: 1.047247\n",
      "102 epoch Loss: 1.026904\n",
      "103 epoch Loss: 1.023314\n",
      "104 epoch Loss: 1.004853\n",
      "105 epoch Loss: 1.030885\n",
      "106 epoch Loss: 1.030207\n",
      "107 epoch Loss: 1.002292\n",
      "108 epoch Loss: 0.990996\n",
      "109 epoch Loss: 1.040196\n",
      "110 epoch Loss: 1.038731\n",
      "111 epoch Loss: 1.003902\n",
      "112 epoch Loss: 1.022397\n",
      "113 epoch Loss: 1.029161\n",
      "114 epoch Loss: 1.043724\n",
      "115 epoch Loss: 1.011670\n",
      "116 epoch Loss: 1.023116\n",
      "117 epoch Loss: 1.023940\n",
      "118 epoch Loss: 1.036521\n",
      "119 epoch Loss: 1.019117\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.000000, Random forests accuracy: 72.285714, Ensemble accuracy: 85.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.301092\n",
      "1 epoch Loss: 3.058680\n",
      "2 epoch Loss: 2.859800\n",
      "3 epoch Loss: 2.686576\n",
      "4 epoch Loss: 2.514249\n",
      "5 epoch Loss: 2.459444\n",
      "6 epoch Loss: 2.341753\n",
      "7 epoch Loss: 2.273892\n",
      "8 epoch Loss: 2.217465\n",
      "9 epoch Loss: 2.150462\n",
      "10 epoch Loss: 2.086863\n",
      "11 epoch Loss: 2.086723\n",
      "12 epoch Loss: 1.993728\n",
      "13 epoch Loss: 1.957869\n",
      "14 epoch Loss: 1.912502\n",
      "15 epoch Loss: 1.838401\n",
      "16 epoch Loss: 1.824266\n",
      "17 epoch Loss: 1.812723\n",
      "18 epoch Loss: 1.733134\n",
      "19 epoch Loss: 1.707964\n",
      "20 epoch Loss: 1.682826\n",
      "21 epoch Loss: 1.648313\n",
      "22 epoch Loss: 1.642080\n",
      "23 epoch Loss: 1.629427\n",
      "24 epoch Loss: 1.595382\n",
      "25 epoch Loss: 1.543376\n",
      "26 epoch Loss: 1.519906\n",
      "27 epoch Loss: 1.549699\n",
      "28 epoch Loss: 1.495097\n",
      "29 epoch Loss: 1.459153\n",
      "30 epoch Loss: 1.406699\n",
      "31 epoch Loss: 1.398858\n",
      "32 epoch Loss: 1.415119\n",
      "33 epoch Loss: 1.374470\n",
      "34 epoch Loss: 1.375785\n",
      "35 epoch Loss: 1.346347\n",
      "36 epoch Loss: 1.362413\n",
      "37 epoch Loss: 1.332418\n",
      "38 epoch Loss: 1.323151\n",
      "39 epoch Loss: 1.306538\n",
      "40 epoch Loss: 1.271507\n",
      "41 epoch Loss: 1.291846\n",
      "42 epoch Loss: 1.279212\n",
      "43 epoch Loss: 1.252521\n",
      "44 epoch Loss: 1.240765\n",
      "45 epoch Loss: 1.211674\n",
      "46 epoch Loss: 1.192196\n",
      "47 epoch Loss: 1.206383\n",
      "48 epoch Loss: 1.200918\n",
      "49 epoch Loss: 1.169274\n",
      "50 epoch Loss: 1.187537\n",
      "51 epoch Loss: 1.198979\n",
      "52 epoch Loss: 1.144571\n",
      "53 epoch Loss: 1.138775\n",
      "54 epoch Loss: 1.181481\n",
      "55 epoch Loss: 1.130428\n",
      "56 epoch Loss: 1.137699\n",
      "57 epoch Loss: 1.133395\n",
      "58 epoch Loss: 1.100258\n",
      "59 epoch Loss: 1.116299\n",
      "60 epoch Loss: 1.131165\n",
      "61 epoch Loss: 1.143285\n",
      "62 epoch Loss: 1.100760\n",
      "63 epoch Loss: 1.071175\n",
      "64 epoch Loss: 1.071159\n",
      "65 epoch Loss: 1.090381\n",
      "66 epoch Loss: 1.079588\n",
      "67 epoch Loss: 1.101725\n",
      "68 epoch Loss: 1.070633\n",
      "69 epoch Loss: 1.091446\n",
      "70 epoch Loss: 1.074100\n",
      "71 epoch Loss: 1.089562\n",
      "72 epoch Loss: 1.057520\n",
      "73 epoch Loss: 1.075346\n",
      "74 epoch Loss: 1.051957\n",
      "75 epoch Loss: 1.056423\n",
      "76 epoch Loss: 1.066830\n",
      "77 epoch Loss: 1.065948\n",
      "78 epoch Loss: 1.057391\n",
      "79 epoch Loss: 1.055069\n",
      "80 epoch Loss: 1.073206\n",
      "81 epoch Loss: 1.071025\n",
      "82 epoch Loss: 1.037322\n",
      "83 epoch Loss: 1.053163\n",
      "84 epoch Loss: 1.051715\n",
      "85 epoch Loss: 1.032186\n",
      "86 epoch Loss: 1.024922\n",
      "87 epoch Loss: 1.046540\n",
      "88 epoch Loss: 1.048214\n",
      "89 epoch Loss: 1.049226\n",
      "90 epoch Loss: 1.053741\n",
      "91 epoch Loss: 1.043743\n",
      "92 epoch Loss: 1.058497\n",
      "93 epoch Loss: 1.041551\n",
      "94 epoch Loss: 1.043110\n",
      "95 epoch Loss: 1.009574\n",
      "96 epoch Loss: 1.041214\n",
      "97 epoch Loss: 1.024917\n",
      "98 epoch Loss: 1.039922\n",
      "99 epoch Loss: 1.049476\n",
      "100 epoch Loss: 1.007855\n",
      "101 epoch Loss: 1.012208\n",
      "102 epoch Loss: 1.005056\n",
      "103 epoch Loss: 1.009014\n",
      "104 epoch Loss: 1.054527\n",
      "105 epoch Loss: 1.004815\n",
      "106 epoch Loss: 1.034497\n",
      "107 epoch Loss: 1.063099\n",
      "108 epoch Loss: 1.033690\n",
      "109 epoch Loss: 1.034608\n",
      "110 epoch Loss: 1.039661\n",
      "111 epoch Loss: 1.057584\n",
      "112 epoch Loss: 1.056974\n",
      "113 epoch Loss: 1.017565\n",
      "114 epoch Loss: 1.043724\n",
      "115 epoch Loss: 1.037631\n",
      "116 epoch Loss: 1.057909\n",
      "117 epoch Loss: 1.025186\n",
      "118 epoch Loss: 1.018833\n",
      "119 epoch Loss: 1.011821\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 85.571429, Random forests accuracy: 73.857143, Ensemble accuracy: 86.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.307845\n",
      "1 epoch Loss: 3.054070\n",
      "2 epoch Loss: 2.835854\n",
      "3 epoch Loss: 2.650138\n",
      "4 epoch Loss: 2.535246\n",
      "5 epoch Loss: 2.442703\n",
      "6 epoch Loss: 2.353704\n",
      "7 epoch Loss: 2.271527\n",
      "8 epoch Loss: 2.208033\n",
      "9 epoch Loss: 2.165883\n",
      "10 epoch Loss: 2.073394\n",
      "11 epoch Loss: 2.054374\n",
      "12 epoch Loss: 2.005544\n",
      "13 epoch Loss: 1.945898\n",
      "14 epoch Loss: 1.901107\n",
      "15 epoch Loss: 1.818436\n",
      "16 epoch Loss: 1.808882\n",
      "17 epoch Loss: 1.776940\n",
      "18 epoch Loss: 1.732081\n",
      "19 epoch Loss: 1.682647\n",
      "20 epoch Loss: 1.645586\n",
      "21 epoch Loss: 1.682561\n",
      "22 epoch Loss: 1.654741\n",
      "23 epoch Loss: 1.580241\n",
      "24 epoch Loss: 1.557944\n",
      "25 epoch Loss: 1.548242\n",
      "26 epoch Loss: 1.545306\n",
      "27 epoch Loss: 1.498853\n",
      "28 epoch Loss: 1.532131\n",
      "29 epoch Loss: 1.455982\n",
      "30 epoch Loss: 1.445089\n",
      "31 epoch Loss: 1.423340\n",
      "32 epoch Loss: 1.390380\n",
      "33 epoch Loss: 1.388738\n",
      "34 epoch Loss: 1.383643\n",
      "35 epoch Loss: 1.382539\n",
      "36 epoch Loss: 1.299300\n",
      "37 epoch Loss: 1.305229\n",
      "38 epoch Loss: 1.298715\n",
      "39 epoch Loss: 1.296716\n",
      "40 epoch Loss: 1.286751\n",
      "41 epoch Loss: 1.266280\n",
      "42 epoch Loss: 1.264879\n",
      "43 epoch Loss: 1.246741\n",
      "44 epoch Loss: 1.258674\n",
      "45 epoch Loss: 1.233434\n",
      "46 epoch Loss: 1.180097\n",
      "47 epoch Loss: 1.218550\n",
      "48 epoch Loss: 1.185300\n",
      "49 epoch Loss: 1.194365\n",
      "50 epoch Loss: 1.184093\n",
      "51 epoch Loss: 1.167447\n",
      "52 epoch Loss: 1.159521\n",
      "53 epoch Loss: 1.180538\n",
      "54 epoch Loss: 1.125731\n",
      "55 epoch Loss: 1.118328\n",
      "56 epoch Loss: 1.136003\n",
      "57 epoch Loss: 1.147534\n",
      "58 epoch Loss: 1.108678\n",
      "59 epoch Loss: 1.133741\n",
      "60 epoch Loss: 1.119744\n",
      "61 epoch Loss: 1.123324\n",
      "62 epoch Loss: 1.107827\n",
      "63 epoch Loss: 1.092484\n",
      "64 epoch Loss: 1.106803\n",
      "65 epoch Loss: 1.094206\n",
      "66 epoch Loss: 1.071513\n",
      "67 epoch Loss: 1.086434\n",
      "68 epoch Loss: 1.084616\n",
      "69 epoch Loss: 1.100534\n",
      "70 epoch Loss: 1.092746\n",
      "71 epoch Loss: 1.078932\n",
      "72 epoch Loss: 1.057964\n",
      "73 epoch Loss: 1.094748\n",
      "74 epoch Loss: 1.046025\n",
      "75 epoch Loss: 1.062950\n",
      "76 epoch Loss: 1.050792\n",
      "77 epoch Loss: 1.078046\n",
      "78 epoch Loss: 1.032457\n",
      "79 epoch Loss: 1.032863\n",
      "80 epoch Loss: 1.052711\n",
      "81 epoch Loss: 1.030536\n",
      "82 epoch Loss: 1.041315\n",
      "83 epoch Loss: 1.053689\n",
      "84 epoch Loss: 1.059084\n",
      "85 epoch Loss: 1.024014\n",
      "86 epoch Loss: 1.037177\n",
      "87 epoch Loss: 1.022372\n",
      "88 epoch Loss: 1.052634\n",
      "89 epoch Loss: 1.013391\n",
      "90 epoch Loss: 1.076863\n",
      "91 epoch Loss: 1.045964\n",
      "92 epoch Loss: 1.039137\n",
      "93 epoch Loss: 1.024931\n",
      "94 epoch Loss: 1.027719\n",
      "95 epoch Loss: 1.049522\n",
      "96 epoch Loss: 1.037603\n",
      "97 epoch Loss: 1.057281\n",
      "98 epoch Loss: 1.040678\n",
      "99 epoch Loss: 1.020445\n",
      "100 epoch Loss: 1.029258\n",
      "101 epoch Loss: 1.012831\n",
      "102 epoch Loss: 1.052711\n",
      "103 epoch Loss: 1.024685\n",
      "104 epoch Loss: 1.027999\n",
      "105 epoch Loss: 1.025983\n",
      "106 epoch Loss: 0.998666\n",
      "107 epoch Loss: 1.042740\n",
      "108 epoch Loss: 0.984930\n",
      "109 epoch Loss: 1.030855\n",
      "110 epoch Loss: 1.030903\n",
      "111 epoch Loss: 1.012250\n",
      "112 epoch Loss: 1.020140\n",
      "113 epoch Loss: 0.998263\n",
      "114 epoch Loss: 1.027039\n",
      "115 epoch Loss: 1.003171\n",
      "116 epoch Loss: 0.989756\n",
      "117 epoch Loss: 1.016544\n",
      "118 epoch Loss: 1.036010\n",
      "119 epoch Loss: 1.010940\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 86.000000, Random forests accuracy: 72.857143, Ensemble accuracy: 86.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.298809\n",
      "1 epoch Loss: 3.040163\n",
      "2 epoch Loss: 2.835002\n",
      "3 epoch Loss: 2.664063\n",
      "4 epoch Loss: 2.528808\n",
      "5 epoch Loss: 2.424531\n",
      "6 epoch Loss: 2.333202\n",
      "7 epoch Loss: 2.256436\n",
      "8 epoch Loss: 2.184590\n",
      "9 epoch Loss: 2.146077\n",
      "10 epoch Loss: 2.061520\n",
      "11 epoch Loss: 2.028784\n",
      "12 epoch Loss: 1.977339\n",
      "13 epoch Loss: 1.924852\n",
      "14 epoch Loss: 1.863451\n",
      "15 epoch Loss: 1.819715\n",
      "16 epoch Loss: 1.825741\n",
      "17 epoch Loss: 1.780716\n",
      "18 epoch Loss: 1.731833\n",
      "19 epoch Loss: 1.723984\n",
      "20 epoch Loss: 1.664620\n",
      "21 epoch Loss: 1.640637\n",
      "22 epoch Loss: 1.609938\n",
      "23 epoch Loss: 1.579983\n",
      "24 epoch Loss: 1.556351\n",
      "25 epoch Loss: 1.547389\n",
      "26 epoch Loss: 1.530760\n",
      "27 epoch Loss: 1.520958\n",
      "28 epoch Loss: 1.472584\n",
      "29 epoch Loss: 1.453415\n",
      "30 epoch Loss: 1.417488\n",
      "31 epoch Loss: 1.411611\n",
      "32 epoch Loss: 1.395660\n",
      "33 epoch Loss: 1.400935\n",
      "34 epoch Loss: 1.349908\n",
      "35 epoch Loss: 1.370995\n",
      "36 epoch Loss: 1.319143\n",
      "37 epoch Loss: 1.334801\n",
      "38 epoch Loss: 1.290439\n",
      "39 epoch Loss: 1.301644\n",
      "40 epoch Loss: 1.265542\n",
      "41 epoch Loss: 1.252707\n",
      "42 epoch Loss: 1.260882\n",
      "43 epoch Loss: 1.202223\n",
      "44 epoch Loss: 1.218513\n",
      "45 epoch Loss: 1.229911\n",
      "46 epoch Loss: 1.218316\n",
      "47 epoch Loss: 1.244263\n",
      "48 epoch Loss: 1.199211\n",
      "49 epoch Loss: 1.189928\n",
      "50 epoch Loss: 1.185384\n",
      "51 epoch Loss: 1.157923\n",
      "52 epoch Loss: 1.138395\n",
      "53 epoch Loss: 1.161349\n",
      "54 epoch Loss: 1.134311\n",
      "55 epoch Loss: 1.140187\n",
      "56 epoch Loss: 1.119800\n",
      "57 epoch Loss: 1.099431\n",
      "58 epoch Loss: 1.149810\n",
      "59 epoch Loss: 1.117763\n",
      "60 epoch Loss: 1.131014\n",
      "61 epoch Loss: 1.121848\n",
      "62 epoch Loss: 1.101344\n",
      "63 epoch Loss: 1.113784\n",
      "64 epoch Loss: 1.069227\n",
      "65 epoch Loss: 1.079646\n",
      "66 epoch Loss: 1.095316\n",
      "67 epoch Loss: 1.077137\n",
      "68 epoch Loss: 1.042151\n",
      "69 epoch Loss: 1.052299\n",
      "70 epoch Loss: 1.094936\n",
      "71 epoch Loss: 1.077693\n",
      "72 epoch Loss: 1.073942\n",
      "73 epoch Loss: 1.070404\n",
      "74 epoch Loss: 1.059662\n",
      "75 epoch Loss: 1.053874\n",
      "76 epoch Loss: 1.061933\n",
      "77 epoch Loss: 1.060541\n",
      "78 epoch Loss: 1.050347\n",
      "79 epoch Loss: 1.061360\n",
      "80 epoch Loss: 1.025196\n",
      "81 epoch Loss: 1.027917\n",
      "82 epoch Loss: 1.044435\n",
      "83 epoch Loss: 1.048590\n",
      "84 epoch Loss: 1.039661\n",
      "85 epoch Loss: 1.042560\n",
      "86 epoch Loss: 1.054353\n",
      "87 epoch Loss: 1.049435\n",
      "88 epoch Loss: 1.028600\n",
      "89 epoch Loss: 1.041771\n",
      "90 epoch Loss: 1.032431\n",
      "91 epoch Loss: 1.064633\n",
      "92 epoch Loss: 1.044464\n",
      "93 epoch Loss: 0.998197\n",
      "94 epoch Loss: 1.018876\n",
      "95 epoch Loss: 1.050714\n",
      "96 epoch Loss: 1.036553\n",
      "97 epoch Loss: 1.036032\n",
      "98 epoch Loss: 1.031921\n",
      "99 epoch Loss: 1.022271\n",
      "100 epoch Loss: 1.059318\n",
      "101 epoch Loss: 1.014666\n",
      "102 epoch Loss: 1.004016\n",
      "103 epoch Loss: 1.023898\n",
      "104 epoch Loss: 1.022410\n",
      "105 epoch Loss: 1.016101\n",
      "106 epoch Loss: 1.021753\n",
      "107 epoch Loss: 1.055833\n",
      "108 epoch Loss: 1.006690\n",
      "109 epoch Loss: 1.036579\n",
      "110 epoch Loss: 1.036343\n",
      "111 epoch Loss: 0.989053\n",
      "112 epoch Loss: 1.027909\n",
      "113 epoch Loss: 1.015800\n",
      "114 epoch Loss: 0.988501\n",
      "115 epoch Loss: 1.041131\n",
      "116 epoch Loss: 1.021979\n",
      "117 epoch Loss: 1.009660\n",
      "118 epoch Loss: 1.014692\n",
      "119 epoch Loss: 1.028115\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 85.000000, Random forests accuracy: 72.714286, Ensemble accuracy: 86.428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.308000\n",
      "1 epoch Loss: 3.080876\n",
      "2 epoch Loss: 2.867749\n",
      "3 epoch Loss: 2.677594\n",
      "4 epoch Loss: 2.530935\n",
      "5 epoch Loss: 2.450881\n",
      "6 epoch Loss: 2.362715\n",
      "7 epoch Loss: 2.275850\n",
      "8 epoch Loss: 2.211630\n",
      "9 epoch Loss: 2.129360\n",
      "10 epoch Loss: 2.106263\n",
      "11 epoch Loss: 2.081056\n",
      "12 epoch Loss: 1.996155\n",
      "13 epoch Loss: 1.952961\n",
      "14 epoch Loss: 1.925194\n",
      "15 epoch Loss: 1.845612\n",
      "16 epoch Loss: 1.832429\n",
      "17 epoch Loss: 1.803304\n",
      "18 epoch Loss: 1.770312\n",
      "19 epoch Loss: 1.709246\n",
      "20 epoch Loss: 1.708583\n",
      "21 epoch Loss: 1.653558\n",
      "22 epoch Loss: 1.624647\n",
      "23 epoch Loss: 1.623245\n",
      "24 epoch Loss: 1.568527\n",
      "25 epoch Loss: 1.570508\n",
      "26 epoch Loss: 1.545486\n",
      "27 epoch Loss: 1.482163\n",
      "28 epoch Loss: 1.468230\n",
      "29 epoch Loss: 1.464309\n",
      "30 epoch Loss: 1.430926\n",
      "31 epoch Loss: 1.415331\n",
      "32 epoch Loss: 1.397830\n",
      "33 epoch Loss: 1.377975\n",
      "34 epoch Loss: 1.364017\n",
      "35 epoch Loss: 1.358585\n",
      "36 epoch Loss: 1.336527\n",
      "37 epoch Loss: 1.323112\n",
      "38 epoch Loss: 1.315572\n",
      "39 epoch Loss: 1.320150\n",
      "40 epoch Loss: 1.274896\n",
      "41 epoch Loss: 1.296631\n",
      "42 epoch Loss: 1.241012\n",
      "43 epoch Loss: 1.286925\n",
      "44 epoch Loss: 1.260138\n",
      "45 epoch Loss: 1.230635\n",
      "46 epoch Loss: 1.216615\n",
      "47 epoch Loss: 1.185319\n",
      "48 epoch Loss: 1.191094\n",
      "49 epoch Loss: 1.168033\n",
      "50 epoch Loss: 1.169547\n",
      "51 epoch Loss: 1.181132\n",
      "52 epoch Loss: 1.163890\n",
      "53 epoch Loss: 1.160458\n",
      "54 epoch Loss: 1.182051\n",
      "55 epoch Loss: 1.168646\n",
      "56 epoch Loss: 1.169226\n",
      "57 epoch Loss: 1.127556\n",
      "58 epoch Loss: 1.083562\n",
      "59 epoch Loss: 1.117483\n",
      "60 epoch Loss: 1.123182\n",
      "61 epoch Loss: 1.111072\n",
      "62 epoch Loss: 1.117520\n",
      "63 epoch Loss: 1.102290\n",
      "64 epoch Loss: 1.092104\n",
      "65 epoch Loss: 1.089704\n",
      "66 epoch Loss: 1.090278\n",
      "67 epoch Loss: 1.083720\n",
      "68 epoch Loss: 1.048305\n",
      "69 epoch Loss: 1.082813\n",
      "70 epoch Loss: 1.094290\n",
      "71 epoch Loss: 1.062940\n",
      "72 epoch Loss: 1.091885\n",
      "73 epoch Loss: 1.077152\n",
      "74 epoch Loss: 1.067349\n",
      "75 epoch Loss: 1.082701\n",
      "76 epoch Loss: 1.060954\n",
      "77 epoch Loss: 1.028470\n",
      "78 epoch Loss: 1.021586\n",
      "79 epoch Loss: 1.062561\n",
      "80 epoch Loss: 1.054898\n",
      "81 epoch Loss: 1.044278\n",
      "82 epoch Loss: 1.037292\n",
      "83 epoch Loss: 1.054673\n",
      "84 epoch Loss: 1.059215\n",
      "85 epoch Loss: 1.067858\n",
      "86 epoch Loss: 1.049069\n",
      "87 epoch Loss: 1.020127\n",
      "88 epoch Loss: 1.050119\n",
      "89 epoch Loss: 1.047357\n",
      "90 epoch Loss: 1.046657\n",
      "91 epoch Loss: 1.042476\n",
      "92 epoch Loss: 1.046649\n",
      "93 epoch Loss: 1.051836\n",
      "94 epoch Loss: 1.037984\n",
      "95 epoch Loss: 1.018984\n",
      "96 epoch Loss: 1.030910\n",
      "97 epoch Loss: 1.012530\n",
      "98 epoch Loss: 1.054275\n",
      "99 epoch Loss: 1.029108\n",
      "100 epoch Loss: 1.029409\n",
      "101 epoch Loss: 1.012072\n",
      "102 epoch Loss: 1.034333\n",
      "103 epoch Loss: 1.022548\n",
      "104 epoch Loss: 1.040629\n",
      "105 epoch Loss: 1.027690\n",
      "106 epoch Loss: 1.014010\n",
      "107 epoch Loss: 1.037700\n",
      "108 epoch Loss: 1.031149\n",
      "109 epoch Loss: 1.039355\n",
      "110 epoch Loss: 1.038904\n",
      "111 epoch Loss: 1.061810\n",
      "112 epoch Loss: 1.018857\n",
      "113 epoch Loss: 1.042147\n",
      "114 epoch Loss: 0.986367\n",
      "115 epoch Loss: 1.027327\n",
      "116 epoch Loss: 1.027815\n",
      "117 epoch Loss: 1.021460\n",
      "118 epoch Loss: 1.050267\n",
      "119 epoch Loss: 1.045299\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 86.428571, Random forests accuracy: 75.714286, Ensemble accuracy: 88.285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.294894\n",
      "1 epoch Loss: 3.069373\n",
      "2 epoch Loss: 2.840021\n",
      "3 epoch Loss: 2.649222\n",
      "4 epoch Loss: 2.512878\n",
      "5 epoch Loss: 2.402183\n",
      "6 epoch Loss: 2.329227\n",
      "7 epoch Loss: 2.272336\n",
      "8 epoch Loss: 2.164235\n",
      "9 epoch Loss: 2.132589\n",
      "10 epoch Loss: 2.092755\n",
      "11 epoch Loss: 2.045479\n",
      "12 epoch Loss: 1.952923\n",
      "13 epoch Loss: 1.921551\n",
      "14 epoch Loss: 1.882305\n",
      "15 epoch Loss: 1.859456\n",
      "16 epoch Loss: 1.794275\n",
      "17 epoch Loss: 1.753563\n",
      "18 epoch Loss: 1.725567\n",
      "19 epoch Loss: 1.699327\n",
      "20 epoch Loss: 1.671585\n",
      "21 epoch Loss: 1.661360\n",
      "22 epoch Loss: 1.653817\n",
      "23 epoch Loss: 1.558971\n",
      "24 epoch Loss: 1.575794\n",
      "25 epoch Loss: 1.533352\n",
      "26 epoch Loss: 1.538994\n",
      "27 epoch Loss: 1.517411\n",
      "28 epoch Loss: 1.485650\n",
      "29 epoch Loss: 1.469154\n",
      "30 epoch Loss: 1.435509\n",
      "31 epoch Loss: 1.415296\n",
      "32 epoch Loss: 1.404718\n",
      "33 epoch Loss: 1.375201\n",
      "34 epoch Loss: 1.343894\n",
      "35 epoch Loss: 1.360041\n",
      "36 epoch Loss: 1.328593\n",
      "37 epoch Loss: 1.366391\n",
      "38 epoch Loss: 1.297215\n",
      "39 epoch Loss: 1.275376\n",
      "40 epoch Loss: 1.250461\n",
      "41 epoch Loss: 1.256404\n",
      "42 epoch Loss: 1.257297\n",
      "43 epoch Loss: 1.246642\n",
      "44 epoch Loss: 1.254067\n",
      "45 epoch Loss: 1.225499\n",
      "46 epoch Loss: 1.185644\n",
      "47 epoch Loss: 1.194906\n",
      "48 epoch Loss: 1.194560\n",
      "49 epoch Loss: 1.188862\n",
      "50 epoch Loss: 1.173041\n",
      "51 epoch Loss: 1.195098\n",
      "52 epoch Loss: 1.155316\n",
      "53 epoch Loss: 1.159987\n",
      "54 epoch Loss: 1.118284\n",
      "55 epoch Loss: 1.116509\n",
      "56 epoch Loss: 1.156979\n",
      "57 epoch Loss: 1.125386\n",
      "58 epoch Loss: 1.119381\n",
      "59 epoch Loss: 1.083457\n",
      "60 epoch Loss: 1.120306\n",
      "61 epoch Loss: 1.116388\n",
      "62 epoch Loss: 1.087834\n",
      "63 epoch Loss: 1.081593\n",
      "64 epoch Loss: 1.101247\n",
      "65 epoch Loss: 1.092393\n",
      "66 epoch Loss: 1.074889\n",
      "67 epoch Loss: 1.089863\n",
      "68 epoch Loss: 1.114296\n",
      "69 epoch Loss: 1.041529\n",
      "70 epoch Loss: 1.090660\n",
      "71 epoch Loss: 1.070317\n",
      "72 epoch Loss: 1.090766\n",
      "73 epoch Loss: 1.074653\n",
      "74 epoch Loss: 1.093151\n",
      "75 epoch Loss: 1.072925\n",
      "76 epoch Loss: 1.041867\n",
      "77 epoch Loss: 1.070871\n",
      "78 epoch Loss: 1.069815\n",
      "79 epoch Loss: 1.065134\n",
      "80 epoch Loss: 1.070155\n",
      "81 epoch Loss: 1.036627\n",
      "82 epoch Loss: 1.078628\n",
      "83 epoch Loss: 1.033711\n",
      "84 epoch Loss: 1.061117\n",
      "85 epoch Loss: 1.054441\n",
      "86 epoch Loss: 1.041909\n",
      "87 epoch Loss: 1.058265\n",
      "88 epoch Loss: 1.033406\n",
      "89 epoch Loss: 1.028888\n",
      "90 epoch Loss: 1.055336\n",
      "91 epoch Loss: 1.038342\n",
      "92 epoch Loss: 1.054852\n",
      "93 epoch Loss: 1.045499\n",
      "94 epoch Loss: 1.044254\n",
      "95 epoch Loss: 1.044847\n",
      "96 epoch Loss: 1.033246\n",
      "97 epoch Loss: 1.034604\n",
      "98 epoch Loss: 1.028262\n",
      "99 epoch Loss: 1.014582\n",
      "100 epoch Loss: 1.031693\n",
      "101 epoch Loss: 1.021876\n",
      "102 epoch Loss: 1.015268\n",
      "103 epoch Loss: 1.027294\n",
      "104 epoch Loss: 1.020449\n",
      "105 epoch Loss: 1.037686\n",
      "106 epoch Loss: 1.027168\n",
      "107 epoch Loss: 1.026515\n",
      "108 epoch Loss: 1.018889\n",
      "109 epoch Loss: 1.031627\n",
      "110 epoch Loss: 1.020946\n",
      "111 epoch Loss: 1.049738\n",
      "112 epoch Loss: 1.037279\n",
      "113 epoch Loss: 1.013656\n",
      "114 epoch Loss: 1.027192\n",
      "115 epoch Loss: 1.023692\n",
      "116 epoch Loss: 1.041192\n",
      "117 epoch Loss: 1.004828\n",
      "118 epoch Loss: 1.017525\n",
      "119 epoch Loss: 1.012165\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 85.000000, Random forests accuracy: 74.000000, Ensemble accuracy: 86.428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.297373\n",
      "1 epoch Loss: 3.050640\n",
      "2 epoch Loss: 2.848047\n",
      "3 epoch Loss: 2.635403\n",
      "4 epoch Loss: 2.525977\n",
      "5 epoch Loss: 2.430155\n",
      "6 epoch Loss: 2.329258\n",
      "7 epoch Loss: 2.283524\n",
      "8 epoch Loss: 2.181920\n",
      "9 epoch Loss: 2.131340\n",
      "10 epoch Loss: 2.095892\n",
      "11 epoch Loss: 2.051627\n",
      "12 epoch Loss: 2.002188\n",
      "13 epoch Loss: 1.942599\n",
      "14 epoch Loss: 1.904583\n",
      "15 epoch Loss: 1.853690\n",
      "16 epoch Loss: 1.817147\n",
      "17 epoch Loss: 1.779894\n",
      "18 epoch Loss: 1.751384\n",
      "19 epoch Loss: 1.724274\n",
      "20 epoch Loss: 1.677838\n",
      "21 epoch Loss: 1.650767\n",
      "22 epoch Loss: 1.623405\n",
      "23 epoch Loss: 1.639282\n",
      "24 epoch Loss: 1.583941\n",
      "25 epoch Loss: 1.538738\n",
      "26 epoch Loss: 1.524196\n",
      "27 epoch Loss: 1.500958\n",
      "28 epoch Loss: 1.494360\n",
      "29 epoch Loss: 1.431478\n",
      "30 epoch Loss: 1.432782\n",
      "31 epoch Loss: 1.406144\n",
      "32 epoch Loss: 1.370996\n",
      "33 epoch Loss: 1.371501\n",
      "34 epoch Loss: 1.377447\n",
      "35 epoch Loss: 1.334923\n",
      "36 epoch Loss: 1.344533\n",
      "37 epoch Loss: 1.337732\n",
      "38 epoch Loss: 1.323594\n",
      "39 epoch Loss: 1.302269\n",
      "40 epoch Loss: 1.247225\n",
      "41 epoch Loss: 1.256811\n",
      "42 epoch Loss: 1.270040\n",
      "43 epoch Loss: 1.253253\n",
      "44 epoch Loss: 1.228050\n",
      "45 epoch Loss: 1.235773\n",
      "46 epoch Loss: 1.219196\n",
      "47 epoch Loss: 1.204946\n",
      "48 epoch Loss: 1.191524\n",
      "49 epoch Loss: 1.205476\n",
      "50 epoch Loss: 1.196451\n",
      "51 epoch Loss: 1.166330\n",
      "52 epoch Loss: 1.193661\n",
      "53 epoch Loss: 1.152042\n",
      "54 epoch Loss: 1.124988\n",
      "55 epoch Loss: 1.154421\n",
      "56 epoch Loss: 1.132646\n",
      "57 epoch Loss: 1.101090\n",
      "58 epoch Loss: 1.144642\n",
      "59 epoch Loss: 1.105833\n",
      "60 epoch Loss: 1.124791\n",
      "61 epoch Loss: 1.107077\n",
      "62 epoch Loss: 1.110963\n",
      "63 epoch Loss: 1.105225\n",
      "64 epoch Loss: 1.107744\n",
      "65 epoch Loss: 1.121731\n",
      "66 epoch Loss: 1.054664\n",
      "67 epoch Loss: 1.089240\n",
      "68 epoch Loss: 1.072727\n",
      "69 epoch Loss: 1.074109\n",
      "70 epoch Loss: 1.067975\n",
      "71 epoch Loss: 1.062431\n",
      "72 epoch Loss: 1.061366\n",
      "73 epoch Loss: 1.071243\n",
      "74 epoch Loss: 1.090302\n",
      "75 epoch Loss: 1.057575\n",
      "76 epoch Loss: 1.074070\n",
      "77 epoch Loss: 1.071557\n",
      "78 epoch Loss: 1.080011\n",
      "79 epoch Loss: 1.067783\n",
      "80 epoch Loss: 1.032807\n",
      "81 epoch Loss: 1.087344\n",
      "82 epoch Loss: 1.057905\n",
      "83 epoch Loss: 1.041397\n",
      "84 epoch Loss: 1.069542\n",
      "85 epoch Loss: 1.052900\n",
      "86 epoch Loss: 1.024005\n",
      "87 epoch Loss: 1.047605\n",
      "88 epoch Loss: 1.050962\n",
      "89 epoch Loss: 1.029691\n",
      "90 epoch Loss: 1.059466\n",
      "91 epoch Loss: 1.019951\n",
      "92 epoch Loss: 1.020092\n",
      "93 epoch Loss: 1.041543\n",
      "94 epoch Loss: 1.050961\n",
      "95 epoch Loss: 1.034007\n",
      "96 epoch Loss: 1.051687\n",
      "97 epoch Loss: 1.030493\n",
      "98 epoch Loss: 1.046108\n",
      "99 epoch Loss: 1.019279\n",
      "100 epoch Loss: 1.031110\n",
      "101 epoch Loss: 1.042047\n",
      "102 epoch Loss: 1.022795\n",
      "103 epoch Loss: 1.038761\n",
      "104 epoch Loss: 1.055365\n",
      "105 epoch Loss: 1.005381\n",
      "106 epoch Loss: 1.027124\n",
      "107 epoch Loss: 0.993210\n",
      "108 epoch Loss: 1.024305\n",
      "109 epoch Loss: 1.041750\n",
      "110 epoch Loss: 1.020128\n",
      "111 epoch Loss: 1.051205\n",
      "112 epoch Loss: 1.028743\n",
      "113 epoch Loss: 1.044931\n",
      "114 epoch Loss: 1.059413\n",
      "115 epoch Loss: 1.029241\n",
      "116 epoch Loss: 1.025366\n",
      "117 epoch Loss: 1.004423\n",
      "118 epoch Loss: 1.046086\n",
      "119 epoch Loss: 1.010613\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.615385, Random forests accuracy: 72.507123, Ensemble accuracy: 85.612536\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#Load data\n",
    "\n",
    "    \n",
    "    #Initialize\n",
    "    label -= 1\n",
    "    np.random.seed(2018)\n",
    "\n",
    "\n",
    "    \n",
    "    t_index = np.random.permutation(int(np.shape(data)[0]/Outer_loop)*Outer_loop)\n",
    "    t_index = np.reshape(t_index, [Outer_loop, -1])\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    box = np.array([4000], dtype=np.int32)\n",
    "    flag = 0\n",
    "    for test_index in t_index:\n",
    "        if flag == Outer_loop-1:\n",
    "            test_index = np.array(np.concatenate((test_index, np.array(range(int(np.shape(data)[0]/Outer_loop)*Outer_loop,np.shape(data)[0]))), axis=0), dtype=np.int32)\n",
    "        train_index = np.setdiff1d(np.array(range(0,np.shape(data)[0])), test_index)\n",
    "        train_data = data[train_index]\n",
    "        train_label = label[train_index]\n",
    "        test_data = data[test_index]\n",
    "        test_label = label[test_index]\n",
    "        \n",
    "        kf = np.random.permutation(int(np.shape(train_data)[0]/Inner_loop)*Inner_loop)\n",
    "        kf = kf.reshape([Inner_loop]+[-1])\n",
    "        val_result = np.zeros([np.shape(train_data)[0],48], dtype=np.float32)\n",
    "        \n",
    "        tot_acc = np.zeros([Inner_loop,5], dtype=np.float32)\n",
    "        #lasso = Lasso()\n",
    "        lsvc = LinearSVC(C=1, penalty=\"l1\", dual=False).fit(data, label)\n",
    "        coef = np.squeeze(np.sum(np.square(np.array(lsvc.coef_)), axis=0))\n",
    "        #coef = np.squeeze(np.sum(np.square(np.array(lasso.coef_)), axis=0))\n",
    "        coefidx = np.argsort(coef)\n",
    "#         for inner_fold in range(0,Inner_loop):\n",
    "#             val_test_ind = kf[inner_fold]\n",
    "#             if inner_fold == Inner_loop-1:\n",
    "#                 val_test_ind = np.array(np.concatenate((val_test_ind,np.array(range(int(np.shape(train_data)[0]/Outer_loop)*Outer_loop,np.shape(train_data)[0]),dtype=np.int32)), axis=0),dtype=np.int32)\n",
    "            \n",
    "#             val_train_ind = np.setdiff1d(np.array(range(0,np.shape(train_data)[0]),dtype=np.int32), val_test_ind)\n",
    "#             val_train = train_data[val_train_ind]\n",
    "#             val_test = train_data[val_test_ind]\n",
    "#             val_train_label = train_label[val_train_ind]\n",
    "#             val_test_label = train_label[val_test_ind]\n",
    "#             temp = 0\n",
    "#             for item in box:\n",
    "#                 idx = coefidx[-item:]\n",
    "#                 vtrain = val_train[:,idx]\n",
    "#                 vtest = val_test[:,idx]\n",
    "#                 nn_acc, result_nn = dnn(vtrain, val_train_label, vtest, val_test_label)\n",
    "#                 rf_acc, result_rf = rfc(vtrain, val_train_label, vtest, val_test_label)\n",
    "#                 en_acc = 0.0\n",
    "#                 for i in range(0,np.shape(vtest)[0]):\n",
    "#                     r = np.argmax(result_nn[i]+result_rf[i])\n",
    "#                     if r == val_test_label[i]:\n",
    "#                         en_acc += 1\n",
    "#                 en_acc /= np.shape(vtest)[0]*0.01\n",
    "#                 tot_acc[inner_fold,temp] = en_acc\n",
    "#                 print(\"Inner_fold # of features: %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (item, nn_acc, rf_acc, en_acc))\n",
    "#                 temp += 1\n",
    "        \n",
    "        u = np.sum(tot_acc,0)\n",
    "        \n",
    "       \n",
    "        best_n = box[np.argmax(u)]\n",
    "        idx = coefidx[-best_n:]\n",
    "        \n",
    "        tr_data = train_data[:,idx]\n",
    "        te_data = test_data[:,idx]\n",
    "        nn_acc, result_nn = dnn(tr_data, train_label, te_data, test_label)\n",
    "        rf_acc, result_rf = rfc(tr_data, train_label, te_data, test_label)\n",
    "        en_acc = 0.0\n",
    "        for i in range(0,np.shape(te_data)[0]):\n",
    "            r = np.argmax(result_nn[i]+result_rf[i])\n",
    "            if r == test_label[i]:\n",
    "                en_acc += 1\n",
    "        en_acc /= np.shape(te_data)[0]*0.01\n",
    "        print(\"Outer_fold # of features:  %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (best_n, nn_acc, rf_acc, en_acc))\n",
    "        flag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4000\n",
    "#lsvc\n",
    "lsvc_nn=[84.142857,84.428571,83.285714,84.000000,85.571429,86.000000,85.000000,86.428571,85.000000,84.615385]\n",
    "lsvc_rf=[74.857143,74.857143,73.285714,72.285714,73.857143,72.857143,72.714286,75.714286,74.000000,72.507123]\n",
    "emsemble=[85.714286,86.714286,85.571429,85.000000,86.857143,86.857143,86.428571,88.285714,86.428571,85.612536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=[85.714286,86.714286,85.571429,85.000000,86.857143,86.857143,86.428571,88.285714,86.428571,85.612536,\n",
    "    84.142857,84.428571,83.285714,84.000000,85.571429,86.000000,85.000000,86.428571,85.000000,84.615385,\n",
    "    74.857143,74.857143,73.285714,72.285714,73.857143,72.857143,72.714286,75.714286,74.000000,72.507123]\n",
    "md=['ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble',\n",
    "    'neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network',\n",
    "    'random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest',\n",
    "    'ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy':acc,'Model':md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\",rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.boxplot(x = \"Feature\", y = \"Accuracy\", data = df, palette=\"Set3\", width=0.6, fliersize=2)\n",
    "ax.set_xticklabels(ax.get_xticklabels(),rotation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.328352\n",
      "1 epoch Loss: 3.091696\n",
      "2 epoch Loss: 2.892011\n",
      "3 epoch Loss: 2.714771\n",
      "4 epoch Loss: 2.595865\n",
      "5 epoch Loss: 2.491039\n",
      "6 epoch Loss: 2.415102\n",
      "7 epoch Loss: 2.364929\n",
      "8 epoch Loss: 2.271390\n",
      "9 epoch Loss: 2.231964\n",
      "10 epoch Loss: 2.144355\n",
      "11 epoch Loss: 2.146807\n",
      "12 epoch Loss: 2.100693\n",
      "13 epoch Loss: 2.032084\n",
      "14 epoch Loss: 1.994187\n",
      "15 epoch Loss: 1.933573\n",
      "16 epoch Loss: 1.957393\n",
      "17 epoch Loss: 1.925466\n",
      "18 epoch Loss: 1.874803\n",
      "19 epoch Loss: 1.872542\n",
      "20 epoch Loss: 1.834398\n",
      "21 epoch Loss: 1.792615\n",
      "22 epoch Loss: 1.790511\n",
      "23 epoch Loss: 1.726637\n",
      "24 epoch Loss: 1.727633\n",
      "25 epoch Loss: 1.724254\n",
      "26 epoch Loss: 1.693733\n",
      "27 epoch Loss: 1.685006\n",
      "28 epoch Loss: 1.644278\n",
      "29 epoch Loss: 1.636811\n",
      "30 epoch Loss: 1.613416\n",
      "31 epoch Loss: 1.574634\n",
      "32 epoch Loss: 1.564381\n",
      "33 epoch Loss: 1.559504\n",
      "34 epoch Loss: 1.519413\n",
      "35 epoch Loss: 1.521092\n",
      "36 epoch Loss: 1.512551\n",
      "37 epoch Loss: 1.433730\n",
      "38 epoch Loss: 1.456333\n",
      "39 epoch Loss: 1.440824\n",
      "40 epoch Loss: 1.432900\n",
      "41 epoch Loss: 1.416477\n",
      "42 epoch Loss: 1.410698\n",
      "43 epoch Loss: 1.396562\n",
      "44 epoch Loss: 1.398079\n",
      "45 epoch Loss: 1.362285\n",
      "46 epoch Loss: 1.356963\n",
      "47 epoch Loss: 1.323405\n",
      "48 epoch Loss: 1.334716\n",
      "49 epoch Loss: 1.293620\n",
      "50 epoch Loss: 1.274817\n",
      "51 epoch Loss: 1.291821\n",
      "52 epoch Loss: 1.313514\n",
      "53 epoch Loss: 1.297121\n",
      "54 epoch Loss: 1.269457\n",
      "55 epoch Loss: 1.265715\n",
      "56 epoch Loss: 1.247643\n",
      "57 epoch Loss: 1.268409\n",
      "58 epoch Loss: 1.268510\n",
      "59 epoch Loss: 1.231749\n",
      "60 epoch Loss: 1.236377\n",
      "61 epoch Loss: 1.217402\n",
      "62 epoch Loss: 1.208574\n",
      "63 epoch Loss: 1.200463\n",
      "64 epoch Loss: 1.183550\n",
      "65 epoch Loss: 1.199432\n",
      "66 epoch Loss: 1.173087\n",
      "67 epoch Loss: 1.185451\n",
      "68 epoch Loss: 1.175295\n",
      "69 epoch Loss: 1.156333\n",
      "70 epoch Loss: 1.180701\n",
      "71 epoch Loss: 1.172297\n",
      "72 epoch Loss: 1.145406\n",
      "73 epoch Loss: 1.152024\n",
      "74 epoch Loss: 1.152707\n",
      "75 epoch Loss: 1.125395\n",
      "76 epoch Loss: 1.178062\n",
      "77 epoch Loss: 1.120620\n",
      "78 epoch Loss: 1.122172\n",
      "79 epoch Loss: 1.136531\n",
      "80 epoch Loss: 1.129268\n",
      "81 epoch Loss: 1.111555\n",
      "82 epoch Loss: 1.109885\n",
      "83 epoch Loss: 1.138352\n",
      "84 epoch Loss: 1.082903\n",
      "85 epoch Loss: 1.117798\n",
      "86 epoch Loss: 1.085558\n",
      "87 epoch Loss: 1.098632\n",
      "88 epoch Loss: 1.094034\n",
      "89 epoch Loss: 1.103637\n",
      "90 epoch Loss: 1.113684\n",
      "91 epoch Loss: 1.094921\n",
      "92 epoch Loss: 1.097007\n",
      "93 epoch Loss: 1.086983\n",
      "94 epoch Loss: 1.066273\n",
      "95 epoch Loss: 1.068379\n",
      "96 epoch Loss: 1.077309\n",
      "97 epoch Loss: 1.078696\n",
      "98 epoch Loss: 1.095962\n",
      "99 epoch Loss: 1.069591\n",
      "100 epoch Loss: 1.079690\n",
      "101 epoch Loss: 1.035371\n",
      "102 epoch Loss: 1.065561\n",
      "103 epoch Loss: 1.065341\n",
      "104 epoch Loss: 1.078691\n",
      "105 epoch Loss: 1.069250\n",
      "106 epoch Loss: 1.052850\n",
      "107 epoch Loss: 1.105644\n",
      "108 epoch Loss: 1.058383\n",
      "109 epoch Loss: 1.085734\n",
      "110 epoch Loss: 1.067004\n",
      "111 epoch Loss: 1.027984\n",
      "112 epoch Loss: 1.054916\n",
      "113 epoch Loss: 1.040008\n",
      "114 epoch Loss: 1.091652\n",
      "115 epoch Loss: 1.075933\n",
      "116 epoch Loss: 1.043486\n",
      "117 epoch Loss: 1.046021\n",
      "118 epoch Loss: 1.053926\n",
      "119 epoch Loss: 1.058547\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 76.000000, Random forests accuracy: 72.428571, Ensemble accuracy: 80.142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.319101\n",
      "1 epoch Loss: 3.080239\n",
      "2 epoch Loss: 2.907012\n",
      "3 epoch Loss: 2.711665\n",
      "4 epoch Loss: 2.595171\n",
      "5 epoch Loss: 2.525228\n",
      "6 epoch Loss: 2.439899\n",
      "7 epoch Loss: 2.368657\n",
      "8 epoch Loss: 2.288194\n",
      "9 epoch Loss: 2.260146\n",
      "10 epoch Loss: 2.181027\n",
      "11 epoch Loss: 2.150870\n",
      "12 epoch Loss: 2.088216\n",
      "13 epoch Loss: 2.055209\n",
      "14 epoch Loss: 2.030825\n",
      "15 epoch Loss: 1.973708\n",
      "16 epoch Loss: 1.949729\n",
      "17 epoch Loss: 1.940446\n",
      "18 epoch Loss: 1.875314\n",
      "19 epoch Loss: 1.857782\n",
      "20 epoch Loss: 1.838844\n",
      "21 epoch Loss: 1.793145\n",
      "22 epoch Loss: 1.768490\n",
      "23 epoch Loss: 1.745430\n",
      "24 epoch Loss: 1.772377\n",
      "25 epoch Loss: 1.732225\n",
      "26 epoch Loss: 1.673078\n",
      "27 epoch Loss: 1.697351\n",
      "28 epoch Loss: 1.635360\n",
      "29 epoch Loss: 1.610352\n",
      "30 epoch Loss: 1.607478\n",
      "31 epoch Loss: 1.607445\n",
      "32 epoch Loss: 1.609931\n",
      "33 epoch Loss: 1.555532\n",
      "34 epoch Loss: 1.516537\n",
      "35 epoch Loss: 1.542679\n",
      "36 epoch Loss: 1.499804\n",
      "37 epoch Loss: 1.507728\n",
      "38 epoch Loss: 1.489653\n",
      "39 epoch Loss: 1.472124\n",
      "40 epoch Loss: 1.452651\n",
      "41 epoch Loss: 1.407843\n",
      "42 epoch Loss: 1.417522\n",
      "43 epoch Loss: 1.397551\n",
      "44 epoch Loss: 1.383317\n",
      "45 epoch Loss: 1.363431\n",
      "46 epoch Loss: 1.387326\n",
      "47 epoch Loss: 1.346113\n",
      "48 epoch Loss: 1.343287\n",
      "49 epoch Loss: 1.345786\n",
      "50 epoch Loss: 1.299375\n",
      "51 epoch Loss: 1.356924\n",
      "52 epoch Loss: 1.307542\n",
      "53 epoch Loss: 1.287356\n",
      "54 epoch Loss: 1.279568\n",
      "55 epoch Loss: 1.250366\n",
      "56 epoch Loss: 1.272929\n",
      "57 epoch Loss: 1.225803\n",
      "58 epoch Loss: 1.257102\n",
      "59 epoch Loss: 1.264894\n",
      "60 epoch Loss: 1.237281\n",
      "61 epoch Loss: 1.229249\n",
      "62 epoch Loss: 1.184256\n",
      "63 epoch Loss: 1.209005\n",
      "64 epoch Loss: 1.164112\n",
      "65 epoch Loss: 1.197797\n",
      "66 epoch Loss: 1.203355\n",
      "67 epoch Loss: 1.184618\n",
      "68 epoch Loss: 1.197353\n",
      "69 epoch Loss: 1.200318\n",
      "70 epoch Loss: 1.189079\n",
      "71 epoch Loss: 1.168810\n",
      "72 epoch Loss: 1.160743\n",
      "73 epoch Loss: 1.162571\n",
      "74 epoch Loss: 1.202725\n",
      "75 epoch Loss: 1.154751\n",
      "76 epoch Loss: 1.152032\n",
      "77 epoch Loss: 1.175823\n",
      "78 epoch Loss: 1.140512\n",
      "79 epoch Loss: 1.121294\n",
      "80 epoch Loss: 1.133940\n",
      "81 epoch Loss: 1.092465\n",
      "82 epoch Loss: 1.098229\n",
      "83 epoch Loss: 1.104426\n",
      "84 epoch Loss: 1.088413\n",
      "85 epoch Loss: 1.115363\n",
      "86 epoch Loss: 1.128363\n",
      "87 epoch Loss: 1.120212\n",
      "88 epoch Loss: 1.094984\n",
      "89 epoch Loss: 1.133793\n",
      "90 epoch Loss: 1.096522\n",
      "91 epoch Loss: 1.116690\n",
      "92 epoch Loss: 1.117970\n",
      "93 epoch Loss: 1.074533\n",
      "94 epoch Loss: 1.092555\n",
      "95 epoch Loss: 1.099170\n",
      "96 epoch Loss: 1.070419\n",
      "97 epoch Loss: 1.089651\n",
      "98 epoch Loss: 1.069439\n",
      "99 epoch Loss: 1.093475\n",
      "100 epoch Loss: 1.088212\n",
      "101 epoch Loss: 1.094145\n",
      "102 epoch Loss: 1.075691\n",
      "103 epoch Loss: 1.086740\n",
      "104 epoch Loss: 1.086805\n",
      "105 epoch Loss: 1.081513\n",
      "106 epoch Loss: 1.059284\n",
      "107 epoch Loss: 1.111776\n",
      "108 epoch Loss: 1.068295\n",
      "109 epoch Loss: 1.077894\n",
      "110 epoch Loss: 1.087518\n",
      "111 epoch Loss: 1.062819\n",
      "112 epoch Loss: 1.055392\n",
      "113 epoch Loss: 1.071888\n",
      "114 epoch Loss: 1.055870\n",
      "115 epoch Loss: 1.080524\n",
      "116 epoch Loss: 1.037507\n",
      "117 epoch Loss: 1.066397\n",
      "118 epoch Loss: 1.050420\n",
      "119 epoch Loss: 1.023589\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 73.142857, Random forests accuracy: 72.428571, Ensemble accuracy: 77.428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.330684\n",
      "1 epoch Loss: 3.098176\n",
      "2 epoch Loss: 2.901483\n",
      "3 epoch Loss: 2.736322\n",
      "4 epoch Loss: 2.608670\n",
      "5 epoch Loss: 2.501938\n",
      "6 epoch Loss: 2.413657\n",
      "7 epoch Loss: 2.372069\n",
      "8 epoch Loss: 2.291968\n",
      "9 epoch Loss: 2.251850\n",
      "10 epoch Loss: 2.217970\n",
      "11 epoch Loss: 2.180146\n",
      "12 epoch Loss: 2.101232\n",
      "13 epoch Loss: 2.056569\n",
      "14 epoch Loss: 2.049433\n",
      "15 epoch Loss: 1.969337\n",
      "16 epoch Loss: 1.979553\n",
      "17 epoch Loss: 1.926075\n",
      "18 epoch Loss: 1.900314\n",
      "19 epoch Loss: 1.841048\n",
      "20 epoch Loss: 1.836024\n",
      "21 epoch Loss: 1.795052\n",
      "22 epoch Loss: 1.810438\n",
      "23 epoch Loss: 1.758356\n",
      "24 epoch Loss: 1.702030\n",
      "25 epoch Loss: 1.726604\n",
      "26 epoch Loss: 1.696688\n",
      "27 epoch Loss: 1.658168\n",
      "28 epoch Loss: 1.657789\n",
      "29 epoch Loss: 1.610607\n",
      "30 epoch Loss: 1.604543\n",
      "31 epoch Loss: 1.597301\n",
      "32 epoch Loss: 1.590726\n",
      "33 epoch Loss: 1.523730\n",
      "34 epoch Loss: 1.543259\n",
      "35 epoch Loss: 1.495998\n",
      "36 epoch Loss: 1.499728\n",
      "37 epoch Loss: 1.459510\n",
      "38 epoch Loss: 1.470810\n",
      "39 epoch Loss: 1.457613\n",
      "40 epoch Loss: 1.454880\n",
      "41 epoch Loss: 1.413947\n",
      "42 epoch Loss: 1.427306\n",
      "43 epoch Loss: 1.414364\n",
      "44 epoch Loss: 1.362879\n",
      "45 epoch Loss: 1.357395\n",
      "46 epoch Loss: 1.347968\n",
      "47 epoch Loss: 1.355368\n",
      "48 epoch Loss: 1.378855\n",
      "49 epoch Loss: 1.295284\n",
      "50 epoch Loss: 1.322877\n",
      "51 epoch Loss: 1.317071\n",
      "52 epoch Loss: 1.297791\n",
      "53 epoch Loss: 1.279255\n",
      "54 epoch Loss: 1.270622\n",
      "55 epoch Loss: 1.239739\n",
      "56 epoch Loss: 1.245773\n",
      "57 epoch Loss: 1.240540\n",
      "58 epoch Loss: 1.249179\n",
      "59 epoch Loss: 1.218546\n",
      "60 epoch Loss: 1.200831\n",
      "61 epoch Loss: 1.221534\n",
      "62 epoch Loss: 1.215071\n",
      "63 epoch Loss: 1.210751\n",
      "64 epoch Loss: 1.190707\n",
      "65 epoch Loss: 1.176551\n",
      "66 epoch Loss: 1.182001\n",
      "67 epoch Loss: 1.184699\n",
      "68 epoch Loss: 1.171469\n",
      "69 epoch Loss: 1.180104\n",
      "70 epoch Loss: 1.152999\n",
      "71 epoch Loss: 1.184708\n",
      "72 epoch Loss: 1.173940\n",
      "73 epoch Loss: 1.141244\n",
      "74 epoch Loss: 1.153051\n",
      "75 epoch Loss: 1.138871\n",
      "76 epoch Loss: 1.145711\n",
      "77 epoch Loss: 1.171297\n",
      "78 epoch Loss: 1.123770\n",
      "79 epoch Loss: 1.126071\n",
      "80 epoch Loss: 1.156311\n",
      "81 epoch Loss: 1.090598\n",
      "82 epoch Loss: 1.105544\n",
      "83 epoch Loss: 1.104876\n",
      "84 epoch Loss: 1.123922\n",
      "85 epoch Loss: 1.133043\n",
      "86 epoch Loss: 1.086665\n",
      "87 epoch Loss: 1.094149\n",
      "88 epoch Loss: 1.114868\n",
      "89 epoch Loss: 1.083724\n",
      "90 epoch Loss: 1.108416\n",
      "91 epoch Loss: 1.120988\n",
      "92 epoch Loss: 1.107257\n",
      "93 epoch Loss: 1.066653\n",
      "94 epoch Loss: 1.104918\n",
      "95 epoch Loss: 1.076417\n",
      "96 epoch Loss: 1.113470\n",
      "97 epoch Loss: 1.080754\n",
      "98 epoch Loss: 1.055707\n",
      "99 epoch Loss: 1.101630\n",
      "100 epoch Loss: 1.070935\n",
      "101 epoch Loss: 1.045677\n",
      "102 epoch Loss: 1.062943\n",
      "103 epoch Loss: 1.102452\n",
      "104 epoch Loss: 1.043689\n",
      "105 epoch Loss: 1.069135\n",
      "106 epoch Loss: 1.060810\n",
      "107 epoch Loss: 1.077377\n",
      "108 epoch Loss: 1.071221\n",
      "109 epoch Loss: 1.055705\n",
      "110 epoch Loss: 1.061999\n",
      "111 epoch Loss: 1.059066\n",
      "112 epoch Loss: 1.067213\n",
      "113 epoch Loss: 1.083847\n",
      "114 epoch Loss: 1.047898\n",
      "115 epoch Loss: 1.041148\n",
      "116 epoch Loss: 1.061490\n",
      "117 epoch Loss: 1.066432\n",
      "118 epoch Loss: 1.052212\n",
      "119 epoch Loss: 1.036887\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 73.428571, Random forests accuracy: 69.571429, Ensemble accuracy: 76.714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.327225\n",
      "1 epoch Loss: 3.111440\n",
      "2 epoch Loss: 2.888025\n",
      "3 epoch Loss: 2.725336\n",
      "4 epoch Loss: 2.596049\n",
      "5 epoch Loss: 2.489345\n",
      "6 epoch Loss: 2.413524\n",
      "7 epoch Loss: 2.356347\n",
      "8 epoch Loss: 2.318730\n",
      "9 epoch Loss: 2.242003\n",
      "10 epoch Loss: 2.206078\n",
      "11 epoch Loss: 2.176392\n",
      "12 epoch Loss: 2.114279\n",
      "13 epoch Loss: 2.069784\n",
      "14 epoch Loss: 2.028475\n",
      "15 epoch Loss: 1.991411\n",
      "16 epoch Loss: 1.979449\n",
      "17 epoch Loss: 1.897753\n",
      "18 epoch Loss: 1.914185\n",
      "19 epoch Loss: 1.837246\n",
      "20 epoch Loss: 1.868345\n",
      "21 epoch Loss: 1.833829\n",
      "22 epoch Loss: 1.802777\n",
      "23 epoch Loss: 1.756313\n",
      "24 epoch Loss: 1.738077\n",
      "25 epoch Loss: 1.750014\n",
      "26 epoch Loss: 1.717720\n",
      "27 epoch Loss: 1.705673\n",
      "28 epoch Loss: 1.641755\n",
      "29 epoch Loss: 1.629741\n",
      "30 epoch Loss: 1.600789\n",
      "31 epoch Loss: 1.589193\n",
      "32 epoch Loss: 1.561939\n",
      "33 epoch Loss: 1.572818\n",
      "34 epoch Loss: 1.537019\n",
      "35 epoch Loss: 1.531280\n",
      "36 epoch Loss: 1.510215\n",
      "37 epoch Loss: 1.478682\n",
      "38 epoch Loss: 1.498397\n",
      "39 epoch Loss: 1.455018\n",
      "40 epoch Loss: 1.443638\n",
      "41 epoch Loss: 1.444983\n",
      "42 epoch Loss: 1.429232\n",
      "43 epoch Loss: 1.391400\n",
      "44 epoch Loss: 1.416414\n",
      "45 epoch Loss: 1.390856\n",
      "46 epoch Loss: 1.363221\n",
      "47 epoch Loss: 1.362515\n",
      "48 epoch Loss: 1.329694\n",
      "49 epoch Loss: 1.333134\n",
      "50 epoch Loss: 1.292684\n",
      "51 epoch Loss: 1.314316\n",
      "52 epoch Loss: 1.294974\n",
      "53 epoch Loss: 1.282143\n",
      "54 epoch Loss: 1.260188\n",
      "55 epoch Loss: 1.311902\n",
      "56 epoch Loss: 1.255142\n",
      "57 epoch Loss: 1.256429\n",
      "58 epoch Loss: 1.249970\n",
      "59 epoch Loss: 1.224824\n",
      "60 epoch Loss: 1.233593\n",
      "61 epoch Loss: 1.225055\n",
      "62 epoch Loss: 1.212606\n",
      "63 epoch Loss: 1.207688\n",
      "64 epoch Loss: 1.197693\n",
      "65 epoch Loss: 1.206368\n",
      "66 epoch Loss: 1.206984\n",
      "67 epoch Loss: 1.175956\n",
      "68 epoch Loss: 1.186766\n",
      "69 epoch Loss: 1.159888\n",
      "70 epoch Loss: 1.154330\n",
      "71 epoch Loss: 1.151908\n",
      "72 epoch Loss: 1.135523\n",
      "73 epoch Loss: 1.138253\n",
      "74 epoch Loss: 1.145495\n",
      "75 epoch Loss: 1.153007\n",
      "76 epoch Loss: 1.156201\n",
      "77 epoch Loss: 1.137352\n",
      "78 epoch Loss: 1.118802\n",
      "79 epoch Loss: 1.120307\n",
      "80 epoch Loss: 1.166105\n",
      "81 epoch Loss: 1.123389\n",
      "82 epoch Loss: 1.117354\n",
      "83 epoch Loss: 1.137976\n",
      "84 epoch Loss: 1.106125\n",
      "85 epoch Loss: 1.111734\n",
      "86 epoch Loss: 1.124746\n",
      "87 epoch Loss: 1.100959\n",
      "88 epoch Loss: 1.091924\n",
      "89 epoch Loss: 1.097695\n",
      "90 epoch Loss: 1.109538\n",
      "91 epoch Loss: 1.074284\n",
      "92 epoch Loss: 1.085974\n",
      "93 epoch Loss: 1.092393\n",
      "94 epoch Loss: 1.115503\n",
      "95 epoch Loss: 1.101126\n",
      "96 epoch Loss: 1.077608\n",
      "97 epoch Loss: 1.081926\n",
      "98 epoch Loss: 1.094482\n",
      "99 epoch Loss: 1.063261\n",
      "100 epoch Loss: 1.082620\n",
      "101 epoch Loss: 1.056622\n",
      "102 epoch Loss: 1.062858\n",
      "103 epoch Loss: 1.080886\n",
      "104 epoch Loss: 1.064627\n",
      "105 epoch Loss: 1.082744\n",
      "106 epoch Loss: 1.032034\n",
      "107 epoch Loss: 1.086208\n",
      "108 epoch Loss: 1.070443\n",
      "109 epoch Loss: 1.022177\n",
      "110 epoch Loss: 1.059739\n",
      "111 epoch Loss: 1.083676\n",
      "112 epoch Loss: 1.076190\n",
      "113 epoch Loss: 1.058803\n",
      "114 epoch Loss: 1.058104\n",
      "115 epoch Loss: 1.040051\n",
      "116 epoch Loss: 1.076164\n",
      "117 epoch Loss: 1.079250\n",
      "118 epoch Loss: 1.062787\n",
      "119 epoch Loss: 1.062655\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.000000, Random forests accuracy: 72.285714, Ensemble accuracy: 79.000000\n",
      "12 epoch Loss: 2.086376\n",
      "13 epoch Loss: 2.075171\n",
      "14 epoch Loss: 2.068375\n",
      "15 epoch Loss: 1.977536\n",
      "16 epoch Loss: 1.966559\n",
      "17 epoch Loss: 1.934041\n",
      "18 epoch Loss: 1.870726\n",
      "19 epoch Loss: 1.874494\n",
      "20 epoch Loss: 1.851619\n",
      "21 epoch Loss: 1.796170\n",
      "22 epoch Loss: 1.809627\n",
      "23 epoch Loss: 1.742855\n",
      "24 epoch Loss: 1.718869\n",
      "25 epoch Loss: 1.718834\n",
      "26 epoch Loss: 1.696962\n",
      "27 epoch Loss: 1.660432\n",
      "28 epoch Loss: 1.663785\n",
      "29 epoch Loss: 1.626944\n",
      "30 epoch Loss: 1.598625\n",
      "31 epoch Loss: 1.594739\n",
      "32 epoch Loss: 1.563740\n",
      "33 epoch Loss: 1.552565\n",
      "34 epoch Loss: 1.543118\n",
      "35 epoch Loss: 1.498395\n",
      "36 epoch Loss: 1.502869\n",
      "37 epoch Loss: 1.526583\n",
      "38 epoch Loss: 1.458794\n",
      "39 epoch Loss: 1.439092\n",
      "40 epoch Loss: 1.472230\n",
      "41 epoch Loss: 1.415193\n",
      "42 epoch Loss: 1.399989\n",
      "43 epoch Loss: 1.424104\n",
      "44 epoch Loss: 1.373129\n",
      "45 epoch Loss: 1.383397\n",
      "46 epoch Loss: 1.356647\n",
      "47 epoch Loss: 1.344187\n",
      "48 epoch Loss: 1.328647\n",
      "49 epoch Loss: 1.334569\n",
      "50 epoch Loss: 1.303298\n",
      "51 epoch Loss: 1.286653\n",
      "52 epoch Loss: 1.299229\n",
      "53 epoch Loss: 1.279609\n",
      "54 epoch Loss: 1.251373\n",
      "55 epoch Loss: 1.266015\n",
      "56 epoch Loss: 1.274081\n",
      "57 epoch Loss: 1.256575\n",
      "58 epoch Loss: 1.245067\n",
      "59 epoch Loss: 1.219382\n",
      "60 epoch Loss: 1.219977\n",
      "61 epoch Loss: 1.214447\n",
      "62 epoch Loss: 1.212097\n",
      "63 epoch Loss: 1.225502\n",
      "64 epoch Loss: 1.183149\n",
      "65 epoch Loss: 1.204921\n",
      "66 epoch Loss: 1.202121\n",
      "67 epoch Loss: 1.189860\n",
      "68 epoch Loss: 1.154046\n",
      "69 epoch Loss: 1.179682\n",
      "70 epoch Loss: 1.150062\n",
      "71 epoch Loss: 1.141674\n",
      "72 epoch Loss: 1.152210\n",
      "73 epoch Loss: 1.173046\n",
      "74 epoch Loss: 1.197243\n",
      "75 epoch Loss: 1.154991\n",
      "76 epoch Loss: 1.156510\n",
      "77 epoch Loss: 1.156832\n",
      "78 epoch Loss: 1.130692\n",
      "79 epoch Loss: 1.144817\n",
      "80 epoch Loss: 1.136149\n",
      "81 epoch Loss: 1.097069\n",
      "82 epoch Loss: 1.128708\n",
      "83 epoch Loss: 1.127949\n",
      "84 epoch Loss: 1.114498\n",
      "85 epoch Loss: 1.129299\n",
      "86 epoch Loss: 1.118963\n",
      "87 epoch Loss: 1.084206\n",
      "88 epoch Loss: 1.114181\n",
      "89 epoch Loss: 1.083892\n",
      "90 epoch Loss: 1.113199\n",
      "91 epoch Loss: 1.086446\n",
      "92 epoch Loss: 1.106190\n",
      "93 epoch Loss: 1.086706\n",
      "94 epoch Loss: 1.086257\n",
      "95 epoch Loss: 1.099658\n",
      "96 epoch Loss: 1.075077\n",
      "97 epoch Loss: 1.076062\n",
      "98 epoch Loss: 1.068844\n",
      "99 epoch Loss: 1.091971\n",
      "100 epoch Loss: 1.082601\n",
      "101 epoch Loss: 1.074915\n",
      "102 epoch Loss: 1.092611\n",
      "103 epoch Loss: 1.081523\n",
      "104 epoch Loss: 1.059613\n",
      "105 epoch Loss: 1.084215\n",
      "106 epoch Loss: 1.069848\n",
      "107 epoch Loss: 1.063850\n",
      "108 epoch Loss: 1.073275\n",
      "109 epoch Loss: 1.077407\n",
      "110 epoch Loss: 1.048705\n",
      "111 epoch Loss: 1.079081\n",
      "112 epoch Loss: 1.062715\n",
      "113 epoch Loss: 1.048759\n",
      "114 epoch Loss: 1.058527\n",
      "115 epoch Loss: 1.013856\n",
      "116 epoch Loss: 1.073090\n",
      "117 epoch Loss: 1.068334\n",
      "118 epoch Loss: 1.041634\n",
      "119 epoch Loss: 1.059761\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 74.857143, Random forests accuracy: 71.714286, Ensemble accuracy: 79.285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.330170\n",
      "1 epoch Loss: 3.073026\n",
      "2 epoch Loss: 2.891737\n",
      "3 epoch Loss: 2.720230\n",
      "4 epoch Loss: 2.602851\n",
      "5 epoch Loss: 2.518633\n",
      "6 epoch Loss: 2.424627\n",
      "7 epoch Loss: 2.335692\n",
      "8 epoch Loss: 2.291162\n",
      "9 epoch Loss: 2.230673\n",
      "10 epoch Loss: 2.198061\n",
      "11 epoch Loss: 2.172966\n",
      "12 epoch Loss: 2.094472\n",
      "13 epoch Loss: 2.066220\n",
      "14 epoch Loss: 2.033127\n",
      "15 epoch Loss: 1.970390\n",
      "16 epoch Loss: 1.969504\n",
      "17 epoch Loss: 1.906925\n",
      "18 epoch Loss: 1.870318\n",
      "19 epoch Loss: 1.883837\n",
      "20 epoch Loss: 1.825450\n",
      "21 epoch Loss: 1.804081\n",
      "22 epoch Loss: 1.782444\n",
      "23 epoch Loss: 1.773128\n",
      "24 epoch Loss: 1.717856\n",
      "25 epoch Loss: 1.708159\n",
      "26 epoch Loss: 1.659128\n",
      "27 epoch Loss: 1.657682\n",
      "28 epoch Loss: 1.632473\n",
      "29 epoch Loss: 1.657372\n",
      "30 epoch Loss: 1.600248\n",
      "31 epoch Loss: 1.614060\n",
      "32 epoch Loss: 1.579103\n",
      "33 epoch Loss: 1.557483\n",
      "34 epoch Loss: 1.514350\n",
      "35 epoch Loss: 1.499392\n",
      "36 epoch Loss: 1.534287\n",
      "37 epoch Loss: 1.498369\n",
      "38 epoch Loss: 1.449335\n",
      "39 epoch Loss: 1.459734\n",
      "40 epoch Loss: 1.418920\n",
      "41 epoch Loss: 1.393918\n",
      "42 epoch Loss: 1.393485\n",
      "43 epoch Loss: 1.428656\n",
      "44 epoch Loss: 1.419293\n",
      "45 epoch Loss: 1.394192\n",
      "46 epoch Loss: 1.379669\n",
      "47 epoch Loss: 1.372848\n",
      "48 epoch Loss: 1.322432\n",
      "49 epoch Loss: 1.337841\n",
      "50 epoch Loss: 1.326697\n",
      "51 epoch Loss: 1.311239\n",
      "52 epoch Loss: 1.319591\n",
      "53 epoch Loss: 1.308228\n",
      "54 epoch Loss: 1.259945\n",
      "55 epoch Loss: 1.248736\n",
      "56 epoch Loss: 1.234376\n",
      "57 epoch Loss: 1.244241\n",
      "58 epoch Loss: 1.236710\n",
      "59 epoch Loss: 1.211151\n",
      "60 epoch Loss: 1.248526\n",
      "61 epoch Loss: 1.210315\n",
      "62 epoch Loss: 1.235963\n",
      "63 epoch Loss: 1.219237\n",
      "64 epoch Loss: 1.219949\n",
      "65 epoch Loss: 1.195712\n",
      "66 epoch Loss: 1.190258\n",
      "67 epoch Loss: 1.184977\n",
      "68 epoch Loss: 1.204209\n",
      "69 epoch Loss: 1.171114\n",
      "70 epoch Loss: 1.151387\n",
      "71 epoch Loss: 1.184019\n",
      "72 epoch Loss: 1.168791\n",
      "73 epoch Loss: 1.160485\n",
      "74 epoch Loss: 1.180899\n",
      "75 epoch Loss: 1.150848\n",
      "76 epoch Loss: 1.150546\n",
      "77 epoch Loss: 1.137679\n",
      "78 epoch Loss: 1.152176\n",
      "79 epoch Loss: 1.156931\n",
      "80 epoch Loss: 1.139269\n",
      "81 epoch Loss: 1.130806\n",
      "82 epoch Loss: 1.109745\n",
      "83 epoch Loss: 1.129826\n",
      "84 epoch Loss: 1.099142\n",
      "85 epoch Loss: 1.109824\n",
      "86 epoch Loss: 1.092491\n",
      "87 epoch Loss: 1.077632\n",
      "88 epoch Loss: 1.110680\n",
      "89 epoch Loss: 1.141159\n",
      "90 epoch Loss: 1.116689\n",
      "91 epoch Loss: 1.097604\n",
      "92 epoch Loss: 1.102053\n",
      "93 epoch Loss: 1.100981\n",
      "94 epoch Loss: 1.085489\n",
      "95 epoch Loss: 1.057957\n",
      "96 epoch Loss: 1.080610\n",
      "97 epoch Loss: 1.083899\n",
      "98 epoch Loss: 1.103007\n",
      "99 epoch Loss: 1.094415\n",
      "100 epoch Loss: 1.075223\n",
      "101 epoch Loss: 1.075011\n",
      "102 epoch Loss: 1.068824\n",
      "103 epoch Loss: 1.047805\n",
      "104 epoch Loss: 1.085113\n",
      "105 epoch Loss: 1.074765\n",
      "106 epoch Loss: 1.061713\n",
      "107 epoch Loss: 1.054404\n",
      "108 epoch Loss: 1.088483\n",
      "109 epoch Loss: 1.052042\n",
      "110 epoch Loss: 1.034226\n",
      "111 epoch Loss: 1.057075\n",
      "112 epoch Loss: 1.067566\n",
      "113 epoch Loss: 1.042726\n",
      "114 epoch Loss: 1.091900\n",
      "115 epoch Loss: 1.057295\n",
      "116 epoch Loss: 1.061819\n",
      "117 epoch Loss: 1.062734\n",
      "118 epoch Loss: 1.060347\n",
      "119 epoch Loss: 1.029638\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 71.857143, Random forests accuracy: 67.714286, Ensemble accuracy: 76.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.326372\n",
      "1 epoch Loss: 3.097404\n",
      "2 epoch Loss: 2.896088\n",
      "3 epoch Loss: 2.724529\n",
      "4 epoch Loss: 2.590405\n",
      "5 epoch Loss: 2.502122\n",
      "6 epoch Loss: 2.427063\n",
      "7 epoch Loss: 2.374562\n",
      "8 epoch Loss: 2.315371\n",
      "9 epoch Loss: 2.248153\n",
      "10 epoch Loss: 2.217150\n",
      "11 epoch Loss: 2.197077\n",
      "12 epoch Loss: 2.100427\n",
      "13 epoch Loss: 2.052589\n",
      "14 epoch Loss: 2.053485\n",
      "15 epoch Loss: 1.969469\n",
      "16 epoch Loss: 1.959413\n",
      "17 epoch Loss: 1.955279\n",
      "18 epoch Loss: 1.890554\n",
      "19 epoch Loss: 1.850054\n",
      "20 epoch Loss: 1.839471\n",
      "21 epoch Loss: 1.832326\n",
      "22 epoch Loss: 1.775065\n",
      "23 epoch Loss: 1.791409\n",
      "24 epoch Loss: 1.734396\n",
      "25 epoch Loss: 1.745961\n",
      "26 epoch Loss: 1.676202\n",
      "27 epoch Loss: 1.671829\n",
      "28 epoch Loss: 1.633082\n",
      "29 epoch Loss: 1.616621\n",
      "30 epoch Loss: 1.604703\n",
      "31 epoch Loss: 1.585916\n",
      "32 epoch Loss: 1.586336\n",
      "33 epoch Loss: 1.526196\n",
      "34 epoch Loss: 1.551329\n",
      "35 epoch Loss: 1.524247\n",
      "36 epoch Loss: 1.519660\n",
      "37 epoch Loss: 1.495516\n",
      "38 epoch Loss: 1.431759\n",
      "39 epoch Loss: 1.461460\n",
      "40 epoch Loss: 1.439446\n",
      "41 epoch Loss: 1.441304\n",
      "42 epoch Loss: 1.386002\n",
      "43 epoch Loss: 1.363707\n",
      "44 epoch Loss: 1.424680\n",
      "45 epoch Loss: 1.335517\n",
      "46 epoch Loss: 1.349589\n",
      "47 epoch Loss: 1.346494\n",
      "48 epoch Loss: 1.338321\n",
      "49 epoch Loss: 1.310413\n",
      "50 epoch Loss: 1.300998\n",
      "51 epoch Loss: 1.313872\n",
      "52 epoch Loss: 1.301451\n",
      "53 epoch Loss: 1.288040\n",
      "54 epoch Loss: 1.287506\n",
      "55 epoch Loss: 1.264091\n",
      "56 epoch Loss: 1.266203\n",
      "57 epoch Loss: 1.275607\n",
      "58 epoch Loss: 1.223873\n",
      "59 epoch Loss: 1.232266\n",
      "60 epoch Loss: 1.238430\n",
      "61 epoch Loss: 1.233512\n",
      "62 epoch Loss: 1.200380\n",
      "63 epoch Loss: 1.201508\n",
      "64 epoch Loss: 1.207249\n",
      "65 epoch Loss: 1.229770\n",
      "66 epoch Loss: 1.209923\n",
      "67 epoch Loss: 1.190967\n",
      "68 epoch Loss: 1.205905\n",
      "69 epoch Loss: 1.175983\n",
      "70 epoch Loss: 1.157709\n",
      "71 epoch Loss: 1.185390\n",
      "72 epoch Loss: 1.165711\n",
      "73 epoch Loss: 1.107944\n",
      "74 epoch Loss: 1.166111\n",
      "75 epoch Loss: 1.164968\n",
      "76 epoch Loss: 1.114409\n",
      "77 epoch Loss: 1.147461\n",
      "78 epoch Loss: 1.111532\n",
      "79 epoch Loss: 1.120410\n",
      "80 epoch Loss: 1.152670\n",
      "81 epoch Loss: 1.103916\n",
      "82 epoch Loss: 1.163420\n",
      "83 epoch Loss: 1.083022\n",
      "84 epoch Loss: 1.122596\n",
      "85 epoch Loss: 1.114616\n",
      "86 epoch Loss: 1.081036\n",
      "87 epoch Loss: 1.047599\n",
      "88 epoch Loss: 1.108254\n",
      "89 epoch Loss: 1.094984\n",
      "90 epoch Loss: 1.090399\n",
      "91 epoch Loss: 1.086280\n",
      "92 epoch Loss: 1.089579\n",
      "93 epoch Loss: 1.095444\n",
      "94 epoch Loss: 1.097836\n",
      "95 epoch Loss: 1.097707\n",
      "96 epoch Loss: 1.088507\n",
      "97 epoch Loss: 1.075702\n",
      "98 epoch Loss: 1.090273\n",
      "99 epoch Loss: 1.090144\n",
      "100 epoch Loss: 1.100713\n",
      "101 epoch Loss: 1.096112\n",
      "102 epoch Loss: 1.075435\n",
      "103 epoch Loss: 1.062594\n",
      "104 epoch Loss: 1.090253\n",
      "105 epoch Loss: 1.082656\n",
      "106 epoch Loss: 1.077525\n",
      "107 epoch Loss: 1.082876\n",
      "108 epoch Loss: 1.030344\n",
      "109 epoch Loss: 1.089443\n",
      "110 epoch Loss: 1.043740\n",
      "111 epoch Loss: 1.035526\n",
      "112 epoch Loss: 1.057321\n",
      "113 epoch Loss: 1.075629\n",
      "114 epoch Loss: 1.071359\n",
      "115 epoch Loss: 1.036872\n",
      "116 epoch Loss: 1.048985\n",
      "117 epoch Loss: 1.080455\n",
      "118 epoch Loss: 1.068115\n",
      "119 epoch Loss: 1.042705\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 71.857143, Random forests accuracy: 71.428571, Ensemble accuracy: 75.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.325386\n",
      "1 epoch Loss: 3.085493\n",
      "2 epoch Loss: 2.910002\n",
      "3 epoch Loss: 2.717391\n",
      "4 epoch Loss: 2.624450\n",
      "5 epoch Loss: 2.552645\n",
      "6 epoch Loss: 2.452383\n",
      "7 epoch Loss: 2.408678\n",
      "8 epoch Loss: 2.320904\n",
      "9 epoch Loss: 2.239176\n",
      "10 epoch Loss: 2.211683\n",
      "11 epoch Loss: 2.182303\n",
      "12 epoch Loss: 2.105785\n",
      "13 epoch Loss: 2.062523\n",
      "14 epoch Loss: 2.062348\n",
      "15 epoch Loss: 1.964934\n",
      "16 epoch Loss: 1.957919\n",
      "17 epoch Loss: 1.918122\n",
      "18 epoch Loss: 1.893287\n",
      "19 epoch Loss: 1.855096\n",
      "20 epoch Loss: 1.840839\n",
      "21 epoch Loss: 1.803353\n",
      "22 epoch Loss: 1.787819\n",
      "23 epoch Loss: 1.743573\n",
      "24 epoch Loss: 1.761649\n",
      "25 epoch Loss: 1.688066\n",
      "26 epoch Loss: 1.691305\n",
      "27 epoch Loss: 1.635191\n",
      "28 epoch Loss: 1.642420\n",
      "29 epoch Loss: 1.634089\n",
      "30 epoch Loss: 1.605181\n",
      "31 epoch Loss: 1.598105\n",
      "32 epoch Loss: 1.552852\n",
      "33 epoch Loss: 1.550497\n",
      "34 epoch Loss: 1.539861\n",
      "35 epoch Loss: 1.522690\n",
      "36 epoch Loss: 1.510912\n",
      "37 epoch Loss: 1.490194\n",
      "38 epoch Loss: 1.460418\n",
      "39 epoch Loss: 1.470480\n",
      "40 epoch Loss: 1.430765\n",
      "41 epoch Loss: 1.422127\n",
      "42 epoch Loss: 1.415961\n",
      "43 epoch Loss: 1.413211\n",
      "44 epoch Loss: 1.383643\n",
      "45 epoch Loss: 1.353905\n",
      "46 epoch Loss: 1.333086\n",
      "47 epoch Loss: 1.346822\n",
      "48 epoch Loss: 1.327038\n",
      "49 epoch Loss: 1.312063\n",
      "50 epoch Loss: 1.313184\n",
      "51 epoch Loss: 1.327968\n",
      "52 epoch Loss: 1.298910\n",
      "53 epoch Loss: 1.282392\n",
      "54 epoch Loss: 1.286830\n",
      "55 epoch Loss: 1.269386\n",
      "56 epoch Loss: 1.257158\n",
      "57 epoch Loss: 1.250662\n",
      "58 epoch Loss: 1.233930\n",
      "59 epoch Loss: 1.205923\n",
      "60 epoch Loss: 1.249049\n",
      "61 epoch Loss: 1.245723\n",
      "62 epoch Loss: 1.228965\n",
      "63 epoch Loss: 1.215335\n",
      "64 epoch Loss: 1.177785\n",
      "65 epoch Loss: 1.211963\n",
      "66 epoch Loss: 1.166230\n",
      "67 epoch Loss: 1.220930\n",
      "68 epoch Loss: 1.161799\n",
      "69 epoch Loss: 1.190016\n",
      "70 epoch Loss: 1.179817\n",
      "71 epoch Loss: 1.153670\n",
      "72 epoch Loss: 1.139316\n",
      "73 epoch Loss: 1.149150\n",
      "74 epoch Loss: 1.162216\n",
      "75 epoch Loss: 1.134050\n",
      "76 epoch Loss: 1.145532\n",
      "77 epoch Loss: 1.144721\n",
      "78 epoch Loss: 1.137719\n",
      "79 epoch Loss: 1.131547\n",
      "80 epoch Loss: 1.134492\n",
      "81 epoch Loss: 1.123996\n",
      "82 epoch Loss: 1.146019\n",
      "83 epoch Loss: 1.104333\n",
      "84 epoch Loss: 1.093780\n",
      "85 epoch Loss: 1.121189\n",
      "86 epoch Loss: 1.109765\n",
      "87 epoch Loss: 1.085698\n",
      "88 epoch Loss: 1.084324\n",
      "89 epoch Loss: 1.114118\n",
      "90 epoch Loss: 1.137576\n",
      "91 epoch Loss: 1.119564\n",
      "92 epoch Loss: 1.126991\n",
      "93 epoch Loss: 1.098006\n",
      "94 epoch Loss: 1.084759\n",
      "95 epoch Loss: 1.082810\n",
      "96 epoch Loss: 1.080233\n",
      "97 epoch Loss: 1.088930\n",
      "98 epoch Loss: 1.090938\n",
      "99 epoch Loss: 1.050428\n",
      "100 epoch Loss: 1.084989\n",
      "101 epoch Loss: 1.048235\n",
      "102 epoch Loss: 1.078331\n",
      "103 epoch Loss: 1.082838\n",
      "104 epoch Loss: 1.099123\n",
      "105 epoch Loss: 1.067877\n",
      "106 epoch Loss: 1.065082\n",
      "107 epoch Loss: 1.081128\n",
      "108 epoch Loss: 1.062972\n",
      "109 epoch Loss: 1.073656\n",
      "110 epoch Loss: 1.060971\n",
      "111 epoch Loss: 1.057384\n",
      "112 epoch Loss: 1.081059\n",
      "113 epoch Loss: 1.055886\n",
      "114 epoch Loss: 1.063540\n",
      "115 epoch Loss: 1.077081\n",
      "116 epoch Loss: 1.079654\n",
      "117 epoch Loss: 1.049702\n",
      "118 epoch Loss: 1.054558\n",
      "119 epoch Loss: 1.058710\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 73.857143, Random forests accuracy: 71.714286, Ensemble accuracy: 77.285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.327216\n",
      "1 epoch Loss: 3.092957\n",
      "2 epoch Loss: 2.908282\n",
      "3 epoch Loss: 2.724093\n",
      "4 epoch Loss: 2.618734\n",
      "5 epoch Loss: 2.514091\n",
      "6 epoch Loss: 2.422679\n",
      "7 epoch Loss: 2.381492\n",
      "8 epoch Loss: 2.320384\n",
      "9 epoch Loss: 2.259322\n",
      "10 epoch Loss: 2.191810\n",
      "11 epoch Loss: 2.176286\n",
      "12 epoch Loss: 2.111419\n",
      "13 epoch Loss: 2.092995\n",
      "14 epoch Loss: 2.035712\n",
      "15 epoch Loss: 1.989474\n",
      "16 epoch Loss: 1.964837\n",
      "17 epoch Loss: 1.902735\n",
      "18 epoch Loss: 1.891673\n",
      "19 epoch Loss: 1.883464\n",
      "20 epoch Loss: 1.849067\n",
      "21 epoch Loss: 1.840981\n",
      "22 epoch Loss: 1.763605\n",
      "23 epoch Loss: 1.741504\n",
      "24 epoch Loss: 1.734866\n",
      "25 epoch Loss: 1.704296\n",
      "26 epoch Loss: 1.679320\n",
      "27 epoch Loss: 1.658299\n",
      "28 epoch Loss: 1.659007\n",
      "29 epoch Loss: 1.641684\n",
      "30 epoch Loss: 1.604883\n",
      "31 epoch Loss: 1.595392\n",
      "32 epoch Loss: 1.573929\n",
      "33 epoch Loss: 1.537259\n",
      "34 epoch Loss: 1.512791\n",
      "35 epoch Loss: 1.532692\n",
      "36 epoch Loss: 1.522046\n",
      "37 epoch Loss: 1.482898\n",
      "38 epoch Loss: 1.458356\n",
      "39 epoch Loss: 1.434351\n",
      "40 epoch Loss: 1.473746\n",
      "41 epoch Loss: 1.433989\n",
      "42 epoch Loss: 1.427444\n",
      "43 epoch Loss: 1.401165\n",
      "44 epoch Loss: 1.383819\n",
      "45 epoch Loss: 1.388625\n",
      "46 epoch Loss: 1.358065\n",
      "47 epoch Loss: 1.350735\n",
      "48 epoch Loss: 1.321374\n",
      "49 epoch Loss: 1.348901\n",
      "50 epoch Loss: 1.307278\n",
      "51 epoch Loss: 1.300792\n",
      "52 epoch Loss: 1.294306\n",
      "53 epoch Loss: 1.295719\n",
      "54 epoch Loss: 1.314294\n",
      "55 epoch Loss: 1.259156\n",
      "56 epoch Loss: 1.236411\n",
      "57 epoch Loss: 1.237141\n",
      "58 epoch Loss: 1.279957\n",
      "59 epoch Loss: 1.223906\n",
      "60 epoch Loss: 1.261338\n",
      "61 epoch Loss: 1.228172\n",
      "62 epoch Loss: 1.213758\n",
      "63 epoch Loss: 1.198008\n",
      "64 epoch Loss: 1.193527\n",
      "65 epoch Loss: 1.211730\n",
      "66 epoch Loss: 1.171574\n",
      "67 epoch Loss: 1.201058\n",
      "68 epoch Loss: 1.184294\n",
      "69 epoch Loss: 1.168522\n",
      "70 epoch Loss: 1.144558\n",
      "71 epoch Loss: 1.181168\n",
      "72 epoch Loss: 1.161175\n",
      "73 epoch Loss: 1.162452\n",
      "74 epoch Loss: 1.169265\n",
      "75 epoch Loss: 1.134819\n",
      "76 epoch Loss: 1.114697\n",
      "77 epoch Loss: 1.146051\n",
      "78 epoch Loss: 1.136121\n",
      "79 epoch Loss: 1.145093\n",
      "80 epoch Loss: 1.138914\n",
      "81 epoch Loss: 1.131157\n",
      "82 epoch Loss: 1.123994\n",
      "83 epoch Loss: 1.107443\n",
      "84 epoch Loss: 1.149081\n",
      "85 epoch Loss: 1.104853\n",
      "86 epoch Loss: 1.093193\n",
      "87 epoch Loss: 1.120930\n",
      "88 epoch Loss: 1.091380\n",
      "89 epoch Loss: 1.118641\n",
      "90 epoch Loss: 1.098885\n",
      "91 epoch Loss: 1.098395\n",
      "92 epoch Loss: 1.087726\n",
      "93 epoch Loss: 1.082996\n",
      "94 epoch Loss: 1.091141\n",
      "95 epoch Loss: 1.073007\n",
      "96 epoch Loss: 1.090656\n",
      "97 epoch Loss: 1.067341\n",
      "98 epoch Loss: 1.067743\n",
      "99 epoch Loss: 1.047428\n",
      "100 epoch Loss: 1.054867\n",
      "101 epoch Loss: 1.080411\n",
      "102 epoch Loss: 1.054346\n",
      "103 epoch Loss: 1.074052\n",
      "104 epoch Loss: 1.075051\n",
      "105 epoch Loss: 1.070415\n",
      "106 epoch Loss: 1.064824\n",
      "107 epoch Loss: 1.078815\n",
      "108 epoch Loss: 1.071843\n",
      "109 epoch Loss: 1.038572\n",
      "110 epoch Loss: 1.055128\n",
      "111 epoch Loss: 1.044418\n",
      "112 epoch Loss: 1.061407\n",
      "113 epoch Loss: 1.052113\n",
      "114 epoch Loss: 1.061024\n",
      "115 epoch Loss: 1.058066\n",
      "116 epoch Loss: 1.067595\n",
      "117 epoch Loss: 1.048259\n",
      "118 epoch Loss: 1.074463\n",
      "119 epoch Loss: 1.023446\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 74.000000, Random forests accuracy: 70.428571, Ensemble accuracy: 78.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.309809\n",
      "1 epoch Loss: 3.096803\n",
      "2 epoch Loss: 2.891518\n",
      "3 epoch Loss: 2.699732\n",
      "4 epoch Loss: 2.605929\n",
      "5 epoch Loss: 2.525665\n",
      "6 epoch Loss: 2.390589\n",
      "7 epoch Loss: 2.366087\n",
      "8 epoch Loss: 2.285006\n",
      "9 epoch Loss: 2.261144\n",
      "10 epoch Loss: 2.196422\n",
      "11 epoch Loss: 2.142839\n",
      "12 epoch Loss: 2.114570\n",
      "13 epoch Loss: 2.095521\n",
      "14 epoch Loss: 2.046006\n",
      "15 epoch Loss: 1.965220\n",
      "16 epoch Loss: 1.953766\n",
      "17 epoch Loss: 1.921729\n",
      "18 epoch Loss: 1.883373\n",
      "19 epoch Loss: 1.908957\n",
      "20 epoch Loss: 1.840869\n",
      "21 epoch Loss: 1.820771\n",
      "22 epoch Loss: 1.808468\n",
      "23 epoch Loss: 1.766584\n",
      "24 epoch Loss: 1.759893\n",
      "25 epoch Loss: 1.698545\n",
      "26 epoch Loss: 1.701962\n",
      "27 epoch Loss: 1.671983\n",
      "28 epoch Loss: 1.665029\n",
      "29 epoch Loss: 1.598361\n",
      "30 epoch Loss: 1.599022\n",
      "31 epoch Loss: 1.611054\n",
      "32 epoch Loss: 1.569490\n",
      "33 epoch Loss: 1.540706\n",
      "34 epoch Loss: 1.529610\n",
      "35 epoch Loss: 1.520298\n",
      "36 epoch Loss: 1.507683\n",
      "37 epoch Loss: 1.477271\n",
      "38 epoch Loss: 1.474683\n",
      "39 epoch Loss: 1.437739\n",
      "40 epoch Loss: 1.438386\n",
      "41 epoch Loss: 1.471643\n",
      "42 epoch Loss: 1.428858\n",
      "43 epoch Loss: 1.414310\n",
      "44 epoch Loss: 1.385144\n",
      "45 epoch Loss: 1.358100\n",
      "46 epoch Loss: 1.333100\n",
      "47 epoch Loss: 1.351138\n",
      "48 epoch Loss: 1.340651\n",
      "49 epoch Loss: 1.311859\n",
      "50 epoch Loss: 1.320677\n",
      "51 epoch Loss: 1.305510\n",
      "52 epoch Loss: 1.292061\n",
      "53 epoch Loss: 1.265208\n",
      "54 epoch Loss: 1.297313\n",
      "55 epoch Loss: 1.277167\n",
      "56 epoch Loss: 1.284452\n",
      "57 epoch Loss: 1.237251\n",
      "58 epoch Loss: 1.244617\n",
      "59 epoch Loss: 1.226620\n",
      "60 epoch Loss: 1.252755\n",
      "61 epoch Loss: 1.231590\n",
      "62 epoch Loss: 1.224750\n",
      "63 epoch Loss: 1.206920\n",
      "64 epoch Loss: 1.221567\n",
      "65 epoch Loss: 1.165783\n",
      "66 epoch Loss: 1.170212\n",
      "67 epoch Loss: 1.165738\n",
      "68 epoch Loss: 1.147348\n",
      "69 epoch Loss: 1.162199\n",
      "70 epoch Loss: 1.165386\n",
      "71 epoch Loss: 1.129372\n",
      "72 epoch Loss: 1.170908\n",
      "73 epoch Loss: 1.137476\n",
      "74 epoch Loss: 1.173873\n",
      "75 epoch Loss: 1.139567\n",
      "76 epoch Loss: 1.168572\n",
      "77 epoch Loss: 1.159874\n",
      "78 epoch Loss: 1.148792\n",
      "79 epoch Loss: 1.124309\n",
      "80 epoch Loss: 1.118873\n",
      "81 epoch Loss: 1.123456\n",
      "82 epoch Loss: 1.129247\n",
      "83 epoch Loss: 1.106141\n",
      "84 epoch Loss: 1.099948\n",
      "85 epoch Loss: 1.098683\n",
      "86 epoch Loss: 1.128470\n",
      "87 epoch Loss: 1.115095\n",
      "88 epoch Loss: 1.109506\n",
      "89 epoch Loss: 1.091538\n",
      "90 epoch Loss: 1.088919\n",
      "91 epoch Loss: 1.119558\n",
      "92 epoch Loss: 1.102485\n",
      "93 epoch Loss: 1.082117\n",
      "94 epoch Loss: 1.107682\n",
      "95 epoch Loss: 1.095579\n",
      "96 epoch Loss: 1.078696\n",
      "97 epoch Loss: 1.073357\n",
      "98 epoch Loss: 1.078547\n",
      "99 epoch Loss: 1.070578\n",
      "100 epoch Loss: 1.090130\n",
      "101 epoch Loss: 1.064145\n",
      "102 epoch Loss: 1.096891\n",
      "103 epoch Loss: 1.040082\n",
      "104 epoch Loss: 1.077736\n",
      "105 epoch Loss: 1.076698\n",
      "106 epoch Loss: 1.073597\n",
      "107 epoch Loss: 1.060300\n",
      "108 epoch Loss: 1.056172\n",
      "109 epoch Loss: 1.039068\n",
      "110 epoch Loss: 1.095030\n",
      "111 epoch Loss: 1.073386\n",
      "112 epoch Loss: 1.037386\n",
      "113 epoch Loss: 1.038783\n",
      "114 epoch Loss: 1.045363\n",
      "115 epoch Loss: 1.058493\n",
      "116 epoch Loss: 1.066527\n",
      "117 epoch Loss: 1.065432\n",
      "118 epoch Loss: 1.076444\n",
      "119 epoch Loss: 1.030047\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 73.219373, Random forests accuracy: 67.948718, Ensemble accuracy: 76.495726\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#Load data\n",
    "\n",
    "    \n",
    "    #Initialize\n",
    "    label -= 1\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "    t_index = np.random.permutation(int(np.shape(data)[0]/Outer_loop)*Outer_loop)\n",
    "    t_index = np.reshape(t_index, [Outer_loop, -1])\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    box = np.array([4000], dtype=np.int32)\n",
    "    flag = 0\n",
    "    for test_index in t_index:\n",
    "        if flag == Outer_loop-1:\n",
    "            test_index = np.array(np.concatenate((test_index, np.array(range(int(np.shape(data)[0]/Outer_loop)*Outer_loop,np.shape(data)[0]))), axis=0), dtype=np.int32)\n",
    "        train_index = np.setdiff1d(np.array(range(0,np.shape(data)[0])), test_index)\n",
    "        train_data = data[train_index]\n",
    "        train_label = label[train_index]\n",
    "        test_data = data[test_index]\n",
    "        test_label = label[test_index]\n",
    "        \n",
    "        kf = np.random.permutation(int(np.shape(train_data)[0]/Inner_loop)*Inner_loop)\n",
    "        kf = kf.reshape([Inner_loop]+[-1])\n",
    "        val_result = np.zeros([np.shape(train_data)[0],48], dtype=np.float32)\n",
    "        \n",
    "        tot_acc = np.zeros([Inner_loop,5], dtype=np.float32)\n",
    "        lasso = Lasso(alpha=0.0003).fit(data, label)#alpha=0.025\n",
    "        #lsvc = LinearSVC(C=1, penalty=\"l1\", dual=False).fit(data, label)\n",
    "        #coef = np.squeeze(np.sum(np.square(np.array(lsvc.coef_)), axis=0))\n",
    "        coef = lasso.coef_\n",
    "        coefidx = np.argsort(coef)\n",
    "        # for inner_fold in range(0,Inner_loop):\n",
    "        #     val_test_ind = kf[inner_fold]\n",
    "        #     if inner_fold == Inner_loop-1:\n",
    "        #         val_test_ind = np.array(np.concatenate((val_test_ind,np.array(range(int(np.shape(train_data)[0]/Outer_loop)*Outer_loop,np.shape(train_data)[0]),dtype=np.int32)), axis=0),dtype=np.int32)\n",
    "            \n",
    "        #     val_train_ind = np.setdiff1d(np.array(range(0,np.shape(train_data)[0]),dtype=np.int32), val_test_ind)\n",
    "        #     val_train = train_data[val_train_ind]\n",
    "        #     val_test = train_data[val_test_ind]\n",
    "        #     val_train_label = train_label[val_train_ind]\n",
    "        #     val_test_label = train_label[val_test_ind]\n",
    "        #     temp = 0\n",
    "        #     for item in box:\n",
    "        #         idx = coefidx[-item:]\n",
    "        #         vtrain = val_train[:,idx]\n",
    "        #         vtest = val_test[:,idx]\n",
    "        #         nn_acc, result_nn = dnn(vtrain, val_train_label, vtest, val_test_label)\n",
    "        #         rf_acc, result_rf = rfc(vtrain, val_train_label, vtest, val_test_label)\n",
    "        #         en_acc = 0.0\n",
    "        #         for i in range(0,np.shape(vtest)[0]):\n",
    "        #             r = np.argmax(result_nn[i]+result_rf[i])\n",
    "        #             if r == val_test_label[i]:\n",
    "        #                 en_acc += 1\n",
    "        #         en_acc /= np.shape(vtest)[0]*0.01\n",
    "        #         tot_acc[inner_fold,temp] = en_acc\n",
    "        #         print(\"Inner_fold # of features: %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (item, nn_acc, rf_acc, en_acc))\n",
    "        #         temp += 1\n",
    "        \n",
    "        u = np.sum(tot_acc,0)\n",
    "       \n",
    "        best_n = box[np.argmax(u)]\n",
    "        idx = coefidx[-best_n:]\n",
    "        \n",
    "        tr_data = train_data[:,idx]\n",
    "        te_data = test_data[:,idx]\n",
    "        nn_acc, result_nn = dnn(tr_data, train_label, te_data, test_label)\n",
    "        rf_acc, result_rf = rfc(tr_data, train_label, te_data, test_label)\n",
    "        #print(rf_acc)\n",
    "        en_acc = 0.0\n",
    "        for i in range(0,np.shape(te_data)[0]):\n",
    "            r = np.argmax(result_nn[i]+result_rf[i])\n",
    "            if r == test_label[i]:\n",
    "                en_acc += 1\n",
    "        en_acc /= np.shape(te_data)[0]*0.01\n",
    "        print(\"Outer_fold # of features:  %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (best_n, nn_acc, rf_acc, en_acc))\n",
    "        flag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy':MD_acc,'Feature Number':MD_num, 'Model':MD_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\",rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x = \"Feature Number\", y = \"Accuracy\", hue=\"Model\", data = df, palette = \"Set3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From <ipython-input-2-6d0cdc7b68c3>:5: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-5-5c2fee5bad1b>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "0 epoch Loss: 3.209282\n",
      "1 epoch Loss: 2.827017\n",
      "2 epoch Loss: 2.620695\n",
      "3 epoch Loss: 2.477607\n",
      "4 epoch Loss: 2.435844\n",
      "5 epoch Loss: 2.396015\n",
      "6 epoch Loss: 2.309214\n",
      "7 epoch Loss: 2.286460\n",
      "8 epoch Loss: 2.231590\n",
      "9 epoch Loss: 2.203421\n",
      "10 epoch Loss: 2.182578\n",
      "11 epoch Loss: 2.141569\n",
      "12 epoch Loss: 2.104564\n",
      "13 epoch Loss: 2.077090\n",
      "14 epoch Loss: 2.066579\n",
      "15 epoch Loss: 2.036577\n",
      "16 epoch Loss: 1.991989\n",
      "17 epoch Loss: 1.973554\n",
      "18 epoch Loss: 1.934472\n",
      "19 epoch Loss: 1.937561\n",
      "20 epoch Loss: 1.910729\n",
      "21 epoch Loss: 1.896951\n",
      "22 epoch Loss: 1.861580\n",
      "23 epoch Loss: 1.867949\n",
      "24 epoch Loss: 1.819347\n",
      "25 epoch Loss: 1.846358\n",
      "26 epoch Loss: 1.811970\n",
      "27 epoch Loss: 1.804945\n",
      "28 epoch Loss: 1.751564\n",
      "29 epoch Loss: 1.771110\n",
      "30 epoch Loss: 1.774357\n",
      "31 epoch Loss: 1.708503\n",
      "32 epoch Loss: 1.720085\n",
      "33 epoch Loss: 1.699811\n",
      "34 epoch Loss: 1.687884\n",
      "35 epoch Loss: 1.659552\n",
      "36 epoch Loss: 1.675577\n",
      "37 epoch Loss: 1.657306\n",
      "38 epoch Loss: 1.668944\n",
      "39 epoch Loss: 1.609033\n",
      "40 epoch Loss: 1.597860\n",
      "41 epoch Loss: 1.629263\n",
      "42 epoch Loss: 1.629772\n",
      "43 epoch Loss: 1.612509\n",
      "44 epoch Loss: 1.570701\n",
      "45 epoch Loss: 1.596633\n",
      "46 epoch Loss: 1.573606\n",
      "47 epoch Loss: 1.532938\n",
      "48 epoch Loss: 1.560840\n",
      "49 epoch Loss: 1.515897\n",
      "50 epoch Loss: 1.515131\n",
      "51 epoch Loss: 1.540210\n",
      "52 epoch Loss: 1.493104\n",
      "53 epoch Loss: 1.517648\n",
      "54 epoch Loss: 1.506274\n",
      "55 epoch Loss: 1.479154\n",
      "56 epoch Loss: 1.466595\n",
      "57 epoch Loss: 1.476683\n",
      "58 epoch Loss: 1.456193\n",
      "59 epoch Loss: 1.466423\n",
      "60 epoch Loss: 1.450647\n",
      "61 epoch Loss: 1.430826\n",
      "62 epoch Loss: 1.440701\n",
      "63 epoch Loss: 1.439793\n",
      "64 epoch Loss: 1.442881\n",
      "65 epoch Loss: 1.453407\n",
      "66 epoch Loss: 1.420769\n",
      "67 epoch Loss: 1.443077\n",
      "68 epoch Loss: 1.388837\n",
      "69 epoch Loss: 1.400138\n",
      "70 epoch Loss: 1.390833\n",
      "71 epoch Loss: 1.387637\n",
      "72 epoch Loss: 1.374956\n",
      "73 epoch Loss: 1.363515\n",
      "74 epoch Loss: 1.366305\n",
      "75 epoch Loss: 1.360269\n",
      "76 epoch Loss: 1.356889\n",
      "77 epoch Loss: 1.360072\n",
      "78 epoch Loss: 1.340537\n",
      "79 epoch Loss: 1.387848\n",
      "80 epoch Loss: 1.338776\n",
      "81 epoch Loss: 1.344364\n",
      "82 epoch Loss: 1.360453\n",
      "83 epoch Loss: 1.318886\n",
      "84 epoch Loss: 1.330406\n",
      "85 epoch Loss: 1.320710\n",
      "86 epoch Loss: 1.332210\n",
      "87 epoch Loss: 1.332001\n",
      "88 epoch Loss: 1.327889\n",
      "89 epoch Loss: 1.322383\n",
      "90 epoch Loss: 1.311811\n",
      "91 epoch Loss: 1.302233\n",
      "92 epoch Loss: 1.296106\n",
      "93 epoch Loss: 1.295137\n",
      "94 epoch Loss: 1.275435\n",
      "95 epoch Loss: 1.303590\n",
      "96 epoch Loss: 1.250009\n",
      "97 epoch Loss: 1.278234\n",
      "98 epoch Loss: 1.293900\n",
      "99 epoch Loss: 1.282403\n",
      "100 epoch Loss: 1.276168\n",
      "101 epoch Loss: 1.306216\n",
      "102 epoch Loss: 1.264630\n",
      "103 epoch Loss: 1.249405\n",
      "104 epoch Loss: 1.253808\n",
      "105 epoch Loss: 1.217027\n",
      "106 epoch Loss: 1.268906\n",
      "107 epoch Loss: 1.239144\n",
      "108 epoch Loss: 1.251041\n",
      "109 epoch Loss: 1.216740\n",
      "110 epoch Loss: 1.227095\n",
      "111 epoch Loss: 1.226681\n",
      "112 epoch Loss: 1.249735\n",
      "113 epoch Loss: 1.224908\n",
      "114 epoch Loss: 1.231597\n",
      "115 epoch Loss: 1.197438\n",
      "116 epoch Loss: 1.226345\n",
      "117 epoch Loss: 1.235605\n",
      "118 epoch Loss: 1.220996\n",
      "119 epoch Loss: 1.227322\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.571429, Random forests accuracy: 75.857143, Ensemble accuracy: 81.142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.210805\n",
      "1 epoch Loss: 2.836194\n",
      "2 epoch Loss: 2.599999\n",
      "3 epoch Loss: 2.470158\n",
      "4 epoch Loss: 2.447382\n",
      "5 epoch Loss: 2.360404\n",
      "6 epoch Loss: 2.311284\n",
      "7 epoch Loss: 2.275073\n",
      "8 epoch Loss: 2.227043\n",
      "9 epoch Loss: 2.201270\n",
      "10 epoch Loss: 2.167644\n",
      "11 epoch Loss: 2.183408\n",
      "12 epoch Loss: 2.111639\n",
      "13 epoch Loss: 2.079440\n",
      "14 epoch Loss: 2.062978\n",
      "15 epoch Loss: 2.025075\n",
      "16 epoch Loss: 2.012119\n",
      "17 epoch Loss: 1.973272\n",
      "18 epoch Loss: 1.954596\n",
      "19 epoch Loss: 1.936935\n",
      "20 epoch Loss: 1.921010\n",
      "21 epoch Loss: 1.878256\n",
      "22 epoch Loss: 1.849393\n",
      "23 epoch Loss: 1.855506\n",
      "24 epoch Loss: 1.857439\n",
      "25 epoch Loss: 1.847508\n",
      "26 epoch Loss: 1.792528\n",
      "27 epoch Loss: 1.776427\n",
      "28 epoch Loss: 1.743725\n",
      "29 epoch Loss: 1.757232\n",
      "30 epoch Loss: 1.755835\n",
      "31 epoch Loss: 1.765947\n",
      "32 epoch Loss: 1.724109\n",
      "33 epoch Loss: 1.713219\n",
      "34 epoch Loss: 1.682474\n",
      "35 epoch Loss: 1.703318\n",
      "36 epoch Loss: 1.657238\n",
      "37 epoch Loss: 1.677992\n",
      "38 epoch Loss: 1.638878\n",
      "39 epoch Loss: 1.600546\n",
      "40 epoch Loss: 1.630365\n",
      "41 epoch Loss: 1.632619\n",
      "42 epoch Loss: 1.611363\n",
      "43 epoch Loss: 1.595544\n",
      "44 epoch Loss: 1.591711\n",
      "45 epoch Loss: 1.553456\n",
      "46 epoch Loss: 1.559564\n",
      "47 epoch Loss: 1.541587\n",
      "48 epoch Loss: 1.541072\n",
      "49 epoch Loss: 1.521285\n",
      "50 epoch Loss: 1.538642\n",
      "51 epoch Loss: 1.524906\n",
      "52 epoch Loss: 1.527917\n",
      "53 epoch Loss: 1.492752\n",
      "54 epoch Loss: 1.515749\n",
      "55 epoch Loss: 1.500749\n",
      "56 epoch Loss: 1.463022\n",
      "57 epoch Loss: 1.458402\n",
      "58 epoch Loss: 1.450455\n",
      "59 epoch Loss: 1.474805\n",
      "60 epoch Loss: 1.463240\n",
      "61 epoch Loss: 1.444690\n",
      "62 epoch Loss: 1.446376\n",
      "63 epoch Loss: 1.393383\n",
      "64 epoch Loss: 1.397668\n",
      "65 epoch Loss: 1.447577\n",
      "66 epoch Loss: 1.438483\n",
      "67 epoch Loss: 1.416188\n",
      "68 epoch Loss: 1.398534\n",
      "69 epoch Loss: 1.369106\n",
      "70 epoch Loss: 1.380979\n",
      "71 epoch Loss: 1.385356\n",
      "72 epoch Loss: 1.378182\n",
      "73 epoch Loss: 1.365472\n",
      "74 epoch Loss: 1.375577\n",
      "75 epoch Loss: 1.379186\n",
      "76 epoch Loss: 1.349190\n",
      "77 epoch Loss: 1.340731\n",
      "78 epoch Loss: 1.327601\n",
      "79 epoch Loss: 1.372964\n",
      "80 epoch Loss: 1.332236\n",
      "81 epoch Loss: 1.342664\n",
      "82 epoch Loss: 1.310116\n",
      "83 epoch Loss: 1.331536\n",
      "84 epoch Loss: 1.337574\n",
      "85 epoch Loss: 1.344959\n",
      "86 epoch Loss: 1.305341\n",
      "87 epoch Loss: 1.319262\n",
      "88 epoch Loss: 1.321807\n",
      "89 epoch Loss: 1.313150\n",
      "90 epoch Loss: 1.295972\n",
      "91 epoch Loss: 1.292855\n",
      "92 epoch Loss: 1.309513\n",
      "93 epoch Loss: 1.288059\n",
      "94 epoch Loss: 1.273683\n",
      "95 epoch Loss: 1.258252\n",
      "96 epoch Loss: 1.303940\n",
      "97 epoch Loss: 1.268048\n",
      "98 epoch Loss: 1.298861\n",
      "99 epoch Loss: 1.259010\n",
      "100 epoch Loss: 1.253585\n",
      "101 epoch Loss: 1.274268\n",
      "102 epoch Loss: 1.273182\n",
      "103 epoch Loss: 1.247935\n",
      "104 epoch Loss: 1.273113\n",
      "105 epoch Loss: 1.244025\n",
      "106 epoch Loss: 1.256552\n",
      "107 epoch Loss: 1.222984\n",
      "108 epoch Loss: 1.234121\n",
      "109 epoch Loss: 1.252225\n",
      "110 epoch Loss: 1.219306\n",
      "111 epoch Loss: 1.211861\n",
      "112 epoch Loss: 1.247779\n",
      "113 epoch Loss: 1.245526\n",
      "114 epoch Loss: 1.266183\n",
      "115 epoch Loss: 1.202050\n",
      "116 epoch Loss: 1.210592\n",
      "117 epoch Loss: 1.231930\n",
      "118 epoch Loss: 1.247152\n",
      "119 epoch Loss: 1.209171\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 78.285714, Random forests accuracy: 72.428571, Ensemble accuracy: 81.428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.203515\n",
      "1 epoch Loss: 2.801920\n",
      "2 epoch Loss: 2.622147\n",
      "3 epoch Loss: 2.483913\n",
      "4 epoch Loss: 2.401751\n",
      "5 epoch Loss: 2.391560\n",
      "6 epoch Loss: 2.336203\n",
      "7 epoch Loss: 2.282487\n",
      "8 epoch Loss: 2.246838\n",
      "9 epoch Loss: 2.247793\n",
      "10 epoch Loss: 2.195043\n",
      "11 epoch Loss: 2.150988\n",
      "12 epoch Loss: 2.116648\n",
      "13 epoch Loss: 2.090184\n",
      "14 epoch Loss: 2.085944\n",
      "15 epoch Loss: 2.026066\n",
      "16 epoch Loss: 2.010385\n",
      "17 epoch Loss: 1.989815\n",
      "18 epoch Loss: 1.946298\n",
      "19 epoch Loss: 1.949751\n",
      "20 epoch Loss: 1.914418\n",
      "21 epoch Loss: 1.891141\n",
      "22 epoch Loss: 1.895193\n",
      "23 epoch Loss: 1.832577\n",
      "24 epoch Loss: 1.850201\n",
      "25 epoch Loss: 1.826805\n",
      "26 epoch Loss: 1.786491\n",
      "27 epoch Loss: 1.803039\n",
      "28 epoch Loss: 1.732451\n",
      "29 epoch Loss: 1.749785\n",
      "30 epoch Loss: 1.731938\n",
      "31 epoch Loss: 1.747027\n",
      "32 epoch Loss: 1.709315\n",
      "33 epoch Loss: 1.648822\n",
      "34 epoch Loss: 1.690974\n",
      "35 epoch Loss: 1.673567\n",
      "36 epoch Loss: 1.677010\n",
      "37 epoch Loss: 1.660640\n",
      "38 epoch Loss: 1.651714\n",
      "39 epoch Loss: 1.648367\n",
      "40 epoch Loss: 1.641331\n",
      "41 epoch Loss: 1.584218\n",
      "42 epoch Loss: 1.630349\n",
      "43 epoch Loss: 1.594313\n",
      "44 epoch Loss: 1.607338\n",
      "45 epoch Loss: 1.593088\n",
      "46 epoch Loss: 1.523476\n",
      "47 epoch Loss: 1.561876\n",
      "48 epoch Loss: 1.528578\n",
      "49 epoch Loss: 1.550260\n",
      "50 epoch Loss: 1.563643\n",
      "51 epoch Loss: 1.512270\n",
      "52 epoch Loss: 1.550702\n",
      "53 epoch Loss: 1.498453\n",
      "54 epoch Loss: 1.486085\n",
      "55 epoch Loss: 1.500653\n",
      "56 epoch Loss: 1.462340\n",
      "57 epoch Loss: 1.516125\n",
      "58 epoch Loss: 1.473342\n",
      "59 epoch Loss: 1.444732\n",
      "60 epoch Loss: 1.453594\n",
      "61 epoch Loss: 1.416163\n",
      "62 epoch Loss: 1.388468\n",
      "63 epoch Loss: 1.429061\n",
      "64 epoch Loss: 1.410839\n",
      "65 epoch Loss: 1.434515\n",
      "66 epoch Loss: 1.426103\n",
      "67 epoch Loss: 1.389318\n",
      "68 epoch Loss: 1.373065\n",
      "69 epoch Loss: 1.417251\n",
      "70 epoch Loss: 1.372005\n",
      "71 epoch Loss: 1.377317\n",
      "72 epoch Loss: 1.370725\n",
      "73 epoch Loss: 1.380148\n",
      "74 epoch Loss: 1.403803\n",
      "75 epoch Loss: 1.372519\n",
      "76 epoch Loss: 1.371065\n",
      "77 epoch Loss: 1.348793\n",
      "78 epoch Loss: 1.350774\n",
      "79 epoch Loss: 1.344548\n",
      "80 epoch Loss: 1.337037\n",
      "81 epoch Loss: 1.320874\n",
      "82 epoch Loss: 1.327840\n",
      "83 epoch Loss: 1.323386\n",
      "84 epoch Loss: 1.317000\n",
      "85 epoch Loss: 1.314023\n",
      "86 epoch Loss: 1.324556\n",
      "87 epoch Loss: 1.305987\n",
      "88 epoch Loss: 1.327695\n",
      "89 epoch Loss: 1.297796\n",
      "90 epoch Loss: 1.288869\n",
      "91 epoch Loss: 1.278886\n",
      "92 epoch Loss: 1.309683\n",
      "93 epoch Loss: 1.294829\n",
      "94 epoch Loss: 1.292819\n",
      "95 epoch Loss: 1.306885\n",
      "96 epoch Loss: 1.257853\n",
      "97 epoch Loss: 1.292961\n",
      "98 epoch Loss: 1.273905\n",
      "99 epoch Loss: 1.271227\n",
      "100 epoch Loss: 1.278993\n",
      "101 epoch Loss: 1.261179\n",
      "102 epoch Loss: 1.259335\n",
      "103 epoch Loss: 1.255154\n",
      "104 epoch Loss: 1.236502\n",
      "105 epoch Loss: 1.229770\n",
      "106 epoch Loss: 1.262920\n",
      "107 epoch Loss: 1.249946\n",
      "108 epoch Loss: 1.237613\n",
      "109 epoch Loss: 1.255762\n",
      "110 epoch Loss: 1.227339\n",
      "111 epoch Loss: 1.206905\n",
      "112 epoch Loss: 1.228943\n",
      "113 epoch Loss: 1.248291\n",
      "114 epoch Loss: 1.227504\n",
      "115 epoch Loss: 1.215905\n",
      "116 epoch Loss: 1.196622\n",
      "117 epoch Loss: 1.214010\n",
      "118 epoch Loss: 1.203275\n",
      "119 epoch Loss: 1.230980\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 76.857143, Random forests accuracy: 72.714286, Ensemble accuracy: 79.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.199661\n",
      "1 epoch Loss: 2.812927\n",
      "2 epoch Loss: 2.594364\n",
      "3 epoch Loss: 2.498265\n",
      "4 epoch Loss: 2.404501\n",
      "5 epoch Loss: 2.381986\n",
      "6 epoch Loss: 2.337869\n",
      "7 epoch Loss: 2.264462\n",
      "8 epoch Loss: 2.228925\n",
      "9 epoch Loss: 2.204296\n",
      "10 epoch Loss: 2.171370\n",
      "11 epoch Loss: 2.152447\n",
      "12 epoch Loss: 2.127780\n",
      "13 epoch Loss: 2.121916\n",
      "14 epoch Loss: 2.048325\n",
      "15 epoch Loss: 2.049461\n",
      "16 epoch Loss: 1.969102\n",
      "17 epoch Loss: 1.959896\n",
      "18 epoch Loss: 1.889600\n",
      "19 epoch Loss: 1.897616\n",
      "20 epoch Loss: 1.870753\n",
      "21 epoch Loss: 1.889834\n",
      "22 epoch Loss: 1.844515\n",
      "23 epoch Loss: 1.846458\n",
      "24 epoch Loss: 1.828335\n",
      "25 epoch Loss: 1.802605\n",
      "26 epoch Loss: 1.788646\n",
      "27 epoch Loss: 1.785603\n",
      "28 epoch Loss: 1.750637\n",
      "29 epoch Loss: 1.737322\n",
      "30 epoch Loss: 1.713767\n",
      "31 epoch Loss: 1.711741\n",
      "32 epoch Loss: 1.704698\n",
      "33 epoch Loss: 1.685854\n",
      "34 epoch Loss: 1.651386\n",
      "35 epoch Loss: 1.671551\n",
      "36 epoch Loss: 1.684320\n",
      "37 epoch Loss: 1.637221\n",
      "38 epoch Loss: 1.652193\n",
      "39 epoch Loss: 1.629136\n",
      "40 epoch Loss: 1.610918\n",
      "41 epoch Loss: 1.615250\n",
      "42 epoch Loss: 1.586201\n",
      "43 epoch Loss: 1.560545\n",
      "44 epoch Loss: 1.592111\n",
      "45 epoch Loss: 1.568611\n",
      "46 epoch Loss: 1.534258\n",
      "47 epoch Loss: 1.512792\n",
      "48 epoch Loss: 1.550619\n",
      "49 epoch Loss: 1.532251\n",
      "50 epoch Loss: 1.477016\n",
      "51 epoch Loss: 1.522014\n",
      "52 epoch Loss: 1.502959\n",
      "53 epoch Loss: 1.453784\n",
      "54 epoch Loss: 1.475606\n",
      "55 epoch Loss: 1.453351\n",
      "56 epoch Loss: 1.463448\n",
      "57 epoch Loss: 1.458649\n",
      "58 epoch Loss: 1.462763\n",
      "59 epoch Loss: 1.462826\n",
      "60 epoch Loss: 1.429943\n",
      "61 epoch Loss: 1.426236\n",
      "62 epoch Loss: 1.418349\n",
      "63 epoch Loss: 1.433850\n",
      "64 epoch Loss: 1.426370\n",
      "65 epoch Loss: 1.432609\n",
      "66 epoch Loss: 1.422204\n",
      "67 epoch Loss: 1.411290\n",
      "68 epoch Loss: 1.398249\n",
      "69 epoch Loss: 1.401924\n",
      "70 epoch Loss: 1.392311\n",
      "71 epoch Loss: 1.393030\n",
      "72 epoch Loss: 1.386834\n",
      "73 epoch Loss: 1.351588\n",
      "74 epoch Loss: 1.358664\n",
      "75 epoch Loss: 1.364305\n",
      "76 epoch Loss: 1.384632\n",
      "77 epoch Loss: 1.357279\n",
      "78 epoch Loss: 1.337778\n",
      "79 epoch Loss: 1.306751\n",
      "80 epoch Loss: 1.308519\n",
      "81 epoch Loss: 1.309398\n",
      "82 epoch Loss: 1.323817\n",
      "83 epoch Loss: 1.337453\n",
      "84 epoch Loss: 1.332034\n",
      "85 epoch Loss: 1.310046\n",
      "86 epoch Loss: 1.277316\n",
      "87 epoch Loss: 1.304755\n",
      "88 epoch Loss: 1.318733\n",
      "89 epoch Loss: 1.307559\n",
      "90 epoch Loss: 1.283739\n",
      "91 epoch Loss: 1.263391\n",
      "92 epoch Loss: 1.273881\n",
      "93 epoch Loss: 1.296683\n",
      "94 epoch Loss: 1.287938\n",
      "95 epoch Loss: 1.290669\n",
      "96 epoch Loss: 1.265052\n",
      "97 epoch Loss: 1.283272\n",
      "98 epoch Loss: 1.305747\n",
      "99 epoch Loss: 1.273410\n",
      "100 epoch Loss: 1.232825\n",
      "101 epoch Loss: 1.236004\n",
      "102 epoch Loss: 1.250327\n",
      "103 epoch Loss: 1.256791\n",
      "104 epoch Loss: 1.254004\n",
      "105 epoch Loss: 1.240717\n",
      "106 epoch Loss: 1.268887\n",
      "107 epoch Loss: 1.239716\n",
      "108 epoch Loss: 1.262310\n",
      "109 epoch Loss: 1.227072\n",
      "110 epoch Loss: 1.220531\n",
      "111 epoch Loss: 1.242810\n",
      "112 epoch Loss: 1.218736\n",
      "113 epoch Loss: 1.232778\n",
      "114 epoch Loss: 1.243275\n",
      "115 epoch Loss: 1.244028\n",
      "116 epoch Loss: 1.179129\n",
      "117 epoch Loss: 1.213655\n",
      "118 epoch Loss: 1.207644\n",
      "119 epoch Loss: 1.215230\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 74.714286, Random forests accuracy: 73.714286, Ensemble accuracy: 77.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.206040\n",
      "1 epoch Loss: 2.826849\n",
      "2 epoch Loss: 2.599271\n",
      "3 epoch Loss: 2.484588\n",
      "4 epoch Loss: 2.416508\n",
      "5 epoch Loss: 2.392344\n",
      "6 epoch Loss: 2.320220\n",
      "7 epoch Loss: 2.293369\n",
      "8 epoch Loss: 2.210267\n",
      "9 epoch Loss: 2.218441\n",
      "10 epoch Loss: 2.197443\n",
      "11 epoch Loss: 2.133579\n",
      "12 epoch Loss: 2.147088\n",
      "13 epoch Loss: 2.078396\n",
      "14 epoch Loss: 2.078985\n",
      "15 epoch Loss: 2.027681\n",
      "16 epoch Loss: 1.980030\n",
      "17 epoch Loss: 1.984230\n",
      "18 epoch Loss: 1.929672\n",
      "19 epoch Loss: 1.894449\n",
      "20 epoch Loss: 1.893591\n",
      "21 epoch Loss: 1.874703\n",
      "22 epoch Loss: 1.856550\n",
      "23 epoch Loss: 1.824492\n",
      "24 epoch Loss: 1.827424\n",
      "25 epoch Loss: 1.817341\n",
      "26 epoch Loss: 1.766875\n",
      "27 epoch Loss: 1.816443\n",
      "28 epoch Loss: 1.762722\n",
      "29 epoch Loss: 1.732295\n",
      "30 epoch Loss: 1.727158\n",
      "31 epoch Loss: 1.740123\n",
      "32 epoch Loss: 1.751114\n",
      "33 epoch Loss: 1.716699\n",
      "34 epoch Loss: 1.726267\n",
      "35 epoch Loss: 1.697671\n",
      "36 epoch Loss: 1.695365\n",
      "37 epoch Loss: 1.675050\n",
      "38 epoch Loss: 1.657182\n",
      "39 epoch Loss: 1.642475\n",
      "40 epoch Loss: 1.617544\n",
      "41 epoch Loss: 1.614072\n",
      "42 epoch Loss: 1.587307\n",
      "43 epoch Loss: 1.594964\n",
      "44 epoch Loss: 1.613505\n",
      "45 epoch Loss: 1.574521\n",
      "46 epoch Loss: 1.541328\n",
      "47 epoch Loss: 1.590503\n",
      "48 epoch Loss: 1.562759\n",
      "49 epoch Loss: 1.530437\n",
      "50 epoch Loss: 1.553289\n",
      "51 epoch Loss: 1.509537\n",
      "52 epoch Loss: 1.522826\n",
      "53 epoch Loss: 1.486279\n",
      "54 epoch Loss: 1.512694\n",
      "55 epoch Loss: 1.512805\n",
      "56 epoch Loss: 1.479358\n",
      "57 epoch Loss: 1.494284\n",
      "58 epoch Loss: 1.482104\n",
      "59 epoch Loss: 1.459429\n",
      "60 epoch Loss: 1.424402\n",
      "61 epoch Loss: 1.442554\n",
      "62 epoch Loss: 1.473859\n",
      "63 epoch Loss: 1.421778\n",
      "64 epoch Loss: 1.433228\n",
      "65 epoch Loss: 1.452981\n",
      "66 epoch Loss: 1.425251\n",
      "67 epoch Loss: 1.418456\n",
      "68 epoch Loss: 1.412985\n",
      "69 epoch Loss: 1.399969\n",
      "70 epoch Loss: 1.385147\n",
      "71 epoch Loss: 1.407055\n",
      "72 epoch Loss: 1.372687\n",
      "73 epoch Loss: 1.355533\n",
      "74 epoch Loss: 1.343741\n",
      "75 epoch Loss: 1.364243\n",
      "76 epoch Loss: 1.381592\n",
      "77 epoch Loss: 1.354286\n",
      "78 epoch Loss: 1.377921\n",
      "79 epoch Loss: 1.360591\n",
      "80 epoch Loss: 1.344383\n",
      "81 epoch Loss: 1.307219\n",
      "82 epoch Loss: 1.306166\n",
      "83 epoch Loss: 1.354950\n",
      "84 epoch Loss: 1.320168\n",
      "85 epoch Loss: 1.297356\n",
      "86 epoch Loss: 1.305675\n",
      "87 epoch Loss: 1.342308\n",
      "88 epoch Loss: 1.334501\n",
      "89 epoch Loss: 1.304878\n",
      "90 epoch Loss: 1.288872\n",
      "91 epoch Loss: 1.282194\n",
      "92 epoch Loss: 1.319113\n",
      "93 epoch Loss: 1.273562\n",
      "94 epoch Loss: 1.318223\n",
      "95 epoch Loss: 1.300812\n",
      "96 epoch Loss: 1.256178\n",
      "97 epoch Loss: 1.283204\n",
      "98 epoch Loss: 1.322690\n",
      "99 epoch Loss: 1.277511\n",
      "100 epoch Loss: 1.287216\n",
      "101 epoch Loss: 1.256462\n",
      "102 epoch Loss: 1.253240\n",
      "103 epoch Loss: 1.268750\n",
      "104 epoch Loss: 1.252687\n",
      "105 epoch Loss: 1.249449\n",
      "106 epoch Loss: 1.278316\n",
      "107 epoch Loss: 1.279847\n",
      "108 epoch Loss: 1.240680\n",
      "109 epoch Loss: 1.262523\n",
      "110 epoch Loss: 1.226351\n",
      "111 epoch Loss: 1.265044\n",
      "112 epoch Loss: 1.272453\n",
      "113 epoch Loss: 1.234775\n",
      "114 epoch Loss: 1.248206\n",
      "115 epoch Loss: 1.204682\n",
      "116 epoch Loss: 1.215714\n",
      "117 epoch Loss: 1.197883\n",
      "118 epoch Loss: 1.244589\n",
      "119 epoch Loss: 1.219963\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 75.714286, Random forests accuracy: 73.142857, Ensemble accuracy: 79.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.217391\n",
      "1 epoch Loss: 2.809593\n",
      "2 epoch Loss: 2.607254\n",
      "3 epoch Loss: 2.474307\n",
      "4 epoch Loss: 2.444454\n",
      "5 epoch Loss: 2.354017\n",
      "6 epoch Loss: 2.310490\n",
      "7 epoch Loss: 2.267724\n",
      "8 epoch Loss: 2.224157\n",
      "9 epoch Loss: 2.193868\n",
      "10 epoch Loss: 2.178081\n",
      "11 epoch Loss: 2.155279\n",
      "12 epoch Loss: 2.082515\n",
      "13 epoch Loss: 2.087367\n",
      "14 epoch Loss: 2.071584\n",
      "15 epoch Loss: 2.029131\n",
      "16 epoch Loss: 2.010965\n",
      "17 epoch Loss: 1.998246\n",
      "18 epoch Loss: 1.946613\n",
      "19 epoch Loss: 1.926223\n",
      "20 epoch Loss: 1.890894\n",
      "21 epoch Loss: 1.864515\n",
      "22 epoch Loss: 1.830803\n",
      "23 epoch Loss: 1.825097\n",
      "24 epoch Loss: 1.844506\n",
      "25 epoch Loss: 1.821023\n",
      "26 epoch Loss: 1.772641\n",
      "27 epoch Loss: 1.771922\n",
      "28 epoch Loss: 1.782328\n",
      "29 epoch Loss: 1.758554\n",
      "30 epoch Loss: 1.746485\n",
      "31 epoch Loss: 1.708348\n",
      "32 epoch Loss: 1.703214\n",
      "33 epoch Loss: 1.689628\n",
      "34 epoch Loss: 1.674317\n",
      "35 epoch Loss: 1.694486\n",
      "36 epoch Loss: 1.656196\n",
      "37 epoch Loss: 1.617631\n",
      "38 epoch Loss: 1.665941\n",
      "39 epoch Loss: 1.594801\n",
      "40 epoch Loss: 1.611906\n",
      "41 epoch Loss: 1.627139\n",
      "42 epoch Loss: 1.591617\n",
      "43 epoch Loss: 1.619166\n",
      "44 epoch Loss: 1.561977\n",
      "45 epoch Loss: 1.557078\n",
      "46 epoch Loss: 1.539474\n",
      "47 epoch Loss: 1.552916\n",
      "48 epoch Loss: 1.558535\n",
      "49 epoch Loss: 1.543116\n",
      "50 epoch Loss: 1.503403\n",
      "51 epoch Loss: 1.509987\n",
      "52 epoch Loss: 1.515248\n",
      "53 epoch Loss: 1.495644\n",
      "54 epoch Loss: 1.478989\n",
      "55 epoch Loss: 1.472618\n",
      "56 epoch Loss: 1.468596\n",
      "57 epoch Loss: 1.477587\n",
      "58 epoch Loss: 1.460755\n",
      "59 epoch Loss: 1.443929\n",
      "60 epoch Loss: 1.433377\n",
      "61 epoch Loss: 1.446188\n",
      "62 epoch Loss: 1.429218\n",
      "63 epoch Loss: 1.416360\n",
      "64 epoch Loss: 1.418465\n",
      "65 epoch Loss: 1.392520\n",
      "66 epoch Loss: 1.402453\n",
      "67 epoch Loss: 1.378414\n",
      "68 epoch Loss: 1.355764\n",
      "69 epoch Loss: 1.379739\n",
      "70 epoch Loss: 1.383370\n",
      "71 epoch Loss: 1.403803\n",
      "72 epoch Loss: 1.381629\n",
      "73 epoch Loss: 1.377097\n",
      "74 epoch Loss: 1.367450\n",
      "75 epoch Loss: 1.361725\n",
      "76 epoch Loss: 1.390486\n",
      "77 epoch Loss: 1.350272\n",
      "78 epoch Loss: 1.325581\n",
      "79 epoch Loss: 1.303947\n",
      "80 epoch Loss: 1.352245\n",
      "81 epoch Loss: 1.334143\n",
      "82 epoch Loss: 1.327527\n",
      "83 epoch Loss: 1.341049\n",
      "84 epoch Loss: 1.330981\n",
      "85 epoch Loss: 1.287662\n",
      "86 epoch Loss: 1.291606\n",
      "87 epoch Loss: 1.306053\n",
      "88 epoch Loss: 1.300683\n",
      "89 epoch Loss: 1.304987\n",
      "90 epoch Loss: 1.274714\n",
      "91 epoch Loss: 1.290960\n",
      "92 epoch Loss: 1.295365\n",
      "93 epoch Loss: 1.257557\n",
      "94 epoch Loss: 1.284648\n",
      "95 epoch Loss: 1.273003\n",
      "96 epoch Loss: 1.273460\n",
      "97 epoch Loss: 1.271248\n",
      "98 epoch Loss: 1.259391\n",
      "99 epoch Loss: 1.257066\n",
      "100 epoch Loss: 1.255396\n",
      "101 epoch Loss: 1.287380\n",
      "102 epoch Loss: 1.253940\n",
      "103 epoch Loss: 1.281126\n",
      "104 epoch Loss: 1.226908\n",
      "105 epoch Loss: 1.245715\n",
      "106 epoch Loss: 1.255534\n",
      "107 epoch Loss: 1.262003\n",
      "108 epoch Loss: 1.260999\n",
      "109 epoch Loss: 1.220484\n",
      "110 epoch Loss: 1.238097\n",
      "111 epoch Loss: 1.216632\n",
      "112 epoch Loss: 1.245193\n",
      "113 epoch Loss: 1.239684\n",
      "114 epoch Loss: 1.200672\n",
      "115 epoch Loss: 1.194715\n",
      "116 epoch Loss: 1.245045\n",
      "117 epoch Loss: 1.217618\n",
      "118 epoch Loss: 1.237682\n",
      "119 epoch Loss: 1.203220\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 73.857143, Random forests accuracy: 71.142857, Ensemble accuracy: 77.285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.187447\n",
      "1 epoch Loss: 2.821174\n",
      "2 epoch Loss: 2.617505\n",
      "3 epoch Loss: 2.484697\n",
      "4 epoch Loss: 2.424732\n",
      "5 epoch Loss: 2.387431\n",
      "6 epoch Loss: 2.325341\n",
      "7 epoch Loss: 2.300384\n",
      "8 epoch Loss: 2.218200\n",
      "9 epoch Loss: 2.176315\n",
      "10 epoch Loss: 2.139232\n",
      "11 epoch Loss: 2.149183\n",
      "12 epoch Loss: 2.120189\n",
      "13 epoch Loss: 2.074305\n",
      "14 epoch Loss: 2.065384\n",
      "15 epoch Loss: 1.994078\n",
      "16 epoch Loss: 2.005336\n",
      "17 epoch Loss: 1.951034\n",
      "18 epoch Loss: 1.938938\n",
      "19 epoch Loss: 1.902527\n",
      "20 epoch Loss: 1.869162\n",
      "21 epoch Loss: 1.867593\n",
      "22 epoch Loss: 1.863141\n",
      "23 epoch Loss: 1.846789\n",
      "24 epoch Loss: 1.834354\n",
      "25 epoch Loss: 1.822183\n",
      "26 epoch Loss: 1.796640\n",
      "27 epoch Loss: 1.757996\n",
      "28 epoch Loss: 1.754364\n",
      "29 epoch Loss: 1.742444\n",
      "30 epoch Loss: 1.715009\n",
      "31 epoch Loss: 1.720840\n",
      "32 epoch Loss: 1.724593\n",
      "33 epoch Loss: 1.700641\n",
      "34 epoch Loss: 1.685992\n",
      "35 epoch Loss: 1.675217\n",
      "36 epoch Loss: 1.652452\n",
      "37 epoch Loss: 1.621342\n",
      "38 epoch Loss: 1.654678\n",
      "39 epoch Loss: 1.614530\n",
      "40 epoch Loss: 1.619063\n",
      "41 epoch Loss: 1.604864\n",
      "42 epoch Loss: 1.614747\n",
      "43 epoch Loss: 1.577900\n",
      "44 epoch Loss: 1.568717\n",
      "45 epoch Loss: 1.559840\n",
      "46 epoch Loss: 1.551570\n",
      "47 epoch Loss: 1.543698\n",
      "48 epoch Loss: 1.520211\n",
      "49 epoch Loss: 1.529027\n",
      "50 epoch Loss: 1.525733\n",
      "51 epoch Loss: 1.517472\n",
      "52 epoch Loss: 1.484124\n",
      "53 epoch Loss: 1.507192\n",
      "54 epoch Loss: 1.487878\n",
      "55 epoch Loss: 1.465603\n",
      "56 epoch Loss: 1.481418\n",
      "57 epoch Loss: 1.466413\n",
      "58 epoch Loss: 1.413816\n",
      "59 epoch Loss: 1.449555\n",
      "60 epoch Loss: 1.457500\n",
      "61 epoch Loss: 1.408458\n",
      "62 epoch Loss: 1.423038\n",
      "63 epoch Loss: 1.429944\n",
      "64 epoch Loss: 1.426572\n",
      "65 epoch Loss: 1.428637\n",
      "66 epoch Loss: 1.415907\n",
      "67 epoch Loss: 1.441030\n",
      "68 epoch Loss: 1.408655\n",
      "69 epoch Loss: 1.406115\n",
      "70 epoch Loss: 1.378592\n",
      "71 epoch Loss: 1.365907\n",
      "72 epoch Loss: 1.376733\n",
      "73 epoch Loss: 1.379209\n",
      "74 epoch Loss: 1.384501\n",
      "75 epoch Loss: 1.356376\n",
      "76 epoch Loss: 1.355863\n",
      "77 epoch Loss: 1.378829\n",
      "78 epoch Loss: 1.357109\n",
      "79 epoch Loss: 1.332051\n",
      "80 epoch Loss: 1.329514\n",
      "81 epoch Loss: 1.326008\n",
      "82 epoch Loss: 1.309476\n",
      "83 epoch Loss: 1.351093\n",
      "84 epoch Loss: 1.312681\n",
      "85 epoch Loss: 1.299653\n",
      "86 epoch Loss: 1.323958\n",
      "87 epoch Loss: 1.333187\n",
      "88 epoch Loss: 1.294044\n",
      "89 epoch Loss: 1.304338\n",
      "90 epoch Loss: 1.259390\n",
      "91 epoch Loss: 1.286393\n",
      "92 epoch Loss: 1.293080\n",
      "93 epoch Loss: 1.276174\n",
      "94 epoch Loss: 1.268196\n",
      "95 epoch Loss: 1.276340\n",
      "96 epoch Loss: 1.278325\n",
      "97 epoch Loss: 1.288356\n",
      "98 epoch Loss: 1.242579\n",
      "99 epoch Loss: 1.235232\n",
      "100 epoch Loss: 1.275487\n",
      "101 epoch Loss: 1.204157\n",
      "102 epoch Loss: 1.265825\n",
      "103 epoch Loss: 1.259870\n",
      "104 epoch Loss: 1.258279\n",
      "105 epoch Loss: 1.240788\n",
      "106 epoch Loss: 1.239794\n",
      "107 epoch Loss: 1.264776\n",
      "108 epoch Loss: 1.226190\n",
      "109 epoch Loss: 1.224430\n",
      "110 epoch Loss: 1.212926\n",
      "111 epoch Loss: 1.244202\n",
      "112 epoch Loss: 1.234274\n",
      "113 epoch Loss: 1.208672\n",
      "114 epoch Loss: 1.218765\n",
      "115 epoch Loss: 1.206114\n",
      "116 epoch Loss: 1.238487\n",
      "117 epoch Loss: 1.213262\n",
      "118 epoch Loss: 1.205002\n",
      "119 epoch Loss: 1.201357\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 73.714286, Random forests accuracy: 72.428571, Ensemble accuracy: 78.142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.196317\n",
      "1 epoch Loss: 2.818705\n",
      "2 epoch Loss: 2.647463\n",
      "3 epoch Loss: 2.475217\n",
      "4 epoch Loss: 2.400411\n",
      "5 epoch Loss: 2.361183\n",
      "6 epoch Loss: 2.347730\n",
      "7 epoch Loss: 2.264718\n",
      "8 epoch Loss: 2.212695\n",
      "9 epoch Loss: 2.174021\n",
      "10 epoch Loss: 2.166238\n",
      "11 epoch Loss: 2.158687\n",
      "12 epoch Loss: 2.135605\n",
      "13 epoch Loss: 2.080966\n",
      "14 epoch Loss: 2.085374\n",
      "15 epoch Loss: 2.032384\n",
      "16 epoch Loss: 1.984689\n",
      "17 epoch Loss: 1.941314\n",
      "18 epoch Loss: 1.944440\n",
      "19 epoch Loss: 1.898805\n",
      "20 epoch Loss: 1.909378\n",
      "21 epoch Loss: 1.894811\n",
      "22 epoch Loss: 1.859695\n",
      "23 epoch Loss: 1.830361\n",
      "24 epoch Loss: 1.794172\n",
      "25 epoch Loss: 1.804878\n",
      "26 epoch Loss: 1.786376\n",
      "27 epoch Loss: 1.789759\n",
      "28 epoch Loss: 1.762608\n",
      "29 epoch Loss: 1.728645\n",
      "30 epoch Loss: 1.737432\n",
      "31 epoch Loss: 1.722673\n",
      "32 epoch Loss: 1.710806\n",
      "33 epoch Loss: 1.680092\n",
      "34 epoch Loss: 1.687357\n",
      "35 epoch Loss: 1.680339\n",
      "36 epoch Loss: 1.656251\n",
      "37 epoch Loss: 1.651981\n",
      "38 epoch Loss: 1.652209\n",
      "39 epoch Loss: 1.627043\n",
      "40 epoch Loss: 1.604685\n",
      "41 epoch Loss: 1.594044\n",
      "42 epoch Loss: 1.598148\n",
      "43 epoch Loss: 1.579640\n",
      "44 epoch Loss: 1.585219\n",
      "45 epoch Loss: 1.585437\n",
      "46 epoch Loss: 1.595174\n",
      "47 epoch Loss: 1.520270\n",
      "48 epoch Loss: 1.546990\n",
      "49 epoch Loss: 1.533106\n",
      "50 epoch Loss: 1.530927\n",
      "51 epoch Loss: 1.490492\n",
      "52 epoch Loss: 1.488869\n",
      "53 epoch Loss: 1.532491\n",
      "54 epoch Loss: 1.499886\n",
      "55 epoch Loss: 1.468231\n",
      "56 epoch Loss: 1.512003\n",
      "57 epoch Loss: 1.474691\n",
      "58 epoch Loss: 1.469526\n",
      "59 epoch Loss: 1.469951\n",
      "60 epoch Loss: 1.457977\n",
      "61 epoch Loss: 1.457386\n",
      "62 epoch Loss: 1.451755\n",
      "63 epoch Loss: 1.425942\n",
      "64 epoch Loss: 1.425919\n",
      "65 epoch Loss: 1.392528\n",
      "66 epoch Loss: 1.391283\n",
      "67 epoch Loss: 1.424661\n",
      "68 epoch Loss: 1.423172\n",
      "69 epoch Loss: 1.385212\n",
      "70 epoch Loss: 1.393445\n",
      "71 epoch Loss: 1.384853\n",
      "72 epoch Loss: 1.377761\n",
      "73 epoch Loss: 1.369067\n",
      "74 epoch Loss: 1.375167\n",
      "75 epoch Loss: 1.356377\n",
      "76 epoch Loss: 1.357514\n",
      "77 epoch Loss: 1.348660\n",
      "78 epoch Loss: 1.343271\n",
      "79 epoch Loss: 1.361438\n",
      "80 epoch Loss: 1.325599\n",
      "81 epoch Loss: 1.334511\n",
      "82 epoch Loss: 1.347684\n",
      "83 epoch Loss: 1.337382\n",
      "84 epoch Loss: 1.326705\n",
      "85 epoch Loss: 1.315185\n",
      "86 epoch Loss: 1.299177\n",
      "87 epoch Loss: 1.292880\n",
      "88 epoch Loss: 1.313936\n",
      "89 epoch Loss: 1.306161\n",
      "90 epoch Loss: 1.323461\n",
      "91 epoch Loss: 1.321491\n",
      "92 epoch Loss: 1.281879\n",
      "93 epoch Loss: 1.307373\n",
      "94 epoch Loss: 1.294943\n",
      "95 epoch Loss: 1.263292\n",
      "96 epoch Loss: 1.289737\n",
      "97 epoch Loss: 1.281698\n",
      "98 epoch Loss: 1.290443\n",
      "99 epoch Loss: 1.270850\n",
      "100 epoch Loss: 1.276797\n",
      "101 epoch Loss: 1.279555\n",
      "102 epoch Loss: 1.249361\n",
      "103 epoch Loss: 1.237422\n",
      "104 epoch Loss: 1.258819\n",
      "105 epoch Loss: 1.230955\n",
      "106 epoch Loss: 1.252557\n",
      "107 epoch Loss: 1.268667\n",
      "108 epoch Loss: 1.262548\n",
      "109 epoch Loss: 1.229548\n",
      "110 epoch Loss: 1.249838\n",
      "111 epoch Loss: 1.239787\n",
      "112 epoch Loss: 1.230448\n",
      "113 epoch Loss: 1.254932\n",
      "114 epoch Loss: 1.222426\n",
      "115 epoch Loss: 1.224672\n",
      "116 epoch Loss: 1.201557\n",
      "117 epoch Loss: 1.223977\n",
      "118 epoch Loss: 1.209532\n",
      "119 epoch Loss: 1.252424\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 77.142857, Random forests accuracy: 74.714286, Ensemble accuracy: 80.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.201736\n",
      "1 epoch Loss: 2.830922\n",
      "2 epoch Loss: 2.610167\n",
      "3 epoch Loss: 2.512798\n",
      "4 epoch Loss: 2.433470\n",
      "5 epoch Loss: 2.397940\n",
      "6 epoch Loss: 2.345793\n",
      "7 epoch Loss: 2.282119\n",
      "8 epoch Loss: 2.235109\n",
      "9 epoch Loss: 2.200836\n",
      "10 epoch Loss: 2.139556\n",
      "11 epoch Loss: 2.149504\n",
      "12 epoch Loss: 2.113309\n",
      "13 epoch Loss: 2.071706\n",
      "14 epoch Loss: 2.072552\n",
      "15 epoch Loss: 1.996889\n",
      "16 epoch Loss: 1.964300\n",
      "17 epoch Loss: 1.957512\n",
      "18 epoch Loss: 1.950167\n",
      "19 epoch Loss: 1.909411\n",
      "20 epoch Loss: 1.894456\n",
      "21 epoch Loss: 1.926723\n",
      "22 epoch Loss: 1.898957\n",
      "23 epoch Loss: 1.891142\n",
      "24 epoch Loss: 1.814457\n",
      "25 epoch Loss: 1.793573\n",
      "26 epoch Loss: 1.807657\n",
      "27 epoch Loss: 1.778636\n",
      "28 epoch Loss: 1.781728\n",
      "29 epoch Loss: 1.754325\n",
      "30 epoch Loss: 1.747529\n",
      "31 epoch Loss: 1.727789\n",
      "32 epoch Loss: 1.714382\n",
      "33 epoch Loss: 1.706832\n",
      "34 epoch Loss: 1.691270\n",
      "35 epoch Loss: 1.707849\n",
      "36 epoch Loss: 1.661665\n",
      "37 epoch Loss: 1.666241\n",
      "38 epoch Loss: 1.621258\n",
      "39 epoch Loss: 1.637038\n",
      "40 epoch Loss: 1.629290\n",
      "41 epoch Loss: 1.613772\n",
      "42 epoch Loss: 1.602753\n",
      "43 epoch Loss: 1.598874\n",
      "44 epoch Loss: 1.575351\n",
      "45 epoch Loss: 1.574623\n",
      "46 epoch Loss: 1.546938\n",
      "47 epoch Loss: 1.533483\n",
      "48 epoch Loss: 1.537728\n",
      "49 epoch Loss: 1.532001\n",
      "50 epoch Loss: 1.557472\n",
      "51 epoch Loss: 1.519501\n",
      "52 epoch Loss: 1.500598\n",
      "53 epoch Loss: 1.503838\n",
      "54 epoch Loss: 1.476233\n",
      "55 epoch Loss: 1.472338\n",
      "56 epoch Loss: 1.458285\n",
      "57 epoch Loss: 1.469317\n",
      "58 epoch Loss: 1.445155\n",
      "59 epoch Loss: 1.469279\n",
      "60 epoch Loss: 1.456530\n",
      "61 epoch Loss: 1.438220\n",
      "62 epoch Loss: 1.443761\n",
      "63 epoch Loss: 1.418651\n",
      "64 epoch Loss: 1.442275\n",
      "65 epoch Loss: 1.424799\n",
      "66 epoch Loss: 1.432192\n",
      "67 epoch Loss: 1.407961\n",
      "68 epoch Loss: 1.380192\n",
      "69 epoch Loss: 1.397212\n",
      "103 epoch Loss: 1.274276\n",
      "104 epoch Loss: 1.264289\n",
      "105 epoch Loss: 1.232868\n",
      "106 epoch Loss: 1.256557\n",
      "107 epoch Loss: 1.236469\n",
      "108 epoch Loss: 1.231721\n",
      "109 epoch Loss: 1.230416\n",
      "110 epoch Loss: 1.244620\n",
      "111 epoch Loss: 1.221117\n",
      "112 epoch Loss: 1.258260\n",
      "113 epoch Loss: 1.261356\n",
      "114 epoch Loss: 1.228013\n",
      "115 epoch Loss: 1.250513\n",
      "116 epoch Loss: 1.196870\n",
      "117 epoch Loss: 1.244787\n",
      "118 epoch Loss: 1.215015\n",
      "119 epoch Loss: 1.226221\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 76.285714, Random forests accuracy: 74.285714, Ensemble accuracy: 78.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13931.0625, tolerance: 44.45615768432617\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.199308\n",
      "1 epoch Loss: 2.822052\n",
      "2 epoch Loss: 2.609075\n",
      "3 epoch Loss: 2.478639\n",
      "4 epoch Loss: 2.430090\n",
      "5 epoch Loss: 2.364105\n",
      "6 epoch Loss: 2.337510\n",
      "7 epoch Loss: 2.314982\n",
      "8 epoch Loss: 2.246808\n",
      "9 epoch Loss: 2.194043\n",
      "10 epoch Loss: 2.155340\n",
      "11 epoch Loss: 2.127621\n",
      "12 epoch Loss: 2.118168\n",
      "13 epoch Loss: 2.095823\n",
      "14 epoch Loss: 2.033170\n",
      "15 epoch Loss: 1.998360\n",
      "16 epoch Loss: 1.956567\n",
      "17 epoch Loss: 1.978468\n",
      "18 epoch Loss: 1.949946\n",
      "19 epoch Loss: 1.923304\n",
      "20 epoch Loss: 1.889725\n",
      "21 epoch Loss: 1.860633\n",
      "22 epoch Loss: 1.853851\n",
      "23 epoch Loss: 1.797378\n",
      "24 epoch Loss: 1.847677\n",
      "25 epoch Loss: 1.786231\n",
      "26 epoch Loss: 1.793131\n",
      "27 epoch Loss: 1.774310\n",
      "28 epoch Loss: 1.745770\n",
      "29 epoch Loss: 1.747682\n",
      "30 epoch Loss: 1.738256\n",
      "31 epoch Loss: 1.726838\n",
      "32 epoch Loss: 1.726177\n",
      "33 epoch Loss: 1.697878\n",
      "34 epoch Loss: 1.665543\n",
      "35 epoch Loss: 1.678942\n",
      "36 epoch Loss: 1.671585\n",
      "37 epoch Loss: 1.628062\n",
      "38 epoch Loss: 1.640997\n",
      "39 epoch Loss: 1.636960\n",
      "40 epoch Loss: 1.624762\n",
      "41 epoch Loss: 1.620682\n",
      "42 epoch Loss: 1.599496\n",
      "43 epoch Loss: 1.595257\n",
      "44 epoch Loss: 1.582164\n",
      "45 epoch Loss: 1.546645\n",
      "46 epoch Loss: 1.573451\n",
      "47 epoch Loss: 1.577460\n",
      "48 epoch Loss: 1.541826\n",
      "49 epoch Loss: 1.539497\n",
      "50 epoch Loss: 1.510886\n",
      "51 epoch Loss: 1.506605\n",
      "52 epoch Loss: 1.503595\n",
      "53 epoch Loss: 1.505886\n",
      "54 epoch Loss: 1.490337\n",
      "55 epoch Loss: 1.488431\n",
      "56 epoch Loss: 1.487284\n",
      "57 epoch Loss: 1.465512\n",
      "58 epoch Loss: 1.476405\n",
      "59 epoch Loss: 1.472501\n",
      "60 epoch Loss: 1.467597\n",
      "61 epoch Loss: 1.414425\n",
      "62 epoch Loss: 1.441479\n",
      "63 epoch Loss: 1.441696\n",
      "64 epoch Loss: 1.400023\n",
      "65 epoch Loss: 1.444755\n",
      "66 epoch Loss: 1.403813\n",
      "67 epoch Loss: 1.387282\n",
      "68 epoch Loss: 1.366345\n",
      "69 epoch Loss: 1.406958\n",
      "70 epoch Loss: 1.389064\n",
      "71 epoch Loss: 1.365221\n",
      "72 epoch Loss: 1.371634\n",
      "73 epoch Loss: 1.367242\n",
      "74 epoch Loss: 1.346159\n",
      "75 epoch Loss: 1.352989\n",
      "76 epoch Loss: 1.384887\n",
      "77 epoch Loss: 1.338656\n",
      "78 epoch Loss: 1.356891\n",
      "79 epoch Loss: 1.343449\n",
      "80 epoch Loss: 1.345211\n",
      "81 epoch Loss: 1.328250\n",
      "82 epoch Loss: 1.308703\n",
      "83 epoch Loss: 1.332741\n",
      "84 epoch Loss: 1.323498\n",
      "85 epoch Loss: 1.344557\n",
      "86 epoch Loss: 1.320412\n",
      "87 epoch Loss: 1.310223\n",
      "88 epoch Loss: 1.325205\n",
      "89 epoch Loss: 1.297973\n",
      "90 epoch Loss: 1.288373\n",
      "91 epoch Loss: 1.308913\n",
      "92 epoch Loss: 1.292005\n",
      "93 epoch Loss: 1.241910\n",
      "94 epoch Loss: 1.311154\n",
      "95 epoch Loss: 1.282346\n",
      "96 epoch Loss: 1.294686\n",
      "97 epoch Loss: 1.272628\n",
      "98 epoch Loss: 1.265993\n",
      "99 epoch Loss: 1.281462\n",
      "100 epoch Loss: 1.236964\n",
      "101 epoch Loss: 1.231135\n",
      "102 epoch Loss: 1.253861\n",
      "103 epoch Loss: 1.264008\n",
      "104 epoch Loss: 1.251602\n",
      "105 epoch Loss: 1.248038\n",
      "106 epoch Loss: 1.239254\n",
      "107 epoch Loss: 1.247663\n",
      "108 epoch Loss: 1.247435\n",
      "109 epoch Loss: 1.196929\n",
      "110 epoch Loss: 1.268571\n",
      "111 epoch Loss: 1.211604\n",
      "112 epoch Loss: 1.220720\n",
      "113 epoch Loss: 1.216126\n",
      "114 epoch Loss: 1.231380\n",
      "115 epoch Loss: 1.208683\n",
      "116 epoch Loss: 1.229371\n",
      "117 epoch Loss: 1.209421\n",
      "118 epoch Loss: 1.226936\n",
      "119 epoch Loss: 1.194665\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 75.641026, Random forests accuracy: 73.646724, Ensemble accuracy: 79.202279\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#Load data\n",
    "\n",
    "    \n",
    "    #Initialize\n",
    "    label -= 1\n",
    "  \n",
    "\n",
    "\n",
    "    \n",
    "    t_index = np.random.permutation(int(np.shape(data)[0]/Outer_loop)*Outer_loop)\n",
    "    t_index = np.reshape(t_index, [Outer_loop, -1])\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    box = np.array([4000], dtype=np.int32)\n",
    "    flag = 0\n",
    "    for test_index in t_index:\n",
    "        if flag == Outer_loop-1:\n",
    "            test_index = np.array(np.concatenate((test_index, np.array(range(int(np.shape(data)[0]/Outer_loop)*Outer_loop,np.shape(data)[0]))), axis=0), dtype=np.int32)\n",
    "        train_index = np.setdiff1d(np.array(range(0,np.shape(data)[0])), test_index)\n",
    "        train_data = data[train_index]\n",
    "        train_label = label[train_index]\n",
    "        test_data = data[test_index]\n",
    "        test_label = label[test_index]\n",
    "        \n",
    "        kf = np.random.permutation(int(np.shape(train_data)[0]/Inner_loop)*Inner_loop)\n",
    "        kf = kf.reshape([Inner_loop]+[-1])\n",
    "        val_result = np.zeros([np.shape(train_data)[0],48], dtype=np.float32)\n",
    "        \n",
    "        tot_acc = np.zeros([Inner_loop,5], dtype=np.float32)\n",
    "        lasso = Lasso(alpha=0.0003).fit(data, label)#alpha=0.025\n",
    "        ExtraTree = ExtraTreesClassifier(n_estimators=50).fit(data, label)\n",
    "        coef=ExtraTree.feature_importances_ \n",
    "        #coef = lasso.coef_\n",
    "        coefidx = np.argsort(coef)\n",
    "        # for inner_fold in range(0,Inner_loop):\n",
    "        #     val_test_ind = kf[inner_fold]\n",
    "        #     if inner_fold == Inner_loop-1:\n",
    "        #         val_test_ind = np.array(np.concatenate((val_test_ind,np.array(range(int(np.shape(train_data)[0]/Outer_loop)*Outer_loop,np.shape(train_data)[0]),dtype=np.int32)), axis=0),dtype=np.int32)\n",
    "            \n",
    "        #     val_train_ind = np.setdiff1d(np.array(range(0,np.shape(train_data)[0]),dtype=np.int32), val_test_ind)\n",
    "        #     val_train = train_data[val_train_ind]\n",
    "        #     val_test = train_data[val_test_ind]\n",
    "        #     val_train_label = train_label[val_train_ind]\n",
    "        #     val_test_label = train_label[val_test_ind]\n",
    "        #     temp = 0\n",
    "        #     for item in box:\n",
    "        #         idx = coefidx[-item:]\n",
    "        #         vtrain = val_train[:,idx]\n",
    "        #         vtest = val_test[:,idx]\n",
    "        #         nn_acc, result_nn = dnn(vtrain, val_train_label, vtest, val_test_label)\n",
    "        #         rf_acc, result_rf = rfc(vtrain, val_train_label, vtest, val_test_label)\n",
    "        #         en_acc = 0.0\n",
    "        #         for i in range(0,np.shape(vtest)[0]):\n",
    "        #             r = np.argmax(result_nn[i]+result_rf[i])\n",
    "        #             if r == val_test_label[i]:\n",
    "        #                 en_acc += 1\n",
    "        #         en_acc /= np.shape(vtest)[0]*0.01\n",
    "        #         tot_acc[inner_fold,temp] = en_acc\n",
    "        #         print(\"Inner_fold # of features: %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (item, nn_acc, rf_acc, en_acc))\n",
    "        #         temp += 1\n",
    "        \n",
    "        u = np.sum(tot_acc,0)\n",
    "       \n",
    "        best_n = box[np.argmax(u)]\n",
    "        idx = coefidx[-best_n:]\n",
    "        \n",
    "        tr_data = train_data[:,idx]\n",
    "        te_data = test_data[:,idx]\n",
    "        nn_acc, result_nn = dnn(tr_data, train_label, te_data, test_label)\n",
    "        rf_acc, result_rf = rfc(tr_data, train_label, te_data, test_label)\n",
    "        en_acc = 0.0\n",
    "        for i in range(0,np.shape(te_data)[0]):\n",
    "            r = np.argmax(result_nn[i]+result_rf[i])\n",
    "            if r == test_label[i]:\n",
    "                en_acc += 1\n",
    "        en_acc /= np.shape(te_data)[0]*0.01\n",
    "        print(\"Outer_fold # of features:  %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (best_n, nn_acc, rf_acc, en_acc))\n",
    "        flag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_no=[78.857143,77.142857,77.428571,78.142857,77.285714,78.285714,79.142857,78.571429,78.857143,77.207977]\n",
    "# RF_no=[74.428571,73.714286,73.714286,73.000000,75.142857,72.857143,76.142857,71.904762,74.142857,71.082621]\n",
    "# emsemble_no=[80.571429,79.571429,80.571429,79.285714,79.571429,80.428571,80.000000,80.793651,81.142857,79.202279]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_lsvc=[84.142857,84.428571,83.285714,84.000000,85.571429,86.000000,85.000000,86.428571,85.000000,84.615385]\n",
    "# RF_lsvc=[74.857143,74.857143,73.285714,72.285714,73.857143,72.857143,72.714286,75.714286,74.000000,72.507123]\n",
    "# emsemble_lsvc=[85.714286,86.714286,85.571429,85.000000,86.857143,86.857143,86.428571,88.285714,86.428571,85.612536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_lasso=[76.000000,73.142857,73.428571,77.000000,74.857143,71.857143,71.857143,73.857143,74.000000,73.219373]\n",
    "# RF_lasso=[72.428571,72.428571,69.571429,72.285714,71.714286,67.714286,71.428571,71.714286,70.428571,67.948718]\n",
    "# emsemble_lasso=[80.142857,77.428571,76.714286,79.000000,79.285714,76.857143,75.571429,77.285714,78.571429,76.495726]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN_tree=[77.571429,78.285714,76.857143,74.714286,75.714286,73.857143,73.714286,77.142857,76.285714,75.641026]\n",
    "# RF_tree=[75.857143,72.428571,72.714286,73.714286,73.142857,71.142857,72.428571,74.714286,74.285714,73.646724]\n",
    "# emsemble_tree=[81.142857,81.428571,79.571429,77.857143,79.571429,77.285714,78.142857,80.857143,78.857143,79.202279]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_acc=[80.571429,79.571429,80.571429,79.285714,79.571429,80.428571,80.000000,80.793651,81.142857,79.202279,#no\n",
    "       78.857143,77.142857,77.428571,78.142857,77.285714,78.285714,79.142857,78.571429,78.857143,77.207977,\n",
    "       74.428571,73.714286,73.714286,73.000000,75.142857,72.857143,76.142857,71.904762,74.142857,71.082621,\n",
    "       81.142857,81.428571,79.571429,77.857143,79.571429,77.285714,78.142857,80.857143,78.857143,79.202279,#extra tree\n",
    "       77.571429,78.285714,76.857143,74.714286,75.714286,73.857143,73.714286,77.142857,76.285714,75.641026,\n",
    "       75.857143,72.428571,72.714286,73.714286,73.142857,71.142857,72.428571,74.714286,74.285714,73.646724,\n",
    "       80.142857,77.428571,76.714286,79.000000,79.285714,76.857143,75.571429,77.285714,78.571429,76.495726,#lasso\n",
    "       76.000000,73.142857,73.428571,77.000000,74.857143,71.857143,71.857143,73.857143,74.000000,73.219373,\n",
    "       72.428571,72.428571,69.571429,72.285714,71.714286,67.714286,71.428571,71.714286,70.428571,67.948718,\n",
    "       85.714286,86.714286,85.571429,85.000000,86.857143,86.857143,86.428571,88.285714,86.428571,85.612536,#lsvc\n",
    "       84.142857,84.428571,83.285714,84.000000,85.571429,86.000000,85.000000,86.428571,85.000000,84.615385,\n",
    "       74.857143,74.857143,73.285714,72.285714,73.857143,72.857143,72.714286,75.714286,74.000000,72.507123\n",
    "       ]\n",
    "MD_sel=['no select','no select','no select','no select','no select','no select','no select','no select','no select','no select',\n",
    "        'no select','no select','no select','no select','no select','no select','no select','no select','no select','no select',\n",
    "        'no select','no select','no select','no select','no select','no select','no select','no select','no select','no select',\n",
    "        'extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree',\n",
    "        'extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree',\n",
    "        'extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree','extra tree',\n",
    "        'lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso',\n",
    "        'lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso','lasso',\n",
    "        'lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc',\n",
    "        'lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc','lsvc',\n",
    "       ]\n",
    "MD_md=['ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble',\n",
    "       'neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network',\n",
    "       'random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest',\n",
    "       'ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble',\n",
    "       'neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network',\n",
    "       'random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest',\n",
    "       'ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble',\n",
    "       'neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network',\n",
    "       'random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest',\n",
    "       'ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble',\n",
    "       'neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network',\n",
    "       'random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest',\n",
    "       \n",
    "       ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(MD_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy':MD_acc,'Feature Select':MD_sel, 'Model':MD_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Feature Select</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80.571429</td>\n",
       "      <td>no select</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.571429</td>\n",
       "      <td>no select</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.571429</td>\n",
       "      <td>no select</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.285714</td>\n",
       "      <td>no select</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.571429</td>\n",
       "      <td>no select</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>72.857143</td>\n",
       "      <td>lsvc</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>72.714286</td>\n",
       "      <td>lsvc</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>75.714286</td>\n",
       "      <td>lsvc</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>lsvc</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>72.507123</td>\n",
       "      <td>lsvc</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy Feature Select          Model\n",
       "0    80.571429      no select       ensemble\n",
       "1    79.571429      no select       ensemble\n",
       "2    80.571429      no select       ensemble\n",
       "3    79.285714      no select       ensemble\n",
       "4    79.571429      no select       ensemble\n",
       "..         ...            ...            ...\n",
       "115  72.857143           lsvc  random forest\n",
       "116  72.714286           lsvc  random forest\n",
       "117  75.714286           lsvc  random forest\n",
       "118  74.000000           lsvc  random forest\n",
       "119  72.507123           lsvc  random forest\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\",rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x = \"Feature Select\", y = \"Accuracy\", hue=\"Model\", data = df, palette = \"Set3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
