{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import hdf5storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnn(x, input_size, output_size, keep_prob, stddev=0.01, constant=0.0001, dropout=True, end=False):\n",
    "    fc_w = tf.Variable(tf.truncated_normal([input_size,output_size], stddev=stddev,seed=np.random.seed(2018)))\n",
    "    fc_b = tf.Variable(tf.constant(constant,shape=[output_size]), dtype=tf.float32)\n",
    "    fc_h = tf.nn.relu(tf.matmul(x,fc_w)+fc_b) if not end else tf.matmul(x,fc_w)+fc_b\n",
    "    return tf.nn.dropout(fc_h, keep_prob,seed=np.random.seed(2018)) if dropout else fc_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn(x, input_size, output_size, nlayers, nparameters, keep_prob):\n",
    "    if nlayers == 1:\n",
    "        h1 = fnn(x, input_size, output_size, keep_prob, end=True)\n",
    "    elif nlayers == 2:\n",
    "        h1 = fnn(fnn(x, input_size, nparameters, keep_prob, end=False), nparameters, output_size, keep_prob, end=True)\n",
    "    elif nlayers >= 3:\n",
    "        h0 = fnn(x, input_size, nparameters, keep_prob, end=False)\n",
    "        for j in range(0,nlayers-2):\n",
    "            if j == 0:\n",
    "                h1 = fnn(h0, nparameters, nparameters, keep_prob, end=False)\n",
    "            else:\n",
    "                h1 = fnn(h1, nparameters, nparameters, keep_prob, end=False)\n",
    "        h1 = fnn(h1, nparameters, output_size, keep_prob, end=True)\n",
    "    else:\n",
    "        print(\"# of layers can't be smaller than 0\")\n",
    "    return h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfc(train_data, train_label, test_data, test_label):\n",
    "    rf = RandomForestClassifier(n_estimators=150,\n",
    "                                    criterion='gini',\n",
    "                                    max_depth=None,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_weight_fraction_leaf=0.0,\n",
    "                                    max_features=None,\n",
    "                                    max_leaf_nodes=None,\n",
    "                                    bootstrap=True,\n",
    "                                    oob_score=False,\n",
    "                                    n_jobs=10,\n",
    "                                    random_state=123,\n",
    "                                    verbose=0,\n",
    "                                    warm_start=False,\n",
    "                                    class_weight=None)\n",
    "    rf.fit(train_data, train_label.ravel())\n",
    "    result = rf.predict_proba(test_data)\n",
    "    acc = 0.0\n",
    "    for i in range(np.shape(test_data)[0]):\n",
    "        r = np.argmax(result[i])\n",
    "        if r == test_label[i]:\n",
    "            acc += 1\n",
    "    acc /= np.shape(test_data)[0]\n",
    "    acc *= 100\n",
    "    return acc, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn(train_data, train_label, test_data, test_label):\n",
    "    g = tf.Graph()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    batch_size = 10\n",
    "    input_size = np.shape(train_data)[1]\n",
    "    output_size = 31\n",
    "\n",
    "    with g.as_default():\n",
    "        p_x = tf.placeholder(tf.float32, [batch_size, 1, input_size, 1])\n",
    "        p_y = tf.placeholder(tf.float32, [batch_size, output_size])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h10_flat = tf.reshape(p_x, [batch_size,-1])\n",
    "        h1 = fnn(h10_flat, input_size, 2048, keep_prob, end=False)\n",
    "        h2 = fnn(h1, 2048, 2048, keep_prob, end=False)\n",
    "        h3 = fnn(h2, 2048, 31, keep_prob, end=True)\n",
    "        h4 = tf.reshape(h3, [batch_size, 31])\n",
    "        h_c = tf.nn.softmax(h4)\n",
    "        loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=p_y, logits=h4))\n",
    "        optim = tf.train.AdamOptimizer(1e-5)\n",
    "        trainer = optim.minimize(loss)\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    result = np.zeros([np.shape(test_data)[0], 31])\n",
    "    with tf.Session(graph=g, config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(0,120):\n",
    "            loss_tot = 0.0\n",
    "            for i in range(0,int(np.ceil(np.shape(train_data)[0]/batch_size))):\n",
    "                a = np.random.randint(0,np.shape(train_data)[0],size=batch_size)\n",
    "                x = train_data[a].reshape([batch_size, 1, input_size, 1])#[4,1,18181,1]\n",
    "                y = np.zeros([batch_size, output_size])\n",
    "                index = train_label[a]\n",
    "                for u in range(0,batch_size):\n",
    "                    y[u,index[u]] = 1\n",
    "                _ , loss_val = sess.run([trainer, loss], feed_dict={p_x:x, p_y:y, keep_prob:0.6})\n",
    "                loss_tot += loss_val\n",
    "            print(\"%d epoch Loss: %f\" % (e,(loss_tot)/np.shape(train_data)[0]))\n",
    "        temp = 0\n",
    "        for i in range(0,int(np.floor(np.shape(test_data)[0]/batch_size))):\n",
    "            x = test_data[i*batch_size:(i+1)*batch_size].reshape([batch_size, 1, input_size, 1])\n",
    "            out = sess.run(h_c, feed_dict={p_x:x, keep_prob:1})\n",
    "            for j in range(0, batch_size):\n",
    "                t = np.squeeze(out[j])\n",
    "                result[temp] = t\n",
    "                temp+=1\n",
    "        remain = int(np.shape(test_data)[0]-np.floor(np.shape(test_data)[0]/batch_size)*batch_size)\n",
    "        if remain > 0:\n",
    "            x = test_data[-batch_size-1:-1].reshape([batch_size, 1, input_size, 1])\n",
    "            out = sess.run(h_c, feed_dict={p_x:x, keep_prob:1})\n",
    "            for j in range(0,int(remain)):\n",
    "                t = np.squeeze(out[j+(batch_size-remain)])\n",
    "                result[temp] = t\n",
    "                temp+=1\n",
    "        for i in range(0,np.shape(test_data)[0]):\n",
    "            ind = np.argmax(np.squeeze(result[i]))\n",
    "            if ind == test_label[i]:\n",
    "                accuracy += 1\n",
    "        accuracy /= np.shape(test_data)[0]*0.01\n",
    "        sess.close()\n",
    "    return accuracy, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataID = hdf5storage.loadmat('data.mat')\n",
    "data = np.array(dataID['data'], dtype=np.float32)\n",
    "gt1 = scipy.io.loadmat('label.mat')\n",
    "label = np.array(gt1['label'], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outer_loop = 10\n",
    "Inner_loop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.310719\n",
      "1 epoch Loss: 3.057256\n",
      "2 epoch Loss: 2.842020\n",
      "3 epoch Loss: 2.679981\n",
      "4 epoch Loss: 2.518098\n",
      "5 epoch Loss: 2.460461\n",
      "6 epoch Loss: 2.322171\n",
      "7 epoch Loss: 2.276983\n",
      "8 epoch Loss: 2.190795\n",
      "9 epoch Loss: 2.159150\n",
      "10 epoch Loss: 2.060299\n",
      "11 epoch Loss: 2.058204\n",
      "12 epoch Loss: 1.976052\n",
      "13 epoch Loss: 1.913965\n",
      "14 epoch Loss: 1.945198\n",
      "15 epoch Loss: 1.869561\n",
      "16 epoch Loss: 1.817971\n",
      "17 epoch Loss: 1.792062\n",
      "18 epoch Loss: 1.741372\n",
      "19 epoch Loss: 1.715327\n",
      "20 epoch Loss: 1.644492\n",
      "21 epoch Loss: 1.659347\n",
      "22 epoch Loss: 1.634921\n",
      "23 epoch Loss: 1.588886\n",
      "24 epoch Loss: 1.562715\n",
      "25 epoch Loss: 1.513573\n",
      "26 epoch Loss: 1.511111\n",
      "27 epoch Loss: 1.493179\n",
      "28 epoch Loss: 1.429190\n",
      "29 epoch Loss: 1.448614\n",
      "30 epoch Loss: 1.452646\n",
      "31 epoch Loss: 1.440261\n",
      "32 epoch Loss: 1.381558\n",
      "33 epoch Loss: 1.366764\n",
      "34 epoch Loss: 1.337785\n",
      "35 epoch Loss: 1.370940\n",
      "36 epoch Loss: 1.320124\n",
      "37 epoch Loss: 1.287493\n",
      "38 epoch Loss: 1.285838\n",
      "39 epoch Loss: 1.245378\n",
      "40 epoch Loss: 1.262948\n",
      "41 epoch Loss: 1.283731\n",
      "42 epoch Loss: 1.255452\n",
      "43 epoch Loss: 1.219838\n",
      "44 epoch Loss: 1.233725\n",
      "45 epoch Loss: 1.225268\n",
      "46 epoch Loss: 1.191419\n",
      "47 epoch Loss: 1.187695\n",
      "48 epoch Loss: 1.171478\n",
      "49 epoch Loss: 1.137155\n",
      "50 epoch Loss: 1.170212\n",
      "51 epoch Loss: 1.168314\n",
      "52 epoch Loss: 1.152394\n",
      "53 epoch Loss: 1.166515\n",
      "54 epoch Loss: 1.133203\n",
      "55 epoch Loss: 1.111066\n",
      "56 epoch Loss: 1.156349\n",
      "57 epoch Loss: 1.122876\n",
      "58 epoch Loss: 1.110026\n",
      "59 epoch Loss: 1.131478\n",
      "60 epoch Loss: 1.102238\n",
      "61 epoch Loss: 1.086660\n",
      "62 epoch Loss: 1.088844\n",
      "63 epoch Loss: 1.118443\n",
      "64 epoch Loss: 1.080913\n",
      "65 epoch Loss: 1.082119\n",
      "66 epoch Loss: 1.091523\n",
      "67 epoch Loss: 1.083335\n",
      "68 epoch Loss: 1.088730\n",
      "69 epoch Loss: 1.080838\n",
      "70 epoch Loss: 1.059235\n",
      "71 epoch Loss: 1.075696\n",
      "72 epoch Loss: 1.076714\n",
      "73 epoch Loss: 1.080571\n",
      "74 epoch Loss: 1.063852\n",
      "75 epoch Loss: 1.032276\n",
      "76 epoch Loss: 1.071250\n",
      "77 epoch Loss: 1.043454\n",
      "78 epoch Loss: 1.040813\n",
      "79 epoch Loss: 1.032642\n",
      "80 epoch Loss: 1.036819\n",
      "81 epoch Loss: 1.033579\n",
      "82 epoch Loss: 1.075852\n",
      "83 epoch Loss: 1.041878\n",
      "84 epoch Loss: 1.020973\n",
      "85 epoch Loss: 1.061661\n",
      "86 epoch Loss: 1.029901\n",
      "87 epoch Loss: 1.071656\n",
      "88 epoch Loss: 1.047689\n",
      "89 epoch Loss: 1.047285\n",
      "90 epoch Loss: 1.048309\n",
      "91 epoch Loss: 1.045349\n",
      "92 epoch Loss: 1.024190\n",
      "93 epoch Loss: 1.014808\n",
      "94 epoch Loss: 1.046853\n",
      "95 epoch Loss: 1.033730\n",
      "96 epoch Loss: 1.019850\n",
      "97 epoch Loss: 1.010313\n",
      "98 epoch Loss: 1.026790\n",
      "99 epoch Loss: 1.033650\n",
      "100 epoch Loss: 1.022983\n",
      "101 epoch Loss: 1.020104\n",
      "102 epoch Loss: 1.019692\n",
      "103 epoch Loss: 1.012219\n",
      "104 epoch Loss: 1.006782\n",
      "105 epoch Loss: 1.021862\n",
      "106 epoch Loss: 1.022689\n",
      "107 epoch Loss: 1.051842\n",
      "108 epoch Loss: 1.034145\n",
      "109 epoch Loss: 1.037185\n",
      "110 epoch Loss: 1.011716\n",
      "111 epoch Loss: 1.008016\n",
      "112 epoch Loss: 1.015478\n",
      "113 epoch Loss: 1.026249\n",
      "114 epoch Loss: 1.043693\n",
      "115 epoch Loss: 1.030272\n",
      "116 epoch Loss: 1.048500\n",
      "117 epoch Loss: 1.017187\n",
      "118 epoch Loss: 1.041455\n",
      "119 epoch Loss: 1.006736\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.142857, Random forests accuracy: 74.857143, Ensemble accuracy: 85.714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.299424\n",
      "1 epoch Loss: 3.062167\n",
      "2 epoch Loss: 2.857814\n",
      "3 epoch Loss: 2.650655\n",
      "4 epoch Loss: 2.540277\n",
      "5 epoch Loss: 2.444268\n",
      "6 epoch Loss: 2.340232\n",
      "7 epoch Loss: 2.267813\n",
      "8 epoch Loss: 2.188918\n",
      "9 epoch Loss: 2.155684\n",
      "10 epoch Loss: 2.089480\n",
      "11 epoch Loss: 2.062339\n",
      "12 epoch Loss: 1.999219\n",
      "13 epoch Loss: 1.939038\n",
      "14 epoch Loss: 1.901223\n",
      "15 epoch Loss: 1.860395\n",
      "16 epoch Loss: 1.841956\n",
      "17 epoch Loss: 1.780767\n",
      "18 epoch Loss: 1.705276\n",
      "19 epoch Loss: 1.711139\n",
      "20 epoch Loss: 1.688436\n",
      "21 epoch Loss: 1.646381\n",
      "22 epoch Loss: 1.650312\n",
      "23 epoch Loss: 1.571922\n",
      "24 epoch Loss: 1.568630\n",
      "25 epoch Loss: 1.517843\n",
      "26 epoch Loss: 1.508607\n",
      "27 epoch Loss: 1.508571\n",
      "28 epoch Loss: 1.467395\n",
      "29 epoch Loss: 1.452396\n",
      "30 epoch Loss: 1.428513\n",
      "31 epoch Loss: 1.398368\n",
      "32 epoch Loss: 1.412186\n",
      "33 epoch Loss: 1.383392\n",
      "34 epoch Loss: 1.355056\n",
      "35 epoch Loss: 1.364988\n",
      "36 epoch Loss: 1.352904\n",
      "37 epoch Loss: 1.358970\n",
      "38 epoch Loss: 1.288204\n",
      "39 epoch Loss: 1.300534\n",
      "40 epoch Loss: 1.305390\n",
      "41 epoch Loss: 1.290972\n",
      "42 epoch Loss: 1.244797\n",
      "43 epoch Loss: 1.234481\n",
      "44 epoch Loss: 1.228014\n",
      "45 epoch Loss: 1.201970\n",
      "46 epoch Loss: 1.216864\n",
      "47 epoch Loss: 1.211889\n",
      "48 epoch Loss: 1.210457\n",
      "49 epoch Loss: 1.174193\n",
      "50 epoch Loss: 1.185286\n",
      "51 epoch Loss: 1.180340\n",
      "52 epoch Loss: 1.153614\n",
      "53 epoch Loss: 1.181857\n",
      "54 epoch Loss: 1.155063\n",
      "55 epoch Loss: 1.128161\n",
      "56 epoch Loss: 1.118938\n",
      "57 epoch Loss: 1.120123\n",
      "58 epoch Loss: 1.105596\n",
      "59 epoch Loss: 1.119586\n",
      "60 epoch Loss: 1.121641\n",
      "61 epoch Loss: 1.077719\n",
      "62 epoch Loss: 1.111983\n",
      "63 epoch Loss: 1.098786\n",
      "64 epoch Loss: 1.111748\n",
      "65 epoch Loss: 1.093320\n",
      "66 epoch Loss: 1.099916\n",
      "67 epoch Loss: 1.058952\n",
      "68 epoch Loss: 1.115578\n",
      "69 epoch Loss: 1.045198\n",
      "70 epoch Loss: 1.059724\n",
      "71 epoch Loss: 1.057018\n",
      "72 epoch Loss: 1.041436\n",
      "73 epoch Loss: 1.065830\n",
      "74 epoch Loss: 1.069061\n",
      "75 epoch Loss: 1.075720\n",
      "76 epoch Loss: 1.048943\n",
      "77 epoch Loss: 1.074621\n",
      "78 epoch Loss: 1.059271\n",
      "79 epoch Loss: 1.077728\n",
      "80 epoch Loss: 1.043528\n",
      "81 epoch Loss: 1.050420\n",
      "82 epoch Loss: 1.030019\n",
      "83 epoch Loss: 1.031270\n",
      "84 epoch Loss: 1.014002\n",
      "85 epoch Loss: 1.008886\n",
      "86 epoch Loss: 1.048129\n",
      "87 epoch Loss: 1.028836\n",
      "88 epoch Loss: 1.046412\n",
      "89 epoch Loss: 1.044609\n",
      "90 epoch Loss: 1.056470\n",
      "91 epoch Loss: 1.037031\n",
      "92 epoch Loss: 1.048817\n",
      "93 epoch Loss: 1.024988\n",
      "94 epoch Loss: 1.011188\n",
      "95 epoch Loss: 1.019432\n",
      "96 epoch Loss: 1.034532\n",
      "97 epoch Loss: 1.010636\n",
      "98 epoch Loss: 1.038268\n",
      "99 epoch Loss: 1.020529\n",
      "100 epoch Loss: 1.050814\n",
      "101 epoch Loss: 1.024968\n",
      "102 epoch Loss: 1.028733\n",
      "103 epoch Loss: 1.045499\n",
      "104 epoch Loss: 1.047804\n",
      "105 epoch Loss: 1.041386\n",
      "106 epoch Loss: 1.032213\n",
      "107 epoch Loss: 1.006788\n",
      "108 epoch Loss: 1.030310\n",
      "109 epoch Loss: 1.025514\n",
      "110 epoch Loss: 1.019787\n",
      "111 epoch Loss: 1.026533\n",
      "112 epoch Loss: 1.016586\n",
      "113 epoch Loss: 1.039489\n",
      "114 epoch Loss: 1.022479\n",
      "115 epoch Loss: 1.008078\n",
      "116 epoch Loss: 1.044860\n",
      "117 epoch Loss: 1.030627\n",
      "118 epoch Loss: 1.031197\n",
      "119 epoch Loss: 1.032500\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.428571, Random forests accuracy: 74.857143, Ensemble accuracy: 86.714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.305181\n",
      "1 epoch Loss: 3.081078\n",
      "2 epoch Loss: 2.849780\n",
      "3 epoch Loss: 2.662886\n",
      "4 epoch Loss: 2.540391\n",
      "5 epoch Loss: 2.439088\n",
      "6 epoch Loss: 2.358856\n",
      "7 epoch Loss: 2.268872\n",
      "8 epoch Loss: 2.217098\n",
      "9 epoch Loss: 2.169975\n",
      "10 epoch Loss: 2.081108\n",
      "11 epoch Loss: 2.049602\n",
      "12 epoch Loss: 1.992225\n",
      "13 epoch Loss: 1.904408\n",
      "14 epoch Loss: 1.933161\n",
      "15 epoch Loss: 1.859438\n",
      "16 epoch Loss: 1.831039\n",
      "17 epoch Loss: 1.763823\n",
      "18 epoch Loss: 1.727908\n",
      "19 epoch Loss: 1.711636\n",
      "20 epoch Loss: 1.672765\n",
      "21 epoch Loss: 1.633638\n",
      "22 epoch Loss: 1.654721\n",
      "23 epoch Loss: 1.592889\n",
      "24 epoch Loss: 1.596395\n",
      "25 epoch Loss: 1.524454\n",
      "26 epoch Loss: 1.517944\n",
      "27 epoch Loss: 1.512854\n",
      "28 epoch Loss: 1.453163\n",
      "29 epoch Loss: 1.444536\n",
      "30 epoch Loss: 1.436003\n",
      "31 epoch Loss: 1.416157\n",
      "32 epoch Loss: 1.402117\n",
      "33 epoch Loss: 1.407093\n",
      "34 epoch Loss: 1.373454\n",
      "35 epoch Loss: 1.340259\n",
      "36 epoch Loss: 1.356929\n",
      "37 epoch Loss: 1.327682\n",
      "38 epoch Loss: 1.310913\n",
      "39 epoch Loss: 1.283607\n",
      "40 epoch Loss: 1.289683\n",
      "41 epoch Loss: 1.262390\n",
      "42 epoch Loss: 1.271442\n",
      "43 epoch Loss: 1.232039\n",
      "44 epoch Loss: 1.219849\n",
      "45 epoch Loss: 1.233826\n",
      "46 epoch Loss: 1.188918\n",
      "47 epoch Loss: 1.198371\n",
      "48 epoch Loss: 1.200108\n",
      "49 epoch Loss: 1.192398\n",
      "50 epoch Loss: 1.193789\n",
      "51 epoch Loss: 1.174183\n",
      "52 epoch Loss: 1.157731\n",
      "53 epoch Loss: 1.169853\n",
      "54 epoch Loss: 1.170614\n",
      "55 epoch Loss: 1.141836\n",
      "56 epoch Loss: 1.150281\n",
      "57 epoch Loss: 1.122116\n",
      "58 epoch Loss: 1.134704\n",
      "59 epoch Loss: 1.137609\n",
      "60 epoch Loss: 1.111991\n",
      "61 epoch Loss: 1.094444\n",
      "62 epoch Loss: 1.105759\n",
      "63 epoch Loss: 1.091670\n",
      "64 epoch Loss: 1.082659\n",
      "65 epoch Loss: 1.059197\n",
      "66 epoch Loss: 1.103785\n",
      "67 epoch Loss: 1.054491\n",
      "68 epoch Loss: 1.088853\n",
      "69 epoch Loss: 1.081525\n",
      "70 epoch Loss: 1.066085\n",
      "71 epoch Loss: 1.063939\n",
      "72 epoch Loss: 1.092991\n",
      "73 epoch Loss: 1.056158\n",
      "74 epoch Loss: 1.084871\n",
      "75 epoch Loss: 1.038515\n",
      "76 epoch Loss: 1.067052\n",
      "77 epoch Loss: 1.077590\n",
      "78 epoch Loss: 1.066539\n",
      "79 epoch Loss: 1.030011\n",
      "80 epoch Loss: 1.047308\n",
      "81 epoch Loss: 1.038139\n",
      "82 epoch Loss: 1.055477\n",
      "83 epoch Loss: 1.051346\n",
      "84 epoch Loss: 1.037259\n",
      "85 epoch Loss: 1.056837\n",
      "86 epoch Loss: 1.050637\n",
      "87 epoch Loss: 1.012532\n",
      "88 epoch Loss: 1.050852\n",
      "89 epoch Loss: 1.055360\n",
      "90 epoch Loss: 1.053220\n",
      "91 epoch Loss: 1.026310\n",
      "92 epoch Loss: 1.041046\n",
      "93 epoch Loss: 1.030724\n",
      "94 epoch Loss: 1.039727\n",
      "95 epoch Loss: 1.054783\n",
      "96 epoch Loss: 1.017918\n",
      "97 epoch Loss: 0.998962\n",
      "98 epoch Loss: 1.021137\n",
      "99 epoch Loss: 1.031824\n",
      "100 epoch Loss: 1.026298\n",
      "101 epoch Loss: 1.008031\n",
      "102 epoch Loss: 1.031342\n",
      "103 epoch Loss: 1.041466\n",
      "104 epoch Loss: 1.049575\n",
      "105 epoch Loss: 1.035511\n",
      "106 epoch Loss: 1.009440\n",
      "107 epoch Loss: 1.044209\n",
      "108 epoch Loss: 1.030105\n",
      "109 epoch Loss: 0.990069\n",
      "110 epoch Loss: 1.024473\n",
      "111 epoch Loss: 1.021067\n",
      "112 epoch Loss: 1.036055\n",
      "113 epoch Loss: 1.027958\n",
      "114 epoch Loss: 1.025622\n",
      "115 epoch Loss: 1.007706\n",
      "116 epoch Loss: 1.008937\n",
      "117 epoch Loss: 1.027487\n",
      "118 epoch Loss: 1.003069\n",
      "119 epoch Loss: 1.003836\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 83.285714, Random forests accuracy: 73.285714, Ensemble accuracy: 85.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.301729\n",
      "1 epoch Loss: 3.059731\n",
      "2 epoch Loss: 2.848877\n",
      "3 epoch Loss: 2.654081\n",
      "4 epoch Loss: 2.549233\n",
      "5 epoch Loss: 2.483818\n",
      "6 epoch Loss: 2.353323\n",
      "7 epoch Loss: 2.292512\n",
      "8 epoch Loss: 2.209664\n",
      "9 epoch Loss: 2.138418\n",
      "10 epoch Loss: 2.078721\n",
      "11 epoch Loss: 2.068344\n",
      "12 epoch Loss: 2.023943\n",
      "13 epoch Loss: 1.936435\n",
      "14 epoch Loss: 1.906122\n",
      "15 epoch Loss: 1.845803\n",
      "16 epoch Loss: 1.815241\n",
      "17 epoch Loss: 1.777111\n",
      "18 epoch Loss: 1.759096\n",
      "19 epoch Loss: 1.731192\n",
      "20 epoch Loss: 1.697060\n",
      "21 epoch Loss: 1.658665\n",
      "22 epoch Loss: 1.606579\n",
      "23 epoch Loss: 1.589828\n",
      "24 epoch Loss: 1.565210\n",
      "25 epoch Loss: 1.560665\n",
      "26 epoch Loss: 1.524722\n",
      "27 epoch Loss: 1.502273\n",
      "28 epoch Loss: 1.487242\n",
      "29 epoch Loss: 1.480385\n",
      "30 epoch Loss: 1.434890\n",
      "31 epoch Loss: 1.430096\n",
      "32 epoch Loss: 1.399053\n",
      "33 epoch Loss: 1.408993\n",
      "34 epoch Loss: 1.365594\n",
      "35 epoch Loss: 1.320412\n",
      "36 epoch Loss: 1.346560\n",
      "37 epoch Loss: 1.314339\n",
      "38 epoch Loss: 1.259795\n",
      "39 epoch Loss: 1.289956\n",
      "40 epoch Loss: 1.299226\n",
      "41 epoch Loss: 1.286077\n",
      "42 epoch Loss: 1.225361\n",
      "43 epoch Loss: 1.258177\n",
      "44 epoch Loss: 1.235628\n",
      "45 epoch Loss: 1.223038\n",
      "46 epoch Loss: 1.240060\n",
      "47 epoch Loss: 1.208751\n",
      "48 epoch Loss: 1.187656\n",
      "49 epoch Loss: 1.169607\n",
      "50 epoch Loss: 1.203523\n",
      "51 epoch Loss: 1.150991\n",
      "52 epoch Loss: 1.172078\n",
      "53 epoch Loss: 1.142416\n",
      "54 epoch Loss: 1.152187\n",
      "55 epoch Loss: 1.148339\n",
      "56 epoch Loss: 1.142148\n",
      "57 epoch Loss: 1.136811\n",
      "58 epoch Loss: 1.105363\n",
      "59 epoch Loss: 1.145796\n",
      "60 epoch Loss: 1.107538\n",
      "61 epoch Loss: 1.106249\n",
      "62 epoch Loss: 1.090953\n",
      "63 epoch Loss: 1.115386\n",
      "64 epoch Loss: 1.082138\n",
      "65 epoch Loss: 1.090974\n",
      "66 epoch Loss: 1.090983\n",
      "67 epoch Loss: 1.092963\n",
      "68 epoch Loss: 1.062656\n",
      "69 epoch Loss: 1.045854\n",
      "70 epoch Loss: 1.047301\n",
      "71 epoch Loss: 1.040647\n",
      "72 epoch Loss: 1.052961\n",
      "73 epoch Loss: 1.062640\n",
      "74 epoch Loss: 1.067122\n",
      "75 epoch Loss: 1.049008\n",
      "76 epoch Loss: 1.046402\n",
      "77 epoch Loss: 1.072588\n",
      "78 epoch Loss: 1.051302\n",
      "79 epoch Loss: 1.046279\n",
      "80 epoch Loss: 1.056711\n",
      "81 epoch Loss: 1.045534\n",
      "82 epoch Loss: 1.053483\n",
      "83 epoch Loss: 1.036120\n",
      "84 epoch Loss: 1.011126\n",
      "85 epoch Loss: 1.024811\n",
      "86 epoch Loss: 1.048483\n",
      "87 epoch Loss: 1.045746\n",
      "88 epoch Loss: 1.023298\n",
      "89 epoch Loss: 1.044171\n",
      "90 epoch Loss: 1.022889\n",
      "91 epoch Loss: 1.047142\n",
      "92 epoch Loss: 1.054950\n",
      "93 epoch Loss: 1.069904\n",
      "94 epoch Loss: 1.054246\n",
      "95 epoch Loss: 1.038801\n",
      "96 epoch Loss: 1.032564\n",
      "97 epoch Loss: 1.053203\n",
      "98 epoch Loss: 1.039899\n",
      "99 epoch Loss: 1.028818\n",
      "100 epoch Loss: 1.046711\n",
      "101 epoch Loss: 1.047247\n",
      "102 epoch Loss: 1.026904\n",
      "103 epoch Loss: 1.023314\n",
      "104 epoch Loss: 1.004853\n",
      "105 epoch Loss: 1.030885\n",
      "106 epoch Loss: 1.030207\n",
      "107 epoch Loss: 1.002292\n",
      "108 epoch Loss: 0.990996\n",
      "109 epoch Loss: 1.040196\n",
      "110 epoch Loss: 1.038731\n",
      "111 epoch Loss: 1.003902\n",
      "112 epoch Loss: 1.022397\n",
      "113 epoch Loss: 1.029161\n",
      "114 epoch Loss: 1.043724\n",
      "115 epoch Loss: 1.011670\n",
      "116 epoch Loss: 1.023116\n",
      "117 epoch Loss: 1.023940\n",
      "118 epoch Loss: 1.036521\n",
      "119 epoch Loss: 1.019117\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.000000, Random forests accuracy: 72.285714, Ensemble accuracy: 85.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.301092\n",
      "1 epoch Loss: 3.058680\n",
      "2 epoch Loss: 2.859800\n",
      "3 epoch Loss: 2.686576\n",
      "4 epoch Loss: 2.514249\n",
      "5 epoch Loss: 2.459444\n",
      "6 epoch Loss: 2.341753\n",
      "7 epoch Loss: 2.273892\n",
      "8 epoch Loss: 2.217465\n",
      "9 epoch Loss: 2.150462\n",
      "10 epoch Loss: 2.086863\n",
      "11 epoch Loss: 2.086723\n",
      "12 epoch Loss: 1.993728\n",
      "13 epoch Loss: 1.957869\n",
      "14 epoch Loss: 1.912502\n",
      "15 epoch Loss: 1.838401\n",
      "16 epoch Loss: 1.824266\n",
      "17 epoch Loss: 1.812723\n",
      "18 epoch Loss: 1.733134\n",
      "19 epoch Loss: 1.707964\n",
      "20 epoch Loss: 1.682826\n",
      "21 epoch Loss: 1.648313\n",
      "22 epoch Loss: 1.642080\n",
      "23 epoch Loss: 1.629427\n",
      "24 epoch Loss: 1.595382\n",
      "25 epoch Loss: 1.543376\n",
      "26 epoch Loss: 1.519906\n",
      "27 epoch Loss: 1.549699\n",
      "28 epoch Loss: 1.495097\n",
      "29 epoch Loss: 1.459153\n",
      "30 epoch Loss: 1.406699\n",
      "31 epoch Loss: 1.398858\n",
      "32 epoch Loss: 1.415119\n",
      "33 epoch Loss: 1.374470\n",
      "34 epoch Loss: 1.375785\n",
      "35 epoch Loss: 1.346347\n",
      "36 epoch Loss: 1.362413\n",
      "37 epoch Loss: 1.332418\n",
      "38 epoch Loss: 1.323151\n",
      "39 epoch Loss: 1.306538\n",
      "40 epoch Loss: 1.271507\n",
      "41 epoch Loss: 1.291846\n",
      "42 epoch Loss: 1.279212\n",
      "43 epoch Loss: 1.252521\n",
      "44 epoch Loss: 1.240765\n",
      "45 epoch Loss: 1.211674\n",
      "46 epoch Loss: 1.192196\n",
      "47 epoch Loss: 1.206383\n",
      "48 epoch Loss: 1.200918\n",
      "49 epoch Loss: 1.169274\n",
      "50 epoch Loss: 1.187537\n",
      "51 epoch Loss: 1.198979\n",
      "52 epoch Loss: 1.144571\n",
      "53 epoch Loss: 1.138775\n",
      "54 epoch Loss: 1.181481\n",
      "55 epoch Loss: 1.130428\n",
      "56 epoch Loss: 1.137699\n",
      "57 epoch Loss: 1.133395\n",
      "58 epoch Loss: 1.100258\n",
      "59 epoch Loss: 1.116299\n",
      "60 epoch Loss: 1.131165\n",
      "61 epoch Loss: 1.143285\n",
      "62 epoch Loss: 1.100760\n",
      "63 epoch Loss: 1.071175\n",
      "64 epoch Loss: 1.071159\n",
      "65 epoch Loss: 1.090381\n",
      "66 epoch Loss: 1.079588\n",
      "67 epoch Loss: 1.101725\n",
      "68 epoch Loss: 1.070633\n",
      "69 epoch Loss: 1.091446\n",
      "70 epoch Loss: 1.074100\n",
      "71 epoch Loss: 1.089562\n",
      "72 epoch Loss: 1.057520\n",
      "73 epoch Loss: 1.075346\n",
      "74 epoch Loss: 1.051957\n",
      "75 epoch Loss: 1.056423\n",
      "76 epoch Loss: 1.066830\n",
      "77 epoch Loss: 1.065948\n",
      "78 epoch Loss: 1.057391\n",
      "79 epoch Loss: 1.055069\n",
      "80 epoch Loss: 1.073206\n",
      "81 epoch Loss: 1.071025\n",
      "82 epoch Loss: 1.037322\n",
      "83 epoch Loss: 1.053163\n",
      "84 epoch Loss: 1.051715\n",
      "85 epoch Loss: 1.032186\n",
      "86 epoch Loss: 1.024922\n",
      "87 epoch Loss: 1.046540\n",
      "88 epoch Loss: 1.048214\n",
      "89 epoch Loss: 1.049226\n",
      "90 epoch Loss: 1.053741\n",
      "91 epoch Loss: 1.043743\n",
      "92 epoch Loss: 1.058497\n",
      "93 epoch Loss: 1.041551\n",
      "94 epoch Loss: 1.043110\n",
      "95 epoch Loss: 1.009574\n",
      "96 epoch Loss: 1.041214\n",
      "97 epoch Loss: 1.024917\n",
      "98 epoch Loss: 1.039922\n",
      "99 epoch Loss: 1.049476\n",
      "100 epoch Loss: 1.007855\n",
      "101 epoch Loss: 1.012208\n",
      "102 epoch Loss: 1.005056\n",
      "103 epoch Loss: 1.009014\n",
      "104 epoch Loss: 1.054527\n",
      "105 epoch Loss: 1.004815\n",
      "106 epoch Loss: 1.034497\n",
      "107 epoch Loss: 1.063099\n",
      "108 epoch Loss: 1.033690\n",
      "109 epoch Loss: 1.034608\n",
      "110 epoch Loss: 1.039661\n",
      "111 epoch Loss: 1.057584\n",
      "112 epoch Loss: 1.056974\n",
      "113 epoch Loss: 1.017565\n",
      "114 epoch Loss: 1.043724\n",
      "115 epoch Loss: 1.037631\n",
      "116 epoch Loss: 1.057909\n",
      "117 epoch Loss: 1.025186\n",
      "118 epoch Loss: 1.018833\n",
      "119 epoch Loss: 1.011821\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 85.571429, Random forests accuracy: 73.857143, Ensemble accuracy: 86.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.307845\n",
      "1 epoch Loss: 3.054070\n",
      "2 epoch Loss: 2.835854\n",
      "3 epoch Loss: 2.650138\n",
      "4 epoch Loss: 2.535246\n",
      "5 epoch Loss: 2.442703\n",
      "6 epoch Loss: 2.353704\n",
      "7 epoch Loss: 2.271527\n",
      "8 epoch Loss: 2.208033\n",
      "9 epoch Loss: 2.165883\n",
      "10 epoch Loss: 2.073394\n",
      "11 epoch Loss: 2.054374\n",
      "12 epoch Loss: 2.005544\n",
      "13 epoch Loss: 1.945898\n",
      "14 epoch Loss: 1.901107\n",
      "15 epoch Loss: 1.818436\n",
      "16 epoch Loss: 1.808882\n",
      "17 epoch Loss: 1.776940\n",
      "18 epoch Loss: 1.732081\n",
      "19 epoch Loss: 1.682647\n",
      "20 epoch Loss: 1.645586\n",
      "21 epoch Loss: 1.682561\n",
      "22 epoch Loss: 1.654741\n",
      "23 epoch Loss: 1.580241\n",
      "24 epoch Loss: 1.557944\n",
      "25 epoch Loss: 1.548242\n",
      "26 epoch Loss: 1.545306\n",
      "27 epoch Loss: 1.498853\n",
      "28 epoch Loss: 1.532131\n",
      "29 epoch Loss: 1.455982\n",
      "30 epoch Loss: 1.445089\n",
      "31 epoch Loss: 1.423340\n",
      "32 epoch Loss: 1.390380\n",
      "33 epoch Loss: 1.388738\n",
      "34 epoch Loss: 1.383643\n",
      "35 epoch Loss: 1.382539\n",
      "36 epoch Loss: 1.299300\n",
      "37 epoch Loss: 1.305229\n",
      "38 epoch Loss: 1.298715\n",
      "39 epoch Loss: 1.296716\n",
      "40 epoch Loss: 1.286751\n",
      "41 epoch Loss: 1.266280\n",
      "42 epoch Loss: 1.264879\n",
      "43 epoch Loss: 1.246741\n",
      "44 epoch Loss: 1.258674\n",
      "45 epoch Loss: 1.233434\n",
      "46 epoch Loss: 1.180097\n",
      "47 epoch Loss: 1.218550\n",
      "48 epoch Loss: 1.185300\n",
      "49 epoch Loss: 1.194365\n",
      "50 epoch Loss: 1.184093\n",
      "51 epoch Loss: 1.167447\n",
      "52 epoch Loss: 1.159521\n",
      "53 epoch Loss: 1.180538\n",
      "54 epoch Loss: 1.125731\n",
      "55 epoch Loss: 1.118328\n",
      "56 epoch Loss: 1.136003\n",
      "57 epoch Loss: 1.147534\n",
      "58 epoch Loss: 1.108678\n",
      "59 epoch Loss: 1.133741\n",
      "60 epoch Loss: 1.119744\n",
      "61 epoch Loss: 1.123324\n",
      "62 epoch Loss: 1.107827\n",
      "63 epoch Loss: 1.092484\n",
      "64 epoch Loss: 1.106803\n",
      "65 epoch Loss: 1.094206\n",
      "66 epoch Loss: 1.071513\n",
      "67 epoch Loss: 1.086434\n",
      "68 epoch Loss: 1.084616\n",
      "69 epoch Loss: 1.100534\n",
      "70 epoch Loss: 1.092746\n",
      "71 epoch Loss: 1.078932\n",
      "72 epoch Loss: 1.057964\n",
      "73 epoch Loss: 1.094748\n",
      "74 epoch Loss: 1.046025\n",
      "75 epoch Loss: 1.062950\n",
      "76 epoch Loss: 1.050792\n",
      "77 epoch Loss: 1.078046\n",
      "78 epoch Loss: 1.032457\n",
      "79 epoch Loss: 1.032863\n",
      "80 epoch Loss: 1.052711\n",
      "81 epoch Loss: 1.030536\n",
      "82 epoch Loss: 1.041315\n",
      "83 epoch Loss: 1.053689\n",
      "84 epoch Loss: 1.059084\n",
      "85 epoch Loss: 1.024014\n",
      "86 epoch Loss: 1.037177\n",
      "87 epoch Loss: 1.022372\n",
      "88 epoch Loss: 1.052634\n",
      "89 epoch Loss: 1.013391\n",
      "90 epoch Loss: 1.076863\n",
      "91 epoch Loss: 1.045964\n",
      "92 epoch Loss: 1.039137\n",
      "93 epoch Loss: 1.024931\n",
      "94 epoch Loss: 1.027719\n",
      "95 epoch Loss: 1.049522\n",
      "96 epoch Loss: 1.037603\n",
      "97 epoch Loss: 1.057281\n",
      "98 epoch Loss: 1.040678\n",
      "99 epoch Loss: 1.020445\n",
      "100 epoch Loss: 1.029258\n",
      "101 epoch Loss: 1.012831\n",
      "102 epoch Loss: 1.052711\n",
      "103 epoch Loss: 1.024685\n",
      "104 epoch Loss: 1.027999\n",
      "105 epoch Loss: 1.025983\n",
      "106 epoch Loss: 0.998666\n",
      "107 epoch Loss: 1.042740\n",
      "108 epoch Loss: 0.984930\n",
      "109 epoch Loss: 1.030855\n",
      "110 epoch Loss: 1.030903\n",
      "111 epoch Loss: 1.012250\n",
      "112 epoch Loss: 1.020140\n",
      "113 epoch Loss: 0.998263\n",
      "114 epoch Loss: 1.027039\n",
      "115 epoch Loss: 1.003171\n",
      "116 epoch Loss: 0.989756\n",
      "117 epoch Loss: 1.016544\n",
      "118 epoch Loss: 1.036010\n",
      "119 epoch Loss: 1.010940\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 86.000000, Random forests accuracy: 72.857143, Ensemble accuracy: 86.857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.298809\n",
      "1 epoch Loss: 3.040163\n",
      "2 epoch Loss: 2.835002\n",
      "3 epoch Loss: 2.664063\n",
      "4 epoch Loss: 2.528808\n",
      "5 epoch Loss: 2.424531\n",
      "6 epoch Loss: 2.333202\n",
      "7 epoch Loss: 2.256436\n",
      "8 epoch Loss: 2.184590\n",
      "9 epoch Loss: 2.146077\n",
      "10 epoch Loss: 2.061520\n",
      "11 epoch Loss: 2.028784\n",
      "12 epoch Loss: 1.977339\n",
      "13 epoch Loss: 1.924852\n",
      "14 epoch Loss: 1.863451\n",
      "15 epoch Loss: 1.819715\n",
      "16 epoch Loss: 1.825741\n",
      "17 epoch Loss: 1.780716\n",
      "18 epoch Loss: 1.731833\n",
      "19 epoch Loss: 1.723984\n",
      "20 epoch Loss: 1.664620\n",
      "21 epoch Loss: 1.640637\n",
      "22 epoch Loss: 1.609938\n",
      "23 epoch Loss: 1.579983\n",
      "24 epoch Loss: 1.556351\n",
      "25 epoch Loss: 1.547389\n",
      "26 epoch Loss: 1.530760\n",
      "27 epoch Loss: 1.520958\n",
      "28 epoch Loss: 1.472584\n",
      "29 epoch Loss: 1.453415\n",
      "30 epoch Loss: 1.417488\n",
      "31 epoch Loss: 1.411611\n",
      "32 epoch Loss: 1.395660\n",
      "33 epoch Loss: 1.400935\n",
      "34 epoch Loss: 1.349908\n",
      "35 epoch Loss: 1.370995\n",
      "36 epoch Loss: 1.319143\n",
      "37 epoch Loss: 1.334801\n",
      "38 epoch Loss: 1.290439\n",
      "39 epoch Loss: 1.301644\n",
      "40 epoch Loss: 1.265542\n",
      "41 epoch Loss: 1.252707\n",
      "42 epoch Loss: 1.260882\n",
      "43 epoch Loss: 1.202223\n",
      "44 epoch Loss: 1.218513\n",
      "45 epoch Loss: 1.229911\n",
      "46 epoch Loss: 1.218316\n",
      "47 epoch Loss: 1.244263\n",
      "48 epoch Loss: 1.199211\n",
      "49 epoch Loss: 1.189928\n",
      "50 epoch Loss: 1.185384\n",
      "51 epoch Loss: 1.157923\n",
      "52 epoch Loss: 1.138395\n",
      "53 epoch Loss: 1.161349\n",
      "54 epoch Loss: 1.134311\n",
      "55 epoch Loss: 1.140187\n",
      "56 epoch Loss: 1.119800\n",
      "57 epoch Loss: 1.099431\n",
      "58 epoch Loss: 1.149810\n",
      "59 epoch Loss: 1.117763\n",
      "60 epoch Loss: 1.131014\n",
      "61 epoch Loss: 1.121848\n",
      "62 epoch Loss: 1.101344\n",
      "63 epoch Loss: 1.113784\n",
      "64 epoch Loss: 1.069227\n",
      "65 epoch Loss: 1.079646\n",
      "66 epoch Loss: 1.095316\n",
      "67 epoch Loss: 1.077137\n",
      "68 epoch Loss: 1.042151\n",
      "69 epoch Loss: 1.052299\n",
      "70 epoch Loss: 1.094936\n",
      "71 epoch Loss: 1.077693\n",
      "72 epoch Loss: 1.073942\n",
      "73 epoch Loss: 1.070404\n",
      "74 epoch Loss: 1.059662\n",
      "75 epoch Loss: 1.053874\n",
      "76 epoch Loss: 1.061933\n",
      "77 epoch Loss: 1.060541\n",
      "78 epoch Loss: 1.050347\n",
      "79 epoch Loss: 1.061360\n",
      "80 epoch Loss: 1.025196\n",
      "81 epoch Loss: 1.027917\n",
      "82 epoch Loss: 1.044435\n",
      "83 epoch Loss: 1.048590\n",
      "84 epoch Loss: 1.039661\n",
      "85 epoch Loss: 1.042560\n",
      "86 epoch Loss: 1.054353\n",
      "87 epoch Loss: 1.049435\n",
      "88 epoch Loss: 1.028600\n",
      "89 epoch Loss: 1.041771\n",
      "90 epoch Loss: 1.032431\n",
      "91 epoch Loss: 1.064633\n",
      "92 epoch Loss: 1.044464\n",
      "93 epoch Loss: 0.998197\n",
      "94 epoch Loss: 1.018876\n",
      "95 epoch Loss: 1.050714\n",
      "96 epoch Loss: 1.036553\n",
      "97 epoch Loss: 1.036032\n",
      "98 epoch Loss: 1.031921\n",
      "99 epoch Loss: 1.022271\n",
      "100 epoch Loss: 1.059318\n",
      "101 epoch Loss: 1.014666\n",
      "102 epoch Loss: 1.004016\n",
      "103 epoch Loss: 1.023898\n",
      "104 epoch Loss: 1.022410\n",
      "105 epoch Loss: 1.016101\n",
      "106 epoch Loss: 1.021753\n",
      "107 epoch Loss: 1.055833\n",
      "108 epoch Loss: 1.006690\n",
      "109 epoch Loss: 1.036579\n",
      "110 epoch Loss: 1.036343\n",
      "111 epoch Loss: 0.989053\n",
      "112 epoch Loss: 1.027909\n",
      "113 epoch Loss: 1.015800\n",
      "114 epoch Loss: 0.988501\n",
      "115 epoch Loss: 1.041131\n",
      "116 epoch Loss: 1.021979\n",
      "117 epoch Loss: 1.009660\n",
      "118 epoch Loss: 1.014692\n",
      "119 epoch Loss: 1.028115\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 85.000000, Random forests accuracy: 72.714286, Ensemble accuracy: 86.428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.308000\n",
      "1 epoch Loss: 3.080876\n",
      "2 epoch Loss: 2.867749\n",
      "3 epoch Loss: 2.677594\n",
      "4 epoch Loss: 2.530935\n",
      "5 epoch Loss: 2.450881\n",
      "6 epoch Loss: 2.362715\n",
      "7 epoch Loss: 2.275850\n",
      "8 epoch Loss: 2.211630\n",
      "9 epoch Loss: 2.129360\n",
      "10 epoch Loss: 2.106263\n",
      "11 epoch Loss: 2.081056\n",
      "12 epoch Loss: 1.996155\n",
      "13 epoch Loss: 1.952961\n",
      "14 epoch Loss: 1.925194\n",
      "15 epoch Loss: 1.845612\n",
      "16 epoch Loss: 1.832429\n",
      "17 epoch Loss: 1.803304\n",
      "18 epoch Loss: 1.770312\n",
      "19 epoch Loss: 1.709246\n",
      "20 epoch Loss: 1.708583\n",
      "21 epoch Loss: 1.653558\n",
      "22 epoch Loss: 1.624647\n",
      "23 epoch Loss: 1.623245\n",
      "24 epoch Loss: 1.568527\n",
      "25 epoch Loss: 1.570508\n",
      "26 epoch Loss: 1.545486\n",
      "27 epoch Loss: 1.482163\n",
      "28 epoch Loss: 1.468230\n",
      "29 epoch Loss: 1.464309\n",
      "30 epoch Loss: 1.430926\n",
      "31 epoch Loss: 1.415331\n",
      "32 epoch Loss: 1.397830\n",
      "33 epoch Loss: 1.377975\n",
      "34 epoch Loss: 1.364017\n",
      "35 epoch Loss: 1.358585\n",
      "36 epoch Loss: 1.336527\n",
      "37 epoch Loss: 1.323112\n",
      "38 epoch Loss: 1.315572\n",
      "39 epoch Loss: 1.320150\n",
      "40 epoch Loss: 1.274896\n",
      "41 epoch Loss: 1.296631\n",
      "42 epoch Loss: 1.241012\n",
      "43 epoch Loss: 1.286925\n",
      "44 epoch Loss: 1.260138\n",
      "45 epoch Loss: 1.230635\n",
      "46 epoch Loss: 1.216615\n",
      "47 epoch Loss: 1.185319\n",
      "48 epoch Loss: 1.191094\n",
      "49 epoch Loss: 1.168033\n",
      "50 epoch Loss: 1.169547\n",
      "51 epoch Loss: 1.181132\n",
      "52 epoch Loss: 1.163890\n",
      "53 epoch Loss: 1.160458\n",
      "54 epoch Loss: 1.182051\n",
      "55 epoch Loss: 1.168646\n",
      "56 epoch Loss: 1.169226\n",
      "57 epoch Loss: 1.127556\n",
      "58 epoch Loss: 1.083562\n",
      "59 epoch Loss: 1.117483\n",
      "60 epoch Loss: 1.123182\n",
      "61 epoch Loss: 1.111072\n",
      "62 epoch Loss: 1.117520\n",
      "63 epoch Loss: 1.102290\n",
      "64 epoch Loss: 1.092104\n",
      "65 epoch Loss: 1.089704\n",
      "66 epoch Loss: 1.090278\n",
      "67 epoch Loss: 1.083720\n",
      "68 epoch Loss: 1.048305\n",
      "69 epoch Loss: 1.082813\n",
      "70 epoch Loss: 1.094290\n",
      "71 epoch Loss: 1.062940\n",
      "72 epoch Loss: 1.091885\n",
      "73 epoch Loss: 1.077152\n",
      "74 epoch Loss: 1.067349\n",
      "75 epoch Loss: 1.082701\n",
      "76 epoch Loss: 1.060954\n",
      "77 epoch Loss: 1.028470\n",
      "78 epoch Loss: 1.021586\n",
      "79 epoch Loss: 1.062561\n",
      "80 epoch Loss: 1.054898\n",
      "81 epoch Loss: 1.044278\n",
      "82 epoch Loss: 1.037292\n",
      "83 epoch Loss: 1.054673\n",
      "84 epoch Loss: 1.059215\n",
      "85 epoch Loss: 1.067858\n",
      "86 epoch Loss: 1.049069\n",
      "87 epoch Loss: 1.020127\n",
      "88 epoch Loss: 1.050119\n",
      "89 epoch Loss: 1.047357\n",
      "90 epoch Loss: 1.046657\n",
      "91 epoch Loss: 1.042476\n",
      "92 epoch Loss: 1.046649\n",
      "93 epoch Loss: 1.051836\n",
      "94 epoch Loss: 1.037984\n",
      "95 epoch Loss: 1.018984\n",
      "96 epoch Loss: 1.030910\n",
      "97 epoch Loss: 1.012530\n",
      "98 epoch Loss: 1.054275\n",
      "99 epoch Loss: 1.029108\n",
      "100 epoch Loss: 1.029409\n",
      "101 epoch Loss: 1.012072\n",
      "102 epoch Loss: 1.034333\n",
      "103 epoch Loss: 1.022548\n",
      "104 epoch Loss: 1.040629\n",
      "105 epoch Loss: 1.027690\n",
      "106 epoch Loss: 1.014010\n",
      "107 epoch Loss: 1.037700\n",
      "108 epoch Loss: 1.031149\n",
      "109 epoch Loss: 1.039355\n",
      "110 epoch Loss: 1.038904\n",
      "111 epoch Loss: 1.061810\n",
      "112 epoch Loss: 1.018857\n",
      "113 epoch Loss: 1.042147\n",
      "114 epoch Loss: 0.986367\n",
      "115 epoch Loss: 1.027327\n",
      "116 epoch Loss: 1.027815\n",
      "117 epoch Loss: 1.021460\n",
      "118 epoch Loss: 1.050267\n",
      "119 epoch Loss: 1.045299\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 86.428571, Random forests accuracy: 75.714286, Ensemble accuracy: 88.285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.294894\n",
      "1 epoch Loss: 3.069373\n",
      "2 epoch Loss: 2.840021\n",
      "3 epoch Loss: 2.649222\n",
      "4 epoch Loss: 2.512878\n",
      "5 epoch Loss: 2.402183\n",
      "6 epoch Loss: 2.329227\n",
      "7 epoch Loss: 2.272336\n",
      "8 epoch Loss: 2.164235\n",
      "9 epoch Loss: 2.132589\n",
      "10 epoch Loss: 2.092755\n",
      "11 epoch Loss: 2.045479\n",
      "12 epoch Loss: 1.952923\n",
      "13 epoch Loss: 1.921551\n",
      "14 epoch Loss: 1.882305\n",
      "15 epoch Loss: 1.859456\n",
      "16 epoch Loss: 1.794275\n",
      "17 epoch Loss: 1.753563\n",
      "18 epoch Loss: 1.725567\n",
      "19 epoch Loss: 1.699327\n",
      "20 epoch Loss: 1.671585\n",
      "21 epoch Loss: 1.661360\n",
      "22 epoch Loss: 1.653817\n",
      "23 epoch Loss: 1.558971\n",
      "24 epoch Loss: 1.575794\n",
      "25 epoch Loss: 1.533352\n",
      "26 epoch Loss: 1.538994\n",
      "27 epoch Loss: 1.517411\n",
      "28 epoch Loss: 1.485650\n",
      "29 epoch Loss: 1.469154\n",
      "30 epoch Loss: 1.435509\n",
      "31 epoch Loss: 1.415296\n",
      "32 epoch Loss: 1.404718\n",
      "33 epoch Loss: 1.375201\n",
      "34 epoch Loss: 1.343894\n",
      "35 epoch Loss: 1.360041\n",
      "36 epoch Loss: 1.328593\n",
      "37 epoch Loss: 1.366391\n",
      "38 epoch Loss: 1.297215\n",
      "39 epoch Loss: 1.275376\n",
      "40 epoch Loss: 1.250461\n",
      "41 epoch Loss: 1.256404\n",
      "42 epoch Loss: 1.257297\n",
      "43 epoch Loss: 1.246642\n",
      "44 epoch Loss: 1.254067\n",
      "45 epoch Loss: 1.225499\n",
      "46 epoch Loss: 1.185644\n",
      "47 epoch Loss: 1.194906\n",
      "48 epoch Loss: 1.194560\n",
      "49 epoch Loss: 1.188862\n",
      "50 epoch Loss: 1.173041\n",
      "51 epoch Loss: 1.195098\n",
      "52 epoch Loss: 1.155316\n",
      "53 epoch Loss: 1.159987\n",
      "54 epoch Loss: 1.118284\n",
      "55 epoch Loss: 1.116509\n",
      "56 epoch Loss: 1.156979\n",
      "57 epoch Loss: 1.125386\n",
      "58 epoch Loss: 1.119381\n",
      "59 epoch Loss: 1.083457\n",
      "60 epoch Loss: 1.120306\n",
      "61 epoch Loss: 1.116388\n",
      "62 epoch Loss: 1.087834\n",
      "63 epoch Loss: 1.081593\n",
      "64 epoch Loss: 1.101247\n",
      "65 epoch Loss: 1.092393\n",
      "66 epoch Loss: 1.074889\n",
      "67 epoch Loss: 1.089863\n",
      "68 epoch Loss: 1.114296\n",
      "69 epoch Loss: 1.041529\n",
      "70 epoch Loss: 1.090660\n",
      "71 epoch Loss: 1.070317\n",
      "72 epoch Loss: 1.090766\n",
      "73 epoch Loss: 1.074653\n",
      "74 epoch Loss: 1.093151\n",
      "75 epoch Loss: 1.072925\n",
      "76 epoch Loss: 1.041867\n",
      "77 epoch Loss: 1.070871\n",
      "78 epoch Loss: 1.069815\n",
      "79 epoch Loss: 1.065134\n",
      "80 epoch Loss: 1.070155\n",
      "81 epoch Loss: 1.036627\n",
      "82 epoch Loss: 1.078628\n",
      "83 epoch Loss: 1.033711\n",
      "84 epoch Loss: 1.061117\n",
      "85 epoch Loss: 1.054441\n",
      "86 epoch Loss: 1.041909\n",
      "87 epoch Loss: 1.058265\n",
      "88 epoch Loss: 1.033406\n",
      "89 epoch Loss: 1.028888\n",
      "90 epoch Loss: 1.055336\n",
      "91 epoch Loss: 1.038342\n",
      "92 epoch Loss: 1.054852\n",
      "93 epoch Loss: 1.045499\n",
      "94 epoch Loss: 1.044254\n",
      "95 epoch Loss: 1.044847\n",
      "96 epoch Loss: 1.033246\n",
      "97 epoch Loss: 1.034604\n",
      "98 epoch Loss: 1.028262\n",
      "99 epoch Loss: 1.014582\n",
      "100 epoch Loss: 1.031693\n",
      "101 epoch Loss: 1.021876\n",
      "102 epoch Loss: 1.015268\n",
      "103 epoch Loss: 1.027294\n",
      "104 epoch Loss: 1.020449\n",
      "105 epoch Loss: 1.037686\n",
      "106 epoch Loss: 1.027168\n",
      "107 epoch Loss: 1.026515\n",
      "108 epoch Loss: 1.018889\n",
      "109 epoch Loss: 1.031627\n",
      "110 epoch Loss: 1.020946\n",
      "111 epoch Loss: 1.049738\n",
      "112 epoch Loss: 1.037279\n",
      "113 epoch Loss: 1.013656\n",
      "114 epoch Loss: 1.027192\n",
      "115 epoch Loss: 1.023692\n",
      "116 epoch Loss: 1.041192\n",
      "117 epoch Loss: 1.004828\n",
      "118 epoch Loss: 1.017525\n",
      "119 epoch Loss: 1.012165\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 85.000000, Random forests accuracy: 74.000000, Ensemble accuracy: 86.428571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/sophie52052/.local/lib/python3.6/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Loss: 3.297373\n",
      "1 epoch Loss: 3.050640\n",
      "2 epoch Loss: 2.848047\n",
      "3 epoch Loss: 2.635403\n",
      "4 epoch Loss: 2.525977\n",
      "5 epoch Loss: 2.430155\n",
      "6 epoch Loss: 2.329258\n",
      "7 epoch Loss: 2.283524\n",
      "8 epoch Loss: 2.181920\n",
      "9 epoch Loss: 2.131340\n",
      "10 epoch Loss: 2.095892\n",
      "11 epoch Loss: 2.051627\n",
      "12 epoch Loss: 2.002188\n",
      "13 epoch Loss: 1.942599\n",
      "14 epoch Loss: 1.904583\n",
      "15 epoch Loss: 1.853690\n",
      "16 epoch Loss: 1.817147\n",
      "17 epoch Loss: 1.779894\n",
      "18 epoch Loss: 1.751384\n",
      "19 epoch Loss: 1.724274\n",
      "20 epoch Loss: 1.677838\n",
      "21 epoch Loss: 1.650767\n",
      "22 epoch Loss: 1.623405\n",
      "23 epoch Loss: 1.639282\n",
      "24 epoch Loss: 1.583941\n",
      "25 epoch Loss: 1.538738\n",
      "26 epoch Loss: 1.524196\n",
      "27 epoch Loss: 1.500958\n",
      "28 epoch Loss: 1.494360\n",
      "29 epoch Loss: 1.431478\n",
      "30 epoch Loss: 1.432782\n",
      "31 epoch Loss: 1.406144\n",
      "32 epoch Loss: 1.370996\n",
      "33 epoch Loss: 1.371501\n",
      "34 epoch Loss: 1.377447\n",
      "35 epoch Loss: 1.334923\n",
      "36 epoch Loss: 1.344533\n",
      "37 epoch Loss: 1.337732\n",
      "38 epoch Loss: 1.323594\n",
      "39 epoch Loss: 1.302269\n",
      "40 epoch Loss: 1.247225\n",
      "41 epoch Loss: 1.256811\n",
      "42 epoch Loss: 1.270040\n",
      "43 epoch Loss: 1.253253\n",
      "44 epoch Loss: 1.228050\n",
      "45 epoch Loss: 1.235773\n",
      "46 epoch Loss: 1.219196\n",
      "47 epoch Loss: 1.204946\n",
      "48 epoch Loss: 1.191524\n",
      "49 epoch Loss: 1.205476\n",
      "50 epoch Loss: 1.196451\n",
      "51 epoch Loss: 1.166330\n",
      "52 epoch Loss: 1.193661\n",
      "53 epoch Loss: 1.152042\n",
      "54 epoch Loss: 1.124988\n",
      "55 epoch Loss: 1.154421\n",
      "56 epoch Loss: 1.132646\n",
      "57 epoch Loss: 1.101090\n",
      "58 epoch Loss: 1.144642\n",
      "59 epoch Loss: 1.105833\n",
      "60 epoch Loss: 1.124791\n",
      "61 epoch Loss: 1.107077\n",
      "62 epoch Loss: 1.110963\n",
      "63 epoch Loss: 1.105225\n",
      "64 epoch Loss: 1.107744\n",
      "65 epoch Loss: 1.121731\n",
      "66 epoch Loss: 1.054664\n",
      "67 epoch Loss: 1.089240\n",
      "68 epoch Loss: 1.072727\n",
      "69 epoch Loss: 1.074109\n",
      "70 epoch Loss: 1.067975\n",
      "71 epoch Loss: 1.062431\n",
      "72 epoch Loss: 1.061366\n",
      "73 epoch Loss: 1.071243\n",
      "74 epoch Loss: 1.090302\n",
      "75 epoch Loss: 1.057575\n",
      "76 epoch Loss: 1.074070\n",
      "77 epoch Loss: 1.071557\n",
      "78 epoch Loss: 1.080011\n",
      "79 epoch Loss: 1.067783\n",
      "80 epoch Loss: 1.032807\n",
      "81 epoch Loss: 1.087344\n",
      "82 epoch Loss: 1.057905\n",
      "83 epoch Loss: 1.041397\n",
      "84 epoch Loss: 1.069542\n",
      "85 epoch Loss: 1.052900\n",
      "86 epoch Loss: 1.024005\n",
      "87 epoch Loss: 1.047605\n",
      "88 epoch Loss: 1.050962\n",
      "89 epoch Loss: 1.029691\n",
      "90 epoch Loss: 1.059466\n",
      "91 epoch Loss: 1.019951\n",
      "92 epoch Loss: 1.020092\n",
      "93 epoch Loss: 1.041543\n",
      "94 epoch Loss: 1.050961\n",
      "95 epoch Loss: 1.034007\n",
      "96 epoch Loss: 1.051687\n",
      "97 epoch Loss: 1.030493\n",
      "98 epoch Loss: 1.046108\n",
      "99 epoch Loss: 1.019279\n",
      "100 epoch Loss: 1.031110\n",
      "101 epoch Loss: 1.042047\n",
      "102 epoch Loss: 1.022795\n",
      "103 epoch Loss: 1.038761\n",
      "104 epoch Loss: 1.055365\n",
      "105 epoch Loss: 1.005381\n",
      "106 epoch Loss: 1.027124\n",
      "107 epoch Loss: 0.993210\n",
      "108 epoch Loss: 1.024305\n",
      "109 epoch Loss: 1.041750\n",
      "110 epoch Loss: 1.020128\n",
      "111 epoch Loss: 1.051205\n",
      "112 epoch Loss: 1.028743\n",
      "113 epoch Loss: 1.044931\n",
      "114 epoch Loss: 1.059413\n",
      "115 epoch Loss: 1.029241\n",
      "116 epoch Loss: 1.025366\n",
      "117 epoch Loss: 1.004423\n",
      "118 epoch Loss: 1.046086\n",
      "119 epoch Loss: 1.010613\n",
      "Outer_fold # of features:  4000, Neural network accuracy: 84.615385, Random forests accuracy: 72.507123, Ensemble accuracy: 85.612536\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#Load data\n",
    "\n",
    "    \n",
    "    #Initialize\n",
    "    label -= 1\n",
    "    np.random.seed(2018)\n",
    "\n",
    "\n",
    "    \n",
    "    t_index = np.random.permutation(int(np.shape(data)[0]/Outer_loop)*Outer_loop)\n",
    "    t_index = np.reshape(t_index, [Outer_loop, -1])\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    \n",
    "    box = np.array([4000], dtype=np.int32)\n",
    "    flag = 0\n",
    "    for test_index in t_index:\n",
    "        if flag == Outer_loop-1:\n",
    "            test_index = np.array(np.concatenate((test_index, np.array(range(int(np.shape(data)[0]/Outer_loop)*Outer_loop,np.shape(data)[0]))), axis=0), dtype=np.int32)\n",
    "        train_index = np.setdiff1d(np.array(range(0,np.shape(data)[0])), test_index)\n",
    "        train_data = data[train_index]\n",
    "        train_label = label[train_index]\n",
    "        test_data = data[test_index]\n",
    "        test_label = label[test_index]\n",
    "        \n",
    "        kf = np.random.permutation(int(np.shape(train_data)[0]/Inner_loop)*Inner_loop)\n",
    "        kf = kf.reshape([Inner_loop]+[-1])\n",
    "        val_result = np.zeros([np.shape(train_data)[0],48], dtype=np.float32)\n",
    "        \n",
    "        tot_acc = np.zeros([Inner_loop,5], dtype=np.float32)\n",
    "        #lasso = Lasso()\n",
    "        lsvc = LinearSVC(C=1, penalty=\"l1\", dual=False).fit(data, label)\n",
    "        coef = np.squeeze(np.sum(np.square(np.array(lsvc.coef_)), axis=0))\n",
    "        #coef = np.squeeze(np.sum(np.square(np.array(lasso.coef_)), axis=0))\n",
    "        coefidx = np.argsort(coef)\n",
    "#         for inner_fold in range(0,Inner_loop):\n",
    "#             val_test_ind = kf[inner_fold]\n",
    "#             if inner_fold == Inner_loop-1:\n",
    "#                 val_test_ind = np.array(np.concatenate((val_test_ind,np.array(range(int(np.shape(train_data)[0]/Outer_loop)*Outer_loop,np.shape(train_data)[0]),dtype=np.int32)), axis=0),dtype=np.int32)\n",
    "            \n",
    "#             val_train_ind = np.setdiff1d(np.array(range(0,np.shape(train_data)[0]),dtype=np.int32), val_test_ind)\n",
    "#             val_train = train_data[val_train_ind]\n",
    "#             val_test = train_data[val_test_ind]\n",
    "#             val_train_label = train_label[val_train_ind]\n",
    "#             val_test_label = train_label[val_test_ind]\n",
    "#             temp = 0\n",
    "#             for item in box:\n",
    "#                 idx = coefidx[-item:]\n",
    "#                 vtrain = val_train[:,idx]\n",
    "#                 vtest = val_test[:,idx]\n",
    "#                 nn_acc, result_nn = dnn(vtrain, val_train_label, vtest, val_test_label)\n",
    "#                 rf_acc, result_rf = rfc(vtrain, val_train_label, vtest, val_test_label)\n",
    "#                 en_acc = 0.0\n",
    "#                 for i in range(0,np.shape(vtest)[0]):\n",
    "#                     r = np.argmax(result_nn[i]+result_rf[i])\n",
    "#                     if r == val_test_label[i]:\n",
    "#                         en_acc += 1\n",
    "#                 en_acc /= np.shape(vtest)[0]*0.01\n",
    "#                 tot_acc[inner_fold,temp] = en_acc\n",
    "#                 print(\"Inner_fold # of features: %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (item, nn_acc, rf_acc, en_acc))\n",
    "#                 temp += 1\n",
    "        \n",
    "        u = np.sum(tot_acc,0)\n",
    "       \n",
    "        best_n = box[np.argmax(u)]\n",
    "        idx = coefidx[-best_n:]\n",
    "        \n",
    "        tr_data = train_data[:,idx]\n",
    "        te_data = test_data[:,idx]\n",
    "        nn_acc, result_nn = dnn(tr_data, train_label, te_data, test_label)\n",
    "        rf_acc, result_rf = rfc(tr_data, train_label, te_data, test_label)\n",
    "        en_acc = 0.0\n",
    "        for i in range(0,np.shape(te_data)[0]):\n",
    "            r = np.argmax(result_nn[i]+result_rf[i])\n",
    "            if r == test_label[i]:\n",
    "                en_acc += 1\n",
    "        en_acc /= np.shape(te_data)[0]*0.01\n",
    "        print(\"Outer_fold # of features:  %d, Neural network accuracy: %f, Random forests accuracy: %f, Ensemble accuracy: %f\" % (best_n, nn_acc, rf_acc, en_acc))\n",
    "        flag += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4000\n",
    "#lsvc\n",
    "lsvc_nn=[84.142857,84.428571,83.285714,84.000000,85.571429,86.000000,85.000000,86.428571,85.000000,84.615385]\n",
    "lsvc_rf=[74.857143,74.857143,73.285714,72.285714,73.857143,72.857143,72.714286,75.714286,74.000000,72.507123]\n",
    "emsemble=[85.714286,86.714286,85.571429,85.000000,86.857143,86.857143,86.428571,88.285714,86.428571,85.612536]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=[85.714286,86.714286,85.571429,85.000000,86.857143,86.857143,86.428571,88.285714,86.428571,85.612536,\n",
    "    84.142857,84.428571,83.285714,84.000000,85.571429,86.000000,85.000000,86.428571,85.000000,84.615385,\n",
    "    74.857143,74.857143,73.285714,72.285714,73.857143,72.857143,72.714286,75.714286,74.000000,72.507123]\n",
    "md=['ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble','ensemble',\n",
    "    'neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network','neural network',\n",
    "    'random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest','random forest',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy':acc,'Model':md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.714286</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.714286</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85.571429</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86.857143</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>86.857143</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.428571</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.285714</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>86.428571</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85.612536</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>84.142857</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>84.428571</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>83.285714</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>84.000000</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>85.571429</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>86.428571</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>85.000000</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>84.615385</td>\n",
       "      <td>neural network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>74.857143</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>74.857143</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73.285714</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>72.285714</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>73.857143</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>72.857143</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>72.714286</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>75.714286</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>74.000000</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>72.507123</td>\n",
       "      <td>random forest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy           Model\n",
       "0   85.714286        ensemble\n",
       "1   86.714286        ensemble\n",
       "2   85.571429        ensemble\n",
       "3   85.000000        ensemble\n",
       "4   86.857143        ensemble\n",
       "5   86.857143        ensemble\n",
       "6   86.428571        ensemble\n",
       "7   88.285714        ensemble\n",
       "8   86.428571        ensemble\n",
       "9   85.612536        ensemble\n",
       "10  84.142857  neural network\n",
       "11  84.428571  neural network\n",
       "12  83.285714  neural network\n",
       "13  84.000000  neural network\n",
       "14  85.571429  neural network\n",
       "15  86.000000  neural network\n",
       "16  85.000000  neural network\n",
       "17  86.428571  neural network\n",
       "18  85.000000  neural network\n",
       "19  84.615385  neural network\n",
       "20  74.857143   random forest\n",
       "21  74.857143   random forest\n",
       "22  73.285714   random forest\n",
       "23  72.285714   random forest\n",
       "24  73.857143   random forest\n",
       "25  72.857143   random forest\n",
       "26  72.714286   random forest\n",
       "27  75.714286   random forest\n",
       "28  74.000000   random forest\n",
       "29  72.507123   random forest"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\",rc={'figure.figsize':(11.7,8.27)})\n",
    "ax = sns.boxplot(x = \"Model\", y = \"Accuracy\", data = df, palette=\"Set3\", width=0.6, fliersize=2)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
